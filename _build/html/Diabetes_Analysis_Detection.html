
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Diabetes Analysis and Detection with Statistics and Machine Learning &#8212; Diabetes Analysis and Detection</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Diabetes_Analysis_Detection';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Diabetes Analysis and Detection" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Diabetes Analysis and Detection - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Diabetes Analysis and Detection - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Diabetes Analysis and Detection
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#"><strong>Diabetes Analysis and Detection with Statistics and Machine Learning</strong></a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/FabioScielzoOrtiz/Diabetes-Analysis-Detection" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/FabioScielzoOrtiz/Diabetes-Analysis-Detection/issues/new?title=Issue%20on%20page%20%2FDiabetes_Analysis_Detection.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Diabetes_Analysis_Detection.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Diabetes Analysis and Detection with Statistics and Machine Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#objective-statement"><strong>Objective Statement</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#requirements"><strong>Requirements</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction"><strong>Introduction</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data"><strong>Data</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reading"><strong>Reading</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape"><strong>Shape</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-types"><strong>Data types</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unique-values"><strong>Unique values</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#missing-values"><strong>Missing values</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eda"><strong>EDA</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#descriptive-summary"><strong>Descriptive summary</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#outliers"><strong>Outliers</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#frequency-table"><strong>Frequency table</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization"><strong>Visualization</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#histogram-matrix"><strong>Histogram matrix</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#boxplot-matrix"><strong>Boxplot matrix</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ecdfplot-matrix"><strong>ECDFplot matrix</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#barplot-matrix"><strong>Barplot matrix</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#response-vs-predictors-analysis"><strong>Response vs Predictors Analysis</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#response-vs-quantitative-predictors"><strong>Response vs Quantitative Predictors</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-quant-cat-descriptive-summary"><strong>Cross quant-cat descriptive summary</strong></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#response-vs-categorical-predictors"><strong>Response vs Categorical Predictors</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-contingence-table"><strong>Conditional contingence table</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-of-conditional-contingence-table"><strong>Visualization of conditional contingence table</strong></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intro-to-machine-learning-with-sklearn"><strong>Intro to Machine Learning with <code class="docutils literal notranslate"><span class="pre">Sklearn</span></code></strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-response-and-predictors"><strong>Defining the response and predictors</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-outer-evaluation-train-test-split"><strong>Defining outer evaluation: train-test split</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-inner-evaluation-k-fold-cross-validation"><strong>Defining inner evaluation: K-Fold Cross Validation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-some-models"><strong>Defining some models</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applying-inner-evaluation"><strong>Applying inner evaluation</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#without-hpo"><strong>Without HPO</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#with-hpo"><strong>With HPO</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#grid-search"><strong>Grid Search</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#random-search"><strong>Random Search</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#optuna"><strong>Optuna</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#applying-hpo-to-all-the-models-together"><strong>Applying HPO to all the models together</strong></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#selecting-the-best-model"><strong>Selecting the best model</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applying-outer-evaluation"><strong>Applying outer evaluation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ml-with-pipelines"><strong>ML with Pipelines</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-machine-learning-workflow"><strong>Advanced Machine Learning Workflow</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"><strong>Defining the response and predictors</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2"><strong>Defining outer evaluation: train-test split</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3"><strong>Defining inner evaluation: K-Fold Cross Validation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-pipelines"><strong>Defining the Pipelines</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4"><strong>Applying inner evaluation</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#grids-for-hpo"><strong>Grids for HPO</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hyper-parameter-optimization-hpo"><strong>Hyper-parameter Optimization (HPO)</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5"><strong>Selecting the best model</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6"><strong>Applying outer evaluation</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-future-performance"><strong>Estimation of future performance</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix"><strong>Confusion matrix</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#improving-the-performance-using-imblearn"><strong>Improving the performance using <code class="docutils literal notranslate"><span class="pre">imblearn</span></code></strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pipelines"><strong>Pipelines</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7"><strong>Applying inner evaluation</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id8"><strong>Hyper-parameter optimization (HPO)</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id9"><strong>Selecting the best model</strong></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id10"><strong>Applying outer evaluation</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id11"><strong>Estimation of future performance</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id12"><strong>Confusion matrix</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trying-with-sequential-feature-selection-based-on-logistic-regression"><strong>Trying with Sequential Feature Selection based on Logistic Regression</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trying-with-stacking"><strong>Trying with Stacking</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#selecting-the-best-overall-model"><strong>Selecting the best overall model</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-predictions"><strong>Probabilistic predictions</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-the-best-model"><strong>Saving the best model</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-new-data"><strong>Predicting new data</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-with-logistic-regression"><strong>Interpretation with Logistic Regression</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusions"><strong>Conclusions</strong></a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="diabetes-analysis-and-detection-with-statistics-and-machine-learning">
<h1><strong>Diabetes Analysis and Detection with Statistics and Machine Learning</strong><a class="headerlink" href="#diabetes-analysis-and-detection-with-statistics-and-machine-learning" title="Link to this heading">#</a></h1>
<section id="objective-statement">
<h2><strong>Objective Statement</strong><a class="headerlink" href="#objective-statement" title="Link to this heading">#</a></h2>
<p>The primary objective of this project is to develop a predictive model capable of classifying individuals based on their <strong>diabetes status</strong>, leveraging a variety of health-related and sociodemographic factors. This endeavor seeks not only to categorize individuals as diabetic or non-diabetic but also aims to quantify the risk of diabetes through probabilistic predictions.</p>
<p>This constitutes a supervised learning task, wherein we employ a dataset with predefined labels <strong>(diabetes presence or absence)</strong> to train our model. The nature of the response variable—binary, indicating the presence or absence of diabetes—positions this as a <strong>binary classification problem</strong>.</p>
<p>To achieve these objectives, we will harness the power of <strong>Machine Learning techniques</strong>. These methodologies will enable us to sift through the complex interplay of variables and identify patterns that significantly contribute to the likelihood of diabetes, thus facilitating more accurate predictions and risk assessments.</p>
</section>
<section id="requirements">
<h2><strong>Requirements</strong><a class="headerlink" href="#requirements" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">polars</span> <span class="k">as</span> <span class="nn">pl</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_theme</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">StratifiedKFold</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OrdinalEncoder</span><span class="p">,</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">ColumnTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">,</span> <span class="n">BaggingClassifier</span><span class="p">,</span> <span class="n">GradientBoostingClassifier</span><span class="p">,</span> <span class="n">ExtraTreesClassifier</span><span class="p">,</span> <span class="n">StackingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">balanced_accuracy_score</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">precision_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">GridSearchCV</span><span class="p">,</span> <span class="n">RandomizedSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span><span class="p">,</span> <span class="n">f_classif</span>
<span class="kn">from</span> <span class="nn">imblearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span> <span class="k">as</span> <span class="n">ImblearnPipeline</span>
<span class="kn">from</span> <span class="nn">imblearn.under_sampling</span> <span class="kn">import</span> <span class="n">RandomUnderSampler</span><span class="p">,</span> <span class="n">NearMiss</span>
<span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">RandomOverSampler</span><span class="p">,</span> <span class="n">SMOTE</span>
<span class="kn">from</span> <span class="nn">imblearn.combine</span> <span class="kn">import</span> <span class="n">SMOTETomek</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">import</span> <span class="nn">joblib</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;C:/Users/fscielzo/Documents/DataScience-GitHub/EDA&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">EDA</span> <span class="kn">import</span> <span class="n">dtypes_df</span><span class="p">,</span> <span class="n">change_type</span><span class="p">,</span> <span class="n">prop_cols_nulls</span><span class="p">,</span> <span class="n">corr_matrix</span><span class="p">,</span> <span class="n">outliers_table</span><span class="p">,</span> <span class="n">histogram_matrix</span><span class="p">,</span> <span class="n">barplot_matrix</span><span class="p">,</span> <span class="n">scatter_matrix</span><span class="p">,</span> <span class="n">quant_to_cat</span><span class="p">,</span> <span class="n">summary</span><span class="p">,</span> <span class="n">histogram</span><span class="p">,</span> <span class="n">freq_table</span><span class="p">,</span> <span class="n">boxplot</span><span class="p">,</span> <span class="n">ecdfplot</span><span class="p">,</span> <span class="n">boxplot</span><span class="p">,</span> <span class="n">boxplot_matrix</span><span class="p">,</span> <span class="n">boxplot_2D_matrix</span><span class="p">,</span> <span class="n">stripplot_matrix</span><span class="p">,</span> <span class="n">histogram_2D_matrix</span><span class="p">,</span> <span class="n">ecdf_2D_matrix</span><span class="p">,</span> <span class="n">cross_quant_cat_summary</span><span class="p">,</span> <span class="n">contingency_table_2D</span><span class="p">,</span> <span class="n">ecdf_matrix</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;C:\Users\fscielzo\Documents\DataScience-GitHub\Regression\ML&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">PyML</span> <span class="kn">import</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">scaler</span><span class="p">,</span> <span class="n">features_selector</span><span class="p">,</span> <span class="n">imputer</span><span class="p">,</span> <span class="n">SimpleEvaluation</span><span class="p">,</span> <span class="n">predictive_plots</span><span class="p">,</span> <span class="n">predictive_intervals</span><span class="p">,</span> <span class="n">OptunaSearchCV</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Why Polars instead of Pandas?</strong></p>
<p><a class="github reference external" href="https://github.com/FabioScielzoOrtiz/Spark_Master_Plot/blob/main/Spark_Pandas_Polars.jpg">FabioScielzoOrtiz/Spark_Master_Plot</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">diabetes_df = pl.read_csv(&#39;diabetes_NaNs.csv&#39;)</span>

<span class="sd">replace_dict = {}</span>
<span class="sd">replace_dict[&#39;Income&#39;] = {&#39;1.0&#39;: &#39;[0, 10k)&#39;, &#39;2.0&#39;: &#39;[10k,15k)&#39;, &#39;3.0&#39;: &#39;[15k, 20k)&#39;,</span>
<span class="sd">                          &#39;4.0&#39;: &#39;[20k,25k)&#39;, &#39;5.0&#39;: &#39;[25k,35k)&#39;, &#39;6.0&#39;: &#39;[35k,50k)&#39;, </span>
<span class="sd">                          &#39;7.0&#39;: &#39;[50k,75k)&#39;, &#39;8.0&#39;: &#39;[75k, inf)&#39;}</span>
<span class="sd">replace_dict[&#39;Education&#39;] = {&#39;1.0&#39;: &#39;Never&#39;, &#39;2.0&#39;: &#39;Elementary&#39;, &#39;3.0&#39;: &#39;SomeHighSchool&#39;,</span>
<span class="sd">                          &#39;4.0&#39;: &#39;HighSchool&#39;, &#39;5.0&#39;: &#39;SomeCollege&#39;, &#39;6.0&#39;: &#39;CollegeGraduate&#39;}</span>
<span class="sd">replace_dict[&#39;Age&#39;] = {&#39;1.0&#39;: &#39;[18, 25)&#39;, &#39;2.0&#39;: &#39;[25,30)&#39;, &#39;3.0&#39;: &#39;[30,35)&#39;,</span>
<span class="sd">                       &#39;4.0&#39;: &#39;[35,40)&#39;, &#39;5.0&#39;: &#39;[40,45)&#39;, &#39;6.0&#39;: &#39;[45,50)&#39;, &#39;7.0&#39;: &#39;[50,55)&#39;,</span>
<span class="sd">                       &#39;8.0&#39;: &#39;[55,60)&#39;, &#39;9.0&#39;: &#39;[60,65)&#39;, &#39;10.0&#39;: &#39;[65,70)&#39;, &#39;11.0&#39;: &#39;[70,75)&#39;,</span>
<span class="sd">                       &#39;12.0&#39;: &#39;[75,80)&#39;, &#39;13.0&#39;: &#39;[80,inf)&#39;}</span>
<span class="sd">replace_dict[&#39;Sex&#39;] = {&#39;0.0&#39;: &#39;Female&#39;, &#39;1.0&#39;: &#39;Male&#39;}</span>
<span class="sd">replace_dict[&#39;DiffWalk&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}</span>
<span class="sd">replace_dict[&#39;GenHlth&#39;] = {&#39;0.0&#39;: &#39;Excellent&#39;, &#39;1.0&#39;: &#39;VeryGood&#39;, &#39;2.0&#39;: &#39;Good&#39;,</span>
<span class="sd">                          &#39;3.0&#39;: &#39;Fair&#39;, &#39;4.0&#39;: &#39;Poor&#39;}</span>
<span class="sd">replace_dict[&#39;NoDocbcCost&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}</span>
<span class="sd">replace_dict[&#39;AnyHealthcare&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}</span>
<span class="sd">replace_dict[&#39;HvyAlcoholConsump&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}</span>
<span class="sd">replace_dict[&#39;Veggies&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}</span>
<span class="sd">replace_dict[&#39;Fruits&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}</span>
<span class="sd">replace_dict[&#39;PhysActivity&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}</span>
<span class="sd">replace_dict[&#39;HeartDiseaseorAttack&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}</span>
<span class="sd">replace_dict[&#39;Stroke&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}</span>
<span class="sd">replace_dict[&#39;Smoker&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}</span>
<span class="sd">replace_dict[&#39;CholCheck&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}</span>
<span class="sd">replace_dict[&#39;HighChol&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}</span>
<span class="sd">replace_dict[&#39;HighBP&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}</span>
<span class="sd">replace_dict[&#39;Diabetes_binary&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}</span>

<span class="sd">for col in replace_dict.keys():</span>
<span class="sd">    diabetes_df = diabetes_df.with_columns(diabetes_df[col].cast(str).alias(col))</span>
<span class="sd">    diabetes_df = diabetes_df.with_columns(pl.col(col).replace(replace_dict[col]).alias(col))</span>

<span class="sd"># diabetes_df.write_csv(&#39;diabetes_session03.csv&#39;)</span>
<span class="sd">&#39;&#39;&#39;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&quot;\ndiabetes_df = pl.read_csv(&#39;diabetes_NaNs.csv&#39;)\n\nreplace_dict = {}\nreplace_dict[&#39;Income&#39;] = {&#39;1.0&#39;: &#39;[0, 10k)&#39;, &#39;2.0&#39;: &#39;[10k,15k)&#39;, &#39;3.0&#39;: &#39;[15k, 20k)&#39;,\n                          &#39;4.0&#39;: &#39;[20k,25k)&#39;, &#39;5.0&#39;: &#39;[25k,35k)&#39;, &#39;6.0&#39;: &#39;[35k,50k)&#39;, \n                          &#39;7.0&#39;: &#39;[50k,75k)&#39;, &#39;8.0&#39;: &#39;[75k, inf)&#39;}\nreplace_dict[&#39;Education&#39;] = {&#39;1.0&#39;: &#39;Never&#39;, &#39;2.0&#39;: &#39;Elementary&#39;, &#39;3.0&#39;: &#39;SomeHighSchool&#39;,\n                          &#39;4.0&#39;: &#39;HighSchool&#39;, &#39;5.0&#39;: &#39;SomeCollege&#39;, &#39;6.0&#39;: &#39;CollegeGraduate&#39;}\nreplace_dict[&#39;Age&#39;] = {&#39;1.0&#39;: &#39;[18, 25)&#39;, &#39;2.0&#39;: &#39;[25,30)&#39;, &#39;3.0&#39;: &#39;[30,35)&#39;,\n                       &#39;4.0&#39;: &#39;[35,40)&#39;, &#39;5.0&#39;: &#39;[40,45)&#39;, &#39;6.0&#39;: &#39;[45,50)&#39;, &#39;7.0&#39;: &#39;[50,55)&#39;,\n                       &#39;8.0&#39;: &#39;[55,60)&#39;, &#39;9.0&#39;: &#39;[60,65)&#39;, &#39;10.0&#39;: &#39;[65,70)&#39;, &#39;11.0&#39;: &#39;[70,75)&#39;,\n                       &#39;12.0&#39;: &#39;[75,80)&#39;, &#39;13.0&#39;: &#39;[80,inf)&#39;}\nreplace_dict[&#39;Sex&#39;] = {&#39;0.0&#39;: &#39;Female&#39;, &#39;1.0&#39;: &#39;Male&#39;}\nreplace_dict[&#39;DiffWalk&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}\nreplace_dict[&#39;GenHlth&#39;] = {&#39;0.0&#39;: &#39;Excellent&#39;, &#39;1.0&#39;: &#39;VeryGood&#39;, &#39;2.0&#39;: &#39;Good&#39;,\n                          &#39;3.0&#39;: &#39;Fair&#39;, &#39;4.0&#39;: &#39;Poor&#39;}\nreplace_dict[&#39;NoDocbcCost&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}\nreplace_dict[&#39;AnyHealthcare&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}\nreplace_dict[&#39;HvyAlcoholConsump&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}\nreplace_dict[&#39;Veggies&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}\nreplace_dict[&#39;Fruits&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}\nreplace_dict[&#39;PhysActivity&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}\nreplace_dict[&#39;HeartDiseaseorAttack&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}\nreplace_dict[&#39;Stroke&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}\nreplace_dict[&#39;Smoker&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}\nreplace_dict[&#39;CholCheck&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}\nreplace_dict[&#39;HighChol&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}\nreplace_dict[&#39;HighBP&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}\nreplace_dict[&#39;Diabetes_binary&#39;] = {&#39;0.0&#39;: &#39;No&#39;, &#39;1.0&#39;: &#39;Yes&#39;}\n\nfor col in replace_dict.keys():\n    diabetes_df = diabetes_df.with_columns(diabetes_df[col].cast(str).alias(col))\n    diabetes_df = diabetes_df.with_columns(pl.col(col).replace(replace_dict[col]).alias(col))\n\n# diabetes_df.write_csv(&#39;diabetes_session03.csv&#39;)\n&quot;
</pre></div>
</div>
</div>
</div>
</section>
<section id="introduction">
<h2><strong>Introduction</strong><a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>Diabetes stands as a paramount challenge within the realm of public health in the United States, affecting millions and imposing a considerable economic toll. This chronic condition undermines the body’s ability to regulate blood glucose levels effectively, which is critical for energy production and overall health. Following digestion, the breakdown of foods into sugars results in their release into the bloodstream, triggering the pancreas to secrete insulin. This hormone is essential for cells to utilize blood sugar for energy. The hallmark of diabetes is the body’s inadequate insulin production or utilization, leading to potential reductions in both quality of life and life expectancy.</p>
<p>The disease is associated with severe complications, including heart disease, vision loss, lower-limb amputations, and kidney disease, stemming from prolonged elevated blood sugar levels. Although incurable, diabetes management strategies—such as weight management, healthy eating, physical activity, and medical treatment—can significantly reduce its adverse effects. Early detection is crucial, enabling lifestyle modifications and enhanced treatment effectiveness, thus underscoring the importance of predictive models for assessing diabetes risk.</p>
<p>Acknowledging the magnitude of diabetes is vital. The Centers for Disease Control and Prevention (CDC) reports that, as of 2018, approximately 34.2 million Americans live with diabetes, and an additional 88 million are in a prediabetic state. Alarmingly, a significant proportion of individuals with diabetes or prediabetes remain unaware of their condition, with estimates suggesting that 1 in 5 diabetics and nearly 80% of prediabetics are undiagnosed. While diabetes manifests in various forms, type II diabetes is the most prevalent, influenced by a range of factors including age, education, income, geographic location, race, and other social determinants of health. The socioeconomic disparity in diabetes prevalence is noteworthy, disproportionately affecting those of lower socioeconomic status.</p>
<p>The economic implications of diabetes are staggering, with direct medical costs for diagnosed diabetes reaching approximately <span class="math notranslate nohighlight">\(327 billion annually. When accounting for undiagnosed diabetes and prediabetes, total expenditures near \)</span>400 billion, highlighting the urgent need for effective public health strategies and interventions to combat this escalating health crisis.</p>
</section>
<section id="data">
<h2><strong>Data</strong><a class="headerlink" href="#data" title="Link to this heading">#</a></h2>
<p>The Behavioral Risk Factor Surveillance System (BRFSS), an annual health-related telephone survey administered by the Centers for Disease Control and Prevention (CDC), systematically gathers data from over 400,000 Americans. This comprehensive survey, initiated in 1984, explores health-related risk behaviors, chronic health conditions, and the utilization of preventive services.</p>
<p>For the purposes of this study, we have utilized a subset of the BRFSS 2015 dataset, made available on Kaggle. This specific subset encompasses responses from 441,455 participants and includes a rich array of 330 features. These features comprise direct questions posed to respondents as well as derived variables calculated from the responses of individual participants.</p>
<p>The <a class="reference external" href="https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset">dataset</a> in question encapsulates 250,000 responses from the 2015 iteration of the CDC’s BRFSS and features 22 variables meticulously extracted from the survey. The variables included offer a comprehensive overview of the participants’ health status and behaviors, serving as the foundation for this project’s analysis. The table below conceptually summarizes the variables within the dataset, delineating the scope of our exploration and analysis throughout this project.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Variable Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">Diabetes</span></code></p></td>
<td><p>0 = no diabetes ; 1 = prediabetes or diabetes</p></td>
<td><p>Binary</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">HighBP</span></code></p></td>
<td><p>0 = no high blood pressure ; 1 = high blood pressure</p></td>
<td><p>Binary</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">HighChol</span></code></p></td>
<td><p>0 = no high cholesterol ; 1 = high cholesterol</p></td>
<td><p>Binary</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">CholCheck</span></code></p></td>
<td><p>0 = no cholesterol check in 5 years ; 1 = yes cholesterol check in 5 years</p></td>
<td><p>Binary</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">BMI</span></code></p></td>
<td><p>Body Mass Index</p></td>
<td><p>Quantitative</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">Smoker</span></code></p></td>
<td><p>Have you smoked at least 100 cigarettes in your entire life? [Note: 5 packs = 100 cigarettes] 0 = no ; 1 = yes</p></td>
<td><p>Binary</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">Stroke</span></code></p></td>
<td><p>Have you had a stroke (ictus). 0 = no ; 1 = yes</p></td>
<td><p>Binary</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">HeartDiseaseorAttack</span></code></p></td>
<td><p>coronary heart disease (CHD) or myocardial infarction (MI). 0 = no ; 1 = yes</p></td>
<td><p>Binary</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">PhysActivity</span></code></p></td>
<td><p>physical activity in past 30 days - not including job. 0 = no ; 1 = yes</p></td>
<td><p>Binary</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">Fruits</span></code></p></td>
<td><p>Consume Fruit 1 or more times per day. 0 = no ; 1 = yes</p></td>
<td><p>Binary</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">Veggies</span></code></p></td>
<td><p>Consume Vegetables 1 or more times per day. 0 = no ; 1 = yes</p></td>
<td><p>Binary</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">HvyAlcoholConsump</span></code></p></td>
<td><p>(adult men &gt;=14 drinks per week and adult women&gt;=7 drinks per week). 0 = no ; 1 = yes</p></td>
<td><p>Binary</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">AnyHealthcare</span></code></p></td>
<td><p>Have any kind of health care coverage, including health insurance, prepaid plans such as HMO, etc. 0 = no ; 1 = yes</p></td>
<td><p>Binary</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">NoDocbcCost</span></code></p></td>
<td><p>Was there a time in the past 12 months when you needed to see a doctor but could not because of cost? 0 = no ; 1 = yes</p></td>
<td><p>Binary</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">GenHlth</span></code></p></td>
<td><p>Would you say that in general your health is: scale 1-5. 1 = excellent ; 2 = very good ; 3 = good ; 4 = fair ; 5 = poor</p></td>
<td><p>Multiclass</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">MentHlth</span></code></p></td>
<td><p>Now thinking about your mental health, which includes stress, depression, and problems with emotions, for how many days during the past 30 days was your mental health not good? scale 1-30 days</p></td>
<td><p>Quantitative</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">PhysHlth</span></code></p></td>
<td><p>Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good? scale 1-30 days</p></td>
<td><p>Quantitative</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">DiffWalk</span></code></p></td>
<td><p>Do you have serious difficulty walking or climbing stairs? 0 = no ; 1 = yes</p></td>
<td><p>Binary</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">Sex</span></code></p></td>
<td><p>0 = female ; 1 = male</p></td>
<td><p>Binary</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">Age</span></code></p></td>
<td><p>13-level age category. 0 = [18, 24] ; 1 = [25 ,29 ]   ; 2 = [30 ,34 ]   ; 3 = [35 ,39 ]    ; 4 = [40 , 44]    ; 5 = [ 45, 49]    ; 6 = [50 , 54]    ; 7 = [55 , 59]    ; 8 = [60 , 64]  ; 9 = [65 , 69] ; 10 = [70 , 74]  ; 11 = [75 , 79]  ; 12 = 80 or older</p></td>
<td><p>Multiclass</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">Education</span></code></p></td>
<td><p>Education level scale 0-5. 0 = Never attended school or only kindergarten ; 1 = Elementary ; 2 = Some high school ; 3 = High school graduate ; 4 = Some college or technical school ; 5 = College graduate</p></td>
<td><p>Multiclass</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">Income</span></code></p></td>
<td><p>Income scale scale 0-7. 0 = lower than 10k ; 1 = [10k, 15k) ; 2 = [15k, 20k) ; 3 = [20k, 25k) ; 4 = [25k, 35k) ; 5 = [35k, 50k) ; 6 = [50k, 75k) ; 7 = higher than 75k</p></td>
<td><p>Multiclass</p></td>
</tr>
</tbody>
</table>
<p>The data has been obtained from <code class="docutils literal notranslate"><span class="pre">Kaggle</span></code>:  <a class="reference external" href="https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset">https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset</a></p>
<section id="reading">
<h3><strong>Reading</strong><a class="headerlink" href="#reading" title="Link to this heading">#</a></h3>
<p>First of all, we read the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">diabetes_df</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;diabetes_session03.csv&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">diabetes_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (5, 22)</small><table border="1" class="dataframe"><thead><tr><th>Diabetes_binary</th><th>HighBP</th><th>HighChol</th><th>CholCheck</th><th>BMI</th><th>Smoker</th><th>Stroke</th><th>HeartDiseaseorAttack</th><th>PhysActivity</th><th>Fruits</th><th>Veggies</th><th>HvyAlcoholConsump</th><th>AnyHealthcare</th><th>NoDocbcCost</th><th>GenHlth</th><th>MentHlth</th><th>PhysHlth</th><th>DiffWalk</th><th>Sex</th><th>Age</th><th>Education</th><th>Income</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;No&quot;</td><td>&quot;Yes&quot;</td><td>&quot;Yes&quot;</td><td>&quot;Yes&quot;</td><td>40.0</td><td>&quot;Yes&quot;</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;Yes&quot;</td><td>&quot;No&quot;</td><td>&quot;Yes&quot;</td><td>&quot;No&quot;</td><td>&quot;5.0&quot;</td><td>18.0</td><td>15.0</td><td>&quot;Yes&quot;</td><td>&quot;Female&quot;</td><td>&quot;[60,65)&quot;</td><td>&quot;HighSchool&quot;</td><td>&quot;[15k, 20k)&quot;</td></tr><tr><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>25.0</td><td>&quot;Yes&quot;</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;Yes&quot;</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;Yes&quot;</td><td>&quot;Fair&quot;</td><td>0.0</td><td>0.0</td><td>&quot;No&quot;</td><td>&quot;Female&quot;</td><td>&quot;[50,55)&quot;</td><td>&quot;CollegeGraduat…</td><td>&quot;[0, 10k)&quot;</td></tr><tr><td>&quot;No&quot;</td><td>&quot;Yes&quot;</td><td>&quot;Yes&quot;</td><td>&quot;Yes&quot;</td><td>28.0</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;Yes&quot;</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;Yes&quot;</td><td>&quot;Yes&quot;</td><td>&quot;5.0&quot;</td><td>30.0</td><td>30.0</td><td>&quot;Yes&quot;</td><td>&quot;Female&quot;</td><td>&quot;[60,65)&quot;</td><td>&quot;HighSchool&quot;</td><td>&quot;[75k, inf)&quot;</td></tr><tr><td>&quot;No&quot;</td><td>&quot;Yes&quot;</td><td>&quot;No&quot;</td><td>&quot;Yes&quot;</td><td>27.0</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;Yes&quot;</td><td>&quot;Yes&quot;</td><td>&quot;Yes&quot;</td><td>&quot;No&quot;</td><td>&quot;Yes&quot;</td><td>&quot;No&quot;</td><td>&quot;Good&quot;</td><td>0.0</td><td>0.0</td><td>&quot;No&quot;</td><td>&quot;Female&quot;</td><td>&quot;[70,75)&quot;</td><td>&quot;SomeHighSchool…</td><td>&quot;[35k,50k)&quot;</td></tr><tr><td>&quot;No&quot;</td><td>&quot;Yes&quot;</td><td>&quot;Yes&quot;</td><td>&quot;Yes&quot;</td><td>24.0</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;Yes&quot;</td><td>&quot;Yes&quot;</td><td>&quot;Yes&quot;</td><td>&quot;No&quot;</td><td>&quot;Yes&quot;</td><td>&quot;No&quot;</td><td>&quot;Good&quot;</td><td>3.0</td><td>0.0</td><td>&quot;No&quot;</td><td>&quot;Female&quot;</td><td>&quot;[70,75)&quot;</td><td>&quot;SomeCollege&quot;</td><td>&quot;[20k,25k)&quot;</td></tr></tbody></table></div></div></div>
</div>
</section>
<section id="shape">
<h3><strong>Shape</strong><a class="headerlink" href="#shape" title="Link to this heading">#</a></h3>
<p>We display the shape of the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">diabetes_df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(253680, 22)
</pre></div>
</div>
</div>
</div>
</section>
<section id="data-types">
<h3><strong>Data types</strong><a class="headerlink" href="#data-types" title="Link to this heading">#</a></h3>
<p>We check the variables data types according to <code class="docutils literal notranslate"><span class="pre">polars</span></code> structure.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pl</span><span class="o">.</span><span class="n">Config</span><span class="p">(</span><span class="n">tbl_rows</span><span class="o">=</span><span class="mi">22</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">dtypes_df</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">diabetes_df</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>shape: (22, 2)
┌──────────────────────┬─────────────┐
│ Columns              ┆ Python_type │
│ ---                  ┆ ---         │
│ str                  ┆ object      │
╞══════════════════════╪═════════════╡
│ Diabetes_binary      ┆ Utf8        │
│ HighBP               ┆ Utf8        │
│ HighChol             ┆ Utf8        │
│ CholCheck            ┆ Utf8        │
│ BMI                  ┆ Float64     │
│ Smoker               ┆ Utf8        │
│ Stroke               ┆ Utf8        │
│ HeartDiseaseorAttack ┆ Utf8        │
│ PhysActivity         ┆ Utf8        │
│ Fruits               ┆ Utf8        │
│ Veggies              ┆ Utf8        │
│ HvyAlcoholConsump    ┆ Utf8        │
│ AnyHealthcare        ┆ Utf8        │
│ NoDocbcCost          ┆ Utf8        │
│ GenHlth              ┆ Utf8        │
│ MentHlth             ┆ Float64     │
│ PhysHlth             ┆ Float64     │
│ DiffWalk             ┆ Utf8        │
│ Sex                  ┆ Utf8        │
│ Age                  ┆ Utf8        │
│ Education            ┆ Utf8        │
│ Income               ┆ Utf8        │
└──────────────────────┴─────────────┘
</pre></div>
</div>
</div>
</div>
</section>
<section id="unique-values">
<h3><strong>Unique values</strong><a class="headerlink" href="#unique-values" title="Link to this heading">#</a></h3>
<p>We compute the unique values of each variable, what is useful to get a more real idea of their nature.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_unique</span> <span class="o">=</span> <span class="p">{}</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of unique values:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">diabetes_df</span><span class="o">.</span><span class="n">columns</span> <span class="p">:</span>
     <span class="n">n_unique</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">diabetes_df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
     <span class="nb">print</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">n_unique</span><span class="p">[</span><span class="n">col</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of unique values:

Diabetes_binary : 2
HighBP : 2
HighChol : 2
CholCheck : 3
BMI : 84
Smoker : 2
Stroke : 2
HeartDiseaseorAttack : 2
PhysActivity : 2
Fruits : 2
Veggies : 3
HvyAlcoholConsump : 2
AnyHealthcare : 2
NoDocbcCost : 3
GenHlth : 5
MentHlth : 32
PhysHlth : 32
DiffWalk : 2
Sex : 3
Age : 14
Education : 6
Income : 8
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">diabetes_df</span><span class="o">.</span><span class="n">columns</span> <span class="p">:</span>
    <span class="n">display</span><span class="p">(</span><span class="n">diabetes_df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2,)</small><table border="1" class="dataframe"><thead><tr><th>Diabetes_binary</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;No&quot;</td></tr><tr><td>&quot;Yes&quot;</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2,)</small><table border="1" class="dataframe"><thead><tr><th>HighBP</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;Yes&quot;</td></tr><tr><td>&quot;No&quot;</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2,)</small><table border="1" class="dataframe"><thead><tr><th>HighChol</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;Yes&quot;</td></tr><tr><td>&quot;No&quot;</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (3,)</small><table border="1" class="dataframe"><thead><tr><th>CholCheck</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;No&quot;</td></tr><tr><td>null</td></tr><tr><td>&quot;Yes&quot;</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (84,)</small><table border="1" class="dataframe"><thead><tr><th>BMI</th></tr><tr><td>f64</td></tr></thead><tbody><tr><td>12.0</td></tr><tr><td>13.0</td></tr><tr><td>14.0</td></tr><tr><td>15.0</td></tr><tr><td>16.0</td></tr><tr><td>17.0</td></tr><tr><td>18.0</td></tr><tr><td>19.0</td></tr><tr><td>20.0</td></tr><tr><td>21.0</td></tr><tr><td>22.0</td></tr><tr><td>23.0</td></tr><tr><td>&hellip;</td></tr><tr><td>84.0</td></tr><tr><td>85.0</td></tr><tr><td>86.0</td></tr><tr><td>87.0</td></tr><tr><td>88.0</td></tr><tr><td>89.0</td></tr><tr><td>90.0</td></tr><tr><td>91.0</td></tr><tr><td>92.0</td></tr><tr><td>95.0</td></tr><tr><td>96.0</td></tr><tr><td>98.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2,)</small><table border="1" class="dataframe"><thead><tr><th>Smoker</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;Yes&quot;</td></tr><tr><td>&quot;No&quot;</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2,)</small><table border="1" class="dataframe"><thead><tr><th>Stroke</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;Yes&quot;</td></tr><tr><td>&quot;No&quot;</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2,)</small><table border="1" class="dataframe"><thead><tr><th>HeartDiseaseorAttack</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;No&quot;</td></tr><tr><td>&quot;Yes&quot;</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2,)</small><table border="1" class="dataframe"><thead><tr><th>PhysActivity</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;Yes&quot;</td></tr><tr><td>&quot;No&quot;</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2,)</small><table border="1" class="dataframe"><thead><tr><th>Fruits</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;Yes&quot;</td></tr><tr><td>&quot;No&quot;</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (3,)</small><table border="1" class="dataframe"><thead><tr><th>Veggies</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>null</td></tr><tr><td>&quot;No&quot;</td></tr><tr><td>&quot;Yes&quot;</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2,)</small><table border="1" class="dataframe"><thead><tr><th>HvyAlcoholConsump</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;Yes&quot;</td></tr><tr><td>&quot;No&quot;</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2,)</small><table border="1" class="dataframe"><thead><tr><th>AnyHealthcare</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;No&quot;</td></tr><tr><td>&quot;Yes&quot;</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (3,)</small><table border="1" class="dataframe"><thead><tr><th>NoDocbcCost</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;Yes&quot;</td></tr><tr><td>&quot;No&quot;</td></tr><tr><td>null</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (5,)</small><table border="1" class="dataframe"><thead><tr><th>GenHlth</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;Poor&quot;</td></tr><tr><td>&quot;VeryGood&quot;</td></tr><tr><td>&quot;Fair&quot;</td></tr><tr><td>&quot;Good&quot;</td></tr><tr><td>&quot;5.0&quot;</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (32,)</small><table border="1" class="dataframe"><thead><tr><th>MentHlth</th></tr><tr><td>f64</td></tr></thead><tbody><tr><td>null</td></tr><tr><td>0.0</td></tr><tr><td>1.0</td></tr><tr><td>2.0</td></tr><tr><td>3.0</td></tr><tr><td>4.0</td></tr><tr><td>5.0</td></tr><tr><td>6.0</td></tr><tr><td>7.0</td></tr><tr><td>8.0</td></tr><tr><td>9.0</td></tr><tr><td>10.0</td></tr><tr><td>&hellip;</td></tr><tr><td>19.0</td></tr><tr><td>20.0</td></tr><tr><td>21.0</td></tr><tr><td>22.0</td></tr><tr><td>23.0</td></tr><tr><td>24.0</td></tr><tr><td>25.0</td></tr><tr><td>26.0</td></tr><tr><td>27.0</td></tr><tr><td>28.0</td></tr><tr><td>29.0</td></tr><tr><td>30.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (32,)</small><table border="1" class="dataframe"><thead><tr><th>PhysHlth</th></tr><tr><td>f64</td></tr></thead><tbody><tr><td>null</td></tr><tr><td>0.0</td></tr><tr><td>1.0</td></tr><tr><td>2.0</td></tr><tr><td>3.0</td></tr><tr><td>4.0</td></tr><tr><td>5.0</td></tr><tr><td>6.0</td></tr><tr><td>7.0</td></tr><tr><td>8.0</td></tr><tr><td>9.0</td></tr><tr><td>10.0</td></tr><tr><td>&hellip;</td></tr><tr><td>19.0</td></tr><tr><td>20.0</td></tr><tr><td>21.0</td></tr><tr><td>22.0</td></tr><tr><td>23.0</td></tr><tr><td>24.0</td></tr><tr><td>25.0</td></tr><tr><td>26.0</td></tr><tr><td>27.0</td></tr><tr><td>28.0</td></tr><tr><td>29.0</td></tr><tr><td>30.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2,)</small><table border="1" class="dataframe"><thead><tr><th>DiffWalk</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;No&quot;</td></tr><tr><td>&quot;Yes&quot;</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (3,)</small><table border="1" class="dataframe"><thead><tr><th>Sex</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>null</td></tr><tr><td>&quot;Female&quot;</td></tr><tr><td>&quot;Male&quot;</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (14,)</small><table border="1" class="dataframe"><thead><tr><th>Age</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;[25,30)&quot;</td></tr><tr><td>&quot;[75,80)&quot;</td></tr><tr><td>&quot;[45,50)&quot;</td></tr><tr><td>&quot;[35,40)&quot;</td></tr><tr><td>&quot;[70,75)&quot;</td></tr><tr><td>&quot;[80,inf)&quot;</td></tr><tr><td>&quot;[18, 25)&quot;</td></tr><tr><td>&quot;[55,60)&quot;</td></tr><tr><td>&quot;[60,65)&quot;</td></tr><tr><td>&quot;[65,70)&quot;</td></tr><tr><td>&quot;[40,45)&quot;</td></tr><tr><td>&quot;[30,35)&quot;</td></tr><tr><td>null</td></tr><tr><td>&quot;[50,55)&quot;</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (6,)</small><table border="1" class="dataframe"><thead><tr><th>Education</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;SomeHighSchool…</td></tr><tr><td>&quot;HighSchool&quot;</td></tr><tr><td>&quot;SomeCollege&quot;</td></tr><tr><td>&quot;Elementary&quot;</td></tr><tr><td>&quot;CollegeGraduat…</td></tr><tr><td>&quot;Never&quot;</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (8,)</small><table border="1" class="dataframe"><thead><tr><th>Income</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;[0, 10k)&quot;</td></tr><tr><td>&quot;[50k,75k)&quot;</td></tr><tr><td>&quot;[35k,50k)&quot;</td></tr><tr><td>&quot;[25k,35k)&quot;</td></tr><tr><td>&quot;[10k,15k)&quot;</td></tr><tr><td>&quot;[20k,25k)&quot;</td></tr><tr><td>&quot;[15k, 20k)&quot;</td></tr><tr><td>&quot;[75k, inf)&quot;</td></tr></tbody></table></div></div></div>
</div>
</section>
<section id="missing-values">
<h3><strong>Missing values</strong><a class="headerlink" href="#missing-values" title="Link to this heading">#</a></h3>
<p>Let’s get a general idea of our missing values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prop_cols_nulls</span><span class="p">(</span><span class="n">diabetes_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (1, 22)</small><table border="1" class="dataframe"><thead><tr><th>Diabetes_binary</th><th>HighBP</th><th>HighChol</th><th>CholCheck</th><th>BMI</th><th>Smoker</th><th>Stroke</th><th>HeartDiseaseorAttack</th><th>PhysActivity</th><th>Fruits</th><th>Veggies</th><th>HvyAlcoholConsump</th><th>AnyHealthcare</th><th>NoDocbcCost</th><th>GenHlth</th><th>MentHlth</th><th>PhysHlth</th><th>DiffWalk</th><th>Sex</th><th>Age</th><th>Education</th><th>Income</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.0</td><td>0.0</td><td>0.0</td><td>0.023372</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.067412</td><td>0.0</td><td>0.0</td><td>0.001336</td><td>0.0</td><td>0.029675</td><td>0.026565</td><td>0.0</td><td>0.011337</td><td>0.04188</td><td>0.0</td><td>0.0</td></tr></tbody></table></div></div></div>
</div>
<p>The proportion of missing values is quite low but it will then be decided whether to impute or eliminate the observations containing any of these.</p>
</section>
</section>
<section id="eda">
<h2><strong>EDA</strong><a class="headerlink" href="#eda" title="Link to this heading">#</a></h2>
<p>In this section we are going to do describe the variables of our data.</p>
<section id="descriptive-summary">
<h3><strong>Descriptive summary</strong><a class="headerlink" href="#descriptive-summary" title="Link to this heading">#</a></h3>
<p>First we define the list of the categorical and quantitative variables.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">quant_columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">diabetes_df</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">diabetes_df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">pl</span><span class="o">.</span><span class="n">Float64</span><span class="p">]</span>
<span class="n">cat_columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">diabetes_df</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">diabetes_df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">pl</span><span class="o">.</span><span class="n">Utf8</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Alternative </span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">len_unique = []</span>
<span class="sd">columns_df = np.array(diabetes_df.columns)</span>
<span class="sd">for col in columns_df :</span>
<span class="sd">    len_unique.append(len(diabetes_df[col].unique()))</span>
<span class="sd">len_unique = np.array(len_unique)</span>

<span class="sd">binary_columns = columns_df[len_unique == 2].tolist()</span>
<span class="sd">multi_columns = columns_df[(len_unique &gt; 2) &amp; (len_unique &lt;= 14)].tolist()</span>
<span class="sd">cat_columns = binary_columns + multi_columns</span>
<span class="sd">quant_columns = columns_df[len_unique &gt; 14].tolist()</span>
<span class="sd">&#39;&#39;&#39;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;\nlen_unique = []\ncolumns_df = np.array(diabetes_df.columns)\nfor col in columns_df :\n    len_unique.append(len(diabetes_df[col].unique()))\nlen_unique = np.array(len_unique)\n\nbinary_columns = columns_df[len_unique == 2].tolist()\nmulti_columns = columns_df[(len_unique &gt; 2) &amp; (len_unique &lt;= 14)].tolist()\ncat_columns = binary_columns + multi_columns\nquant_columns = columns_df[len_unique &gt; 14].tolist()\n&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">quant_columns</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;BMI&#39;, &#39;MentHlth&#39;, &#39;PhysHlth&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cat_columns</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;Diabetes_binary&#39;,
 &#39;HighBP&#39;,
 &#39;HighChol&#39;,
 &#39;CholCheck&#39;,
 &#39;Smoker&#39;,
 &#39;Stroke&#39;,
 &#39;HeartDiseaseorAttack&#39;,
 &#39;PhysActivity&#39;,
 &#39;Fruits&#39;,
 &#39;Veggies&#39;,
 &#39;HvyAlcoholConsump&#39;,
 &#39;AnyHealthcare&#39;,
 &#39;NoDocbcCost&#39;,
 &#39;GenHlth&#39;,
 &#39;DiffWalk&#39;,
 &#39;Sex&#39;,
 &#39;Age&#39;,
 &#39;Education&#39;,
 &#39;Income&#39;]
</pre></div>
</div>
</div>
</div>
<p>We will use the function <code class="docutils literal notranslate"><span class="pre">summary</span></code> to obtain a descriptive summary of both the categorical and quantitative variables.</p>
<p>This function give us a table with useful metrics for both quantitative and categorical variables.</p>
<p>For quantitative the following metrics are computed:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">n_unique</span></code>: number of unique values of the variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prop_nan</span></code>: proportion of missing values of the variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mean</span></code>: mean of the variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">std</span></code>: standard deviation of the variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">min</span></code>: minimum value of the variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Q10</span></code>: 10-quantile of the variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Q25</span></code>: 25-quantile of the variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">median</span></code>:</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Q75</span></code>: 75-quantile of the variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Q90</span></code>: 90-quantile of the variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max</span></code>: maximum value of the variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kurtosis</span></code>: the kurtosis of the variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">skew</span></code>: the skewness of the variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_outliers</span></code>: the number of outlier observations of the variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_not_outliers</span></code>: the number of not outlier observations of the variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prop_outliers</span></code>: the proportion of outlier observations of the variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prop_not_outliers</span></code>: the proportion of not outlier observations of the variable.</p></li>
</ul>
<p>And for categorical, the following:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">n_unique</span></code>: number of unique values of the variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prop_nan</span></code>: proportion of missing values of the variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mode</span></code>: the mode of the variable.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">quant_summary</span><span class="p">,</span> <span class="n">cat_summary</span> <span class="o">=</span> <span class="n">summary</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">diabetes_df</span><span class="p">,</span> <span class="n">auto_col</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                     <span class="n">quant_col_names</span><span class="o">=</span><span class="n">quant_columns</span><span class="p">,</span>
                                     <span class="n">cat_col_names</span><span class="o">=</span><span class="n">cat_columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">quant_summary</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>BMI</th>
      <th>MentHlth</th>
      <th>PhysHlth</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>n_unique</th>
      <td>84</td>
      <td>32</td>
      <td>32</td>
    </tr>
    <tr>
      <th>prop_nan</th>
      <td>0.0</td>
      <td>0.029675</td>
      <td>0.026565</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>28.382364</td>
      <td>3.185739</td>
      <td>4.244253</td>
    </tr>
    <tr>
      <th>std</th>
      <td>6.608694</td>
      <td>7.414006</td>
      <td>8.720454</td>
    </tr>
    <tr>
      <th>min</th>
      <td>12.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Q10</th>
      <td>22.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Q25</th>
      <td>24.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>median</th>
      <td>27.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Q75</th>
      <td>31.0</td>
      <td>2.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>Q90</th>
      <td>36.0</td>
      <td>10.0</td>
      <td>20.0</td>
    </tr>
    <tr>
      <th>max</th>
      <td>98.0</td>
      <td>30.0</td>
      <td>30.0</td>
    </tr>
    <tr>
      <th>kurtosis</th>
      <td>13.997233</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>skew</th>
      <td>2.121991</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>n_outliers</th>
      <td>9847</td>
      <td>35146</td>
      <td>39877</td>
    </tr>
    <tr>
      <th>n_not_outliers</th>
      <td>243833</td>
      <td>218534</td>
      <td>213803</td>
    </tr>
    <tr>
      <th>prop_outliers</th>
      <td>0.038817</td>
      <td>0.138545</td>
      <td>0.157194</td>
    </tr>
    <tr>
      <th>prop_not_outliers</th>
      <td>0.961183</td>
      <td>0.861455</td>
      <td>0.842806</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cat_summary</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Diabetes_binary</th>
      <th>HighBP</th>
      <th>HighChol</th>
      <th>CholCheck</th>
      <th>Smoker</th>
      <th>Stroke</th>
      <th>HeartDiseaseorAttack</th>
      <th>PhysActivity</th>
      <th>Fruits</th>
      <th>Veggies</th>
      <th>HvyAlcoholConsump</th>
      <th>AnyHealthcare</th>
      <th>NoDocbcCost</th>
      <th>GenHlth</th>
      <th>DiffWalk</th>
      <th>Sex</th>
      <th>Age</th>
      <th>Education</th>
      <th>Income</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>n_unique</th>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>5</td>
      <td>2</td>
      <td>2</td>
      <td>13</td>
      <td>6</td>
      <td>8</td>
    </tr>
    <tr>
      <th>prop_nan</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.023372</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.067412</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.001336</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.011337</td>
      <td>0.04188</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>mode</th>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>No</td>
      <td>Yes</td>
      <td>No</td>
      <td>Good</td>
      <td>No</td>
      <td>Female</td>
      <td>[60,65)</td>
      <td>CollegeGraduate</td>
      <td>[75k, inf)</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="outliers">
<h3><strong>Outliers</strong><a class="headerlink" href="#outliers" title="Link to this heading">#</a></h3>
<p>Next, we employ the <code class="docutils literal notranslate"><span class="pre">outliers_table</span></code> function to generate a table encompassing various metrics related to outliers within the quantitative variables.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">outliers_table</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">diabetes_df</span><span class="p">,</span> <span class="n">auto</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">col_names</span><span class="o">=</span><span class="n">quant_columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (3, 7)</small><table border="1" class="dataframe"><thead><tr><th>quant_variables</th><th>lower_bound</th><th>upper_bound</th><th>n_outliers</th><th>n_not_outliers</th><th>prop_outliers</th><th>prop_not_outliers</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;BMI&quot;</td><td>13.5</td><td>41.5</td><td>9847</td><td>243833</td><td>0.038817</td><td>0.961183</td></tr><tr><td>&quot;MentHlth&quot;</td><td>-3.0</td><td>5.0</td><td>35146</td><td>211006</td><td>0.142782</td><td>0.857218</td></tr><tr><td>&quot;PhysHlth&quot;</td><td>-4.5</td><td>7.5</td><td>39877</td><td>207064</td><td>0.161484</td><td>0.838516</td></tr></tbody></table></div></div></div>
</div>
<p>This table contain the lower and upper bound from which the outliers are defined, as well as the number and proportion of outliers and not outlier observation of each quantitative variable.</p>
</section>
<section id="frequency-table">
<h3><strong>Frequency table</strong><a class="headerlink" href="#frequency-table" title="Link to this heading">#</a></h3>
<p>Here we compute a frequency table for each categorical variable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">diabetes_df_non_cat_NaNs</span> <span class="o">=</span> <span class="n">diabetes_df</span><span class="o">.</span><span class="n">drop_nulls</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="n">cat_columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">cat_columns</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">display</span><span class="p">(</span><span class="n">freq_table</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">diabetes_df_non_cat_NaNs</span><span class="p">[</span><span class="n">x</span><span class="p">]))</span>  
    <span class="k">except</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Couldn</span><span class="se">\&#39;</span><span class="s1">t be computed for </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 5)</small><table border="1" class="dataframe"><thead><tr><th>Diabetes_binary: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;No&quot;</td><td>188001</td><td>0.8603</td><td>188001</td><td>0.860283</td></tr><tr><td>&quot;Yes&quot;</td><td>30533</td><td>0.1397</td><td>218534</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 5)</small><table border="1" class="dataframe"><thead><tr><th>HighBP: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;No&quot;</td><td>124818</td><td>0.5712</td><td>124818</td><td>0.571161</td></tr><tr><td>&quot;Yes&quot;</td><td>93716</td><td>0.4288</td><td>218534</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 5)</small><table border="1" class="dataframe"><thead><tr><th>HighChol: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;No&quot;</td><td>125796</td><td>0.5756</td><td>125796</td><td>0.575636</td></tr><tr><td>&quot;Yes&quot;</td><td>92738</td><td>0.4244</td><td>218534</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 5)</small><table border="1" class="dataframe"><thead><tr><th>CholCheck: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;No&quot;</td><td>8150</td><td>0.0373</td><td>8150</td><td>0.037294</td></tr><tr><td>&quot;Yes&quot;</td><td>210384</td><td>0.9627</td><td>218534</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 5)</small><table border="1" class="dataframe"><thead><tr><th>Smoker: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;No&quot;</td><td>121774</td><td>0.5572</td><td>121774</td><td>0.557231</td></tr><tr><td>&quot;Yes&quot;</td><td>96760</td><td>0.4428</td><td>218534</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 5)</small><table border="1" class="dataframe"><thead><tr><th>Stroke: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;No&quot;</td><td>209632</td><td>0.9593</td><td>209632</td><td>0.959265</td></tr><tr><td>&quot;Yes&quot;</td><td>8902</td><td>0.0407</td><td>218534</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 5)</small><table border="1" class="dataframe"><thead><tr><th>HeartDiseaseorAttack: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;No&quot;</td><td>197971</td><td>0.9059</td><td>197971</td><td>0.905905</td></tr><tr><td>&quot;Yes&quot;</td><td>20563</td><td>0.0941</td><td>218534</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 5)</small><table border="1" class="dataframe"><thead><tr><th>PhysActivity: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;No&quot;</td><td>53302</td><td>0.2439</td><td>53302</td><td>0.243907</td></tr><tr><td>&quot;Yes&quot;</td><td>165232</td><td>0.7561</td><td>218534</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 5)</small><table border="1" class="dataframe"><thead><tr><th>Fruits: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;No&quot;</td><td>79866</td><td>0.3655</td><td>79866</td><td>0.365463</td></tr><tr><td>&quot;Yes&quot;</td><td>138668</td><td>0.6345</td><td>218534</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 5)</small><table border="1" class="dataframe"><thead><tr><th>Veggies: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;No&quot;</td><td>41320</td><td>0.1891</td><td>41320</td><td>0.189078</td></tr><tr><td>&quot;Yes&quot;</td><td>177214</td><td>0.8109</td><td>218534</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 5)</small><table border="1" class="dataframe"><thead><tr><th>HvyAlcoholConsump: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;No&quot;</td><td>206254</td><td>0.9438</td><td>206254</td><td>0.943807</td></tr><tr><td>&quot;Yes&quot;</td><td>12280</td><td>0.0562</td><td>218534</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 5)</small><table border="1" class="dataframe"><thead><tr><th>AnyHealthcare: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;No&quot;</td><td>10705</td><td>0.049</td><td>10705</td><td>0.048986</td></tr><tr><td>&quot;Yes&quot;</td><td>207829</td><td>0.951</td><td>218534</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 5)</small><table border="1" class="dataframe"><thead><tr><th>NoDocbcCost: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;No&quot;</td><td>200146</td><td>0.9159</td><td>200146</td><td>0.915857</td></tr><tr><td>&quot;Yes&quot;</td><td>18388</td><td>0.0841</td><td>218534</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (5, 5)</small><table border="1" class="dataframe"><thead><tr><th>GenHlth: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;5.0&quot;</td><td>10377</td><td>0.0475</td><td>10377</td><td>0.047485</td></tr><tr><td>&quot;Fair&quot;</td><td>65144</td><td>0.2981</td><td>75521</td><td>0.34558</td></tr><tr><td>&quot;Good&quot;</td><td>76805</td><td>0.3515</td><td>152326</td><td>0.697036</td></tr><tr><td>&quot;Poor&quot;</td><td>27183</td><td>0.1244</td><td>179509</td><td>0.821424</td></tr><tr><td>&quot;VeryGood&quot;</td><td>39025</td><td>0.1786</td><td>218534</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 5)</small><table border="1" class="dataframe"><thead><tr><th>DiffWalk: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;No&quot;</td><td>181758</td><td>0.8317</td><td>181758</td><td>0.831715</td></tr><tr><td>&quot;Yes&quot;</td><td>36776</td><td>0.1683</td><td>218534</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 5)</small><table border="1" class="dataframe"><thead><tr><th>Sex: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;Female&quot;</td><td>122388</td><td>0.56</td><td>122388</td><td>0.560041</td></tr><tr><td>&quot;Male&quot;</td><td>96146</td><td>0.44</td><td>218534</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (13, 5)</small><table border="1" class="dataframe"><thead><tr><th>Age: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;[18, 25)&quot;</td><td>4920</td><td>0.0225</td><td>4920</td><td>0.022514</td></tr><tr><td>&quot;[25,30)&quot;</td><td>6507</td><td>0.0298</td><td>11427</td><td>0.052289</td></tr><tr><td>&quot;[30,35)&quot;</td><td>9603</td><td>0.0439</td><td>21030</td><td>0.096232</td></tr><tr><td>&quot;[35,40)&quot;</td><td>11917</td><td>0.0545</td><td>32947</td><td>0.150764</td></tr><tr><td>&quot;[40,45)&quot;</td><td>13914</td><td>0.0637</td><td>46861</td><td>0.214433</td></tr><tr><td>&quot;[45,50)&quot;</td><td>17104</td><td>0.0783</td><td>63965</td><td>0.2927</td></tr><tr><td>&quot;[50,55)&quot;</td><td>22666</td><td>0.1037</td><td>86631</td><td>0.396419</td></tr><tr><td>&quot;[55,60)&quot;</td><td>26541</td><td>0.1215</td><td>113172</td><td>0.517869</td></tr><tr><td>&quot;[60,65)&quot;</td><td>28624</td><td>0.131</td><td>141796</td><td>0.648851</td></tr><tr><td>&quot;[65,70)&quot;</td><td>27711</td><td>0.1268</td><td>169507</td><td>0.775655</td></tr><tr><td>&quot;[70,75)&quot;</td><td>20241</td><td>0.0926</td><td>189748</td><td>0.868277</td></tr><tr><td>&quot;[75,80)&quot;</td><td>13811</td><td>0.0632</td><td>203559</td><td>0.931475</td></tr><tr><td>&quot;[80,inf)&quot;</td><td>14975</td><td>0.0685</td><td>218534</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (6, 5)</small><table border="1" class="dataframe"><thead><tr><th>Education: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;CollegeGraduat…</td><td>92442</td><td>0.423</td><td>92442</td><td>0.42301</td></tr><tr><td>&quot;Elementary&quot;</td><td>3455</td><td>0.0158</td><td>95897</td><td>0.43882</td></tr><tr><td>&quot;HighSchool&quot;</td><td>54003</td><td>0.2471</td><td>149900</td><td>0.685934</td></tr><tr><td>&quot;Never&quot;</td><td>146</td><td>0.0007</td><td>150046</td><td>0.686603</td></tr><tr><td>&quot;SomeCollege&quot;</td><td>60238</td><td>0.2756</td><td>210284</td><td>0.962248</td></tr><tr><td>&quot;SomeHighSchool…</td><td>8250</td><td>0.0378</td><td>218534</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (8, 5)</small><table border="1" class="dataframe"><thead><tr><th>Income: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;[0, 10k)&quot;</td><td>8492</td><td>0.0389</td><td>8492</td><td>0.038859</td></tr><tr><td>&quot;[10k,15k)&quot;</td><td>10151</td><td>0.0465</td><td>18643</td><td>0.085309</td></tr><tr><td>&quot;[15k, 20k)&quot;</td><td>13775</td><td>0.063</td><td>32418</td><td>0.148343</td></tr><tr><td>&quot;[20k,25k)&quot;</td><td>17326</td><td>0.0793</td><td>49744</td><td>0.227626</td></tr><tr><td>&quot;[25k,35k)&quot;</td><td>22355</td><td>0.1023</td><td>72099</td><td>0.329921</td></tr><tr><td>&quot;[35k,50k)&quot;</td><td>31384</td><td>0.1436</td><td>103483</td><td>0.473533</td></tr><tr><td>&quot;[50k,75k)&quot;</td><td>37275</td><td>0.1706</td><td>140758</td><td>0.644101</td></tr><tr><td>&quot;[75k, inf)&quot;</td><td>77776</td><td>0.3559</td><td>218534</td><td>1.0</td></tr></tbody></table></div></div></div>
</div>
<p>Similarly, we compute frequency tables for the quantitative variables, although this may be less informative for certain variables unless they are discretized beforehand.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">quant_columns</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">display</span><span class="p">(</span><span class="n">freq_table</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">diabetes_df</span><span class="p">[</span><span class="n">x</span><span class="p">]))</span>  
    <span class="k">except</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Couldnt be computed for </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (84, 5)</small><table border="1" class="dataframe"><thead><tr><th>BMI: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>f64</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>12.0</td><td>6</td><td>0.0</td><td>6</td><td>0.000024</td></tr><tr><td>13.0</td><td>21</td><td>0.0001</td><td>27</td><td>0.000106</td></tr><tr><td>14.0</td><td>41</td><td>0.0002</td><td>68</td><td>0.000268</td></tr><tr><td>15.0</td><td>132</td><td>0.0005</td><td>200</td><td>0.000788</td></tr><tr><td>16.0</td><td>348</td><td>0.0014</td><td>548</td><td>0.00216</td></tr><tr><td>17.0</td><td>776</td><td>0.0031</td><td>1324</td><td>0.005219</td></tr><tr><td>18.0</td><td>1803</td><td>0.0071</td><td>3127</td><td>0.012327</td></tr><tr><td>19.0</td><td>3968</td><td>0.0156</td><td>7095</td><td>0.027968</td></tr><tr><td>20.0</td><td>6327</td><td>0.0249</td><td>13422</td><td>0.052909</td></tr><tr><td>21.0</td><td>9855</td><td>0.0388</td><td>23277</td><td>0.091757</td></tr><tr><td>22.0</td><td>13643</td><td>0.0538</td><td>36920</td><td>0.145538</td></tr><tr><td>23.0</td><td>15610</td><td>0.0615</td><td>52530</td><td>0.207072</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>84.0</td><td>44</td><td>0.0002</td><td>253533</td><td>0.999421</td></tr><tr><td>85.0</td><td>1</td><td>0.0</td><td>253534</td><td>0.999424</td></tr><tr><td>86.0</td><td>1</td><td>0.0</td><td>253535</td><td>0.999428</td></tr><tr><td>87.0</td><td>61</td><td>0.0002</td><td>253596</td><td>0.999669</td></tr><tr><td>88.0</td><td>2</td><td>0.0</td><td>253598</td><td>0.999677</td></tr><tr><td>89.0</td><td>28</td><td>0.0001</td><td>253626</td><td>0.999787</td></tr><tr><td>90.0</td><td>1</td><td>0.0</td><td>253627</td><td>0.999791</td></tr><tr><td>91.0</td><td>1</td><td>0.0</td><td>253628</td><td>0.999795</td></tr><tr><td>92.0</td><td>32</td><td>0.0001</td><td>253660</td><td>0.999921</td></tr><tr><td>95.0</td><td>12</td><td>0.0</td><td>253672</td><td>0.999968</td></tr><tr><td>96.0</td><td>1</td><td>0.0</td><td>253673</td><td>0.999972</td></tr><tr><td>98.0</td><td>7</td><td>0.0</td><td>253680</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (32, 5)</small><table border="1" class="dataframe"><thead><tr><th>MentHlth: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>f64</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>0.0</td><td>170458</td><td>0.6719</td><td>170458</td><td>0.671941</td></tr><tr><td>1.0</td><td>8283</td><td>0.0327</td><td>178741</td><td>0.704592</td></tr><tr><td>2.0</td><td>12668</td><td>0.0499</td><td>191409</td><td>0.754529</td></tr><tr><td>3.0</td><td>7153</td><td>0.0282</td><td>198562</td><td>0.782726</td></tr><tr><td>4.0</td><td>3693</td><td>0.0146</td><td>202255</td><td>0.797284</td></tr><tr><td>5.0</td><td>8751</td><td>0.0345</td><td>211006</td><td>0.83178</td></tr><tr><td>6.0</td><td>967</td><td>0.0038</td><td>211973</td><td>0.835592</td></tr><tr><td>7.0</td><td>3001</td><td>0.0118</td><td>214974</td><td>0.847422</td></tr><tr><td>8.0</td><td>617</td><td>0.0024</td><td>215591</td><td>0.849854</td></tr><tr><td>9.0</td><td>84</td><td>0.0003</td><td>215675</td><td>0.850185</td></tr><tr><td>10.0</td><td>6188</td><td>0.0244</td><td>221863</td><td>0.874578</td></tr><tr><td>11.0</td><td>40</td><td>0.0002</td><td>221903</td><td>0.874736</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>20.0</td><td>3277</td><td>0.0129</td><td>232330</td><td>0.915839</td></tr><tr><td>21.0</td><td>220</td><td>0.0009</td><td>232550</td><td>0.916706</td></tr><tr><td>22.0</td><td>62</td><td>0.0002</td><td>232612</td><td>0.91695</td></tr><tr><td>23.0</td><td>37</td><td>0.0001</td><td>232649</td><td>0.917096</td></tr><tr><td>24.0</td><td>32</td><td>0.0001</td><td>232681</td><td>0.917222</td></tr><tr><td>25.0</td><td>1142</td><td>0.0045</td><td>233823</td><td>0.921724</td></tr><tr><td>26.0</td><td>44</td><td>0.0002</td><td>233867</td><td>0.921898</td></tr><tr><td>27.0</td><td>78</td><td>0.0003</td><td>233945</td><td>0.922205</td></tr><tr><td>28.0</td><td>320</td><td>0.0013</td><td>234265</td><td>0.923467</td></tr><tr><td>29.0</td><td>151</td><td>0.0006</td><td>234416</td><td>0.924062</td></tr><tr><td>30.0</td><td>11736</td><td>0.0463</td><td>246152</td><td>0.970325</td></tr><tr><td>NaN</td><td>7528</td><td>0.0297</td><td>253680</td><td>1.0</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (32, 5)</small><table border="1" class="dataframe"><thead><tr><th>PhysHlth: unique values</th><th>abs_freq</th><th>rel_freq</th><th>cum_abs_freq</th><th>cum_rel_freq</th></tr><tr><td>f64</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>0.0</td><td>155770</td><td>0.614</td><td>155770</td><td>0.614041</td></tr><tr><td>1.0</td><td>11092</td><td>0.0437</td><td>166862</td><td>0.657766</td></tr><tr><td>2.0</td><td>14361</td><td>0.0566</td><td>181223</td><td>0.714376</td></tr><tr><td>3.0</td><td>8271</td><td>0.0326</td><td>189494</td><td>0.74698</td></tr><tr><td>4.0</td><td>4422</td><td>0.0174</td><td>193916</td><td>0.764412</td></tr><tr><td>5.0</td><td>7438</td><td>0.0293</td><td>201354</td><td>0.793732</td></tr><tr><td>6.0</td><td>1294</td><td>0.0051</td><td>202648</td><td>0.798833</td></tr><tr><td>7.0</td><td>4416</td><td>0.0174</td><td>207064</td><td>0.816241</td></tr><tr><td>8.0</td><td>788</td><td>0.0031</td><td>207852</td><td>0.819347</td></tr><tr><td>9.0</td><td>173</td><td>0.0007</td><td>208025</td><td>0.820029</td></tr><tr><td>10.0</td><td>5469</td><td>0.0216</td><td>213494</td><td>0.841588</td></tr><tr><td>11.0</td><td>58</td><td>0.0002</td><td>213552</td><td>0.841816</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>20.0</td><td>3198</td><td>0.0126</td><td>225018</td><td>0.887015</td></tr><tr><td>21.0</td><td>650</td><td>0.0026</td><td>225668</td><td>0.889577</td></tr><tr><td>22.0</td><td>70</td><td>0.0003</td><td>225738</td><td>0.889853</td></tr><tr><td>23.0</td><td>54</td><td>0.0002</td><td>225792</td><td>0.890066</td></tr><tr><td>24.0</td><td>66</td><td>0.0003</td><td>225858</td><td>0.890326</td></tr><tr><td>25.0</td><td>1299</td><td>0.0051</td><td>227157</td><td>0.895447</td></tr><tr><td>26.0</td><td>68</td><td>0.0003</td><td>227225</td><td>0.895715</td></tr><tr><td>27.0</td><td>97</td><td>0.0004</td><td>227322</td><td>0.896097</td></tr><tr><td>28.0</td><td>509</td><td>0.002</td><td>227831</td><td>0.898104</td></tr><tr><td>29.0</td><td>209</td><td>0.0008</td><td>228040</td><td>0.898928</td></tr><tr><td>30.0</td><td>18901</td><td>0.0745</td><td>246941</td><td>0.973435</td></tr><tr><td>NaN</td><td>6739</td><td>0.0266</td><td>253680</td><td>1.0</td></tr></tbody></table></div></div></div>
</div>
</section>
<section id="visualization">
<h3><strong>Visualization</strong><a class="headerlink" href="#visualization" title="Link to this heading">#</a></h3>
<p>We will now proceed to conduct a comprehensive visualization of our dataset in this section.</p>
<section id="histogram-matrix">
<h4><strong>Histogram matrix</strong><a class="headerlink" href="#histogram-matrix" title="Link to this heading">#</a></h4>
<p>We compute an histogram for each quantitative variable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">histogram_matrix</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">diabetes_df</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_cols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Histogram Matrix - Quantitative variables&#39;</span><span class="p">,</span> <span class="n">title_fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">subtitles_fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
                 <span class="n">n_xticks</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">17</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">auto_col</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">quant_col_names</span><span class="o">=</span><span class="n">quant_columns</span><span class="p">,</span> <span class="n">title_height</span><span class="o">=</span><span class="mf">1.01</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b6cd44feb319686356dcf111b4a7bc711e74191c9e2aa9b44ff211fb21be60ce.png" src="_images/b6cd44feb319686356dcf111b4a7bc711e74191c9e2aa9b44ff211fb21be60ce.png" />
</div>
</div>
<p>Each histogram represents the distribution of respondents’ reported values for these health indicators.</p>
<ul class="simple">
<li><p>The BMI histogram shows a right-skewed distribution, suggesting that a larger number of respondents have a BMI in the lower range, with fewer individuals having higher BMI values.</p></li>
<li><p>The Mental Health histogram is highly skewed to the left, indicating most respondents report lower scores (presumably representing fewer mental health issues), with a small proportion reporting higher scores.</p></li>
<li><p>The Physical Health histogram exhibits a similar left skewness to the Mental Health histogram, with the majority of responses clustering at the lower end of the scale (indicating better physical health) and fewer responses indicating worse physical health.</p></li>
</ul>
<p>Overall, these histograms suggest that the majority of the survey’s respondents report better mental and physical health, with body mass index showing a more varied distribution.</p>
</section>
<section id="boxplot-matrix">
<h4><strong>Boxplot matrix</strong><a class="headerlink" href="#boxplot-matrix" title="Link to this heading">#</a></h4>
<p>We compute an boxplot for each quantitative variable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">boxplot_matrix</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">diabetes_df</span><span class="p">,</span> <span class="n">n_cols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Boxplot Matrix - Quantitative variables&#39;</span><span class="p">,</span> <span class="n">title_fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">subtitles_fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
               <span class="n">n_xticks</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">auto_col</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">quant_col_names</span><span class="o">=</span><span class="n">quant_columns</span><span class="p">,</span> <span class="n">title_height</span><span class="o">=</span><span class="mf">1.05</span><span class="p">,</span>
               <span class="n">statistics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;median&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="n">color_stats</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">],</span>
               <span class="n">lines_width</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">legend_size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/15999710eb3d9562b7dcee1d1570def9ec7fe569f84acb4beccaaa05220fd073.png" src="_images/15999710eb3d9562b7dcee1d1570def9ec7fe569f84acb4beccaaa05220fd073.png" />
</div>
</div>
<p>The last two variables haven’t lines with the median and mean because have missing values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">diabetes_df_non_quant_NaNs</span> <span class="o">=</span> <span class="n">diabetes_df</span><span class="o">.</span><span class="n">drop_nulls</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="n">quant_columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prop_cols_nulls</span><span class="p">(</span><span class="n">diabetes_df_non_quant_NaNs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (1, 22)</small><table border="1" class="dataframe"><thead><tr><th>Diabetes_binary</th><th>HighBP</th><th>HighChol</th><th>CholCheck</th><th>BMI</th><th>Smoker</th><th>Stroke</th><th>HeartDiseaseorAttack</th><th>PhysActivity</th><th>Fruits</th><th>Veggies</th><th>HvyAlcoholConsump</th><th>AnyHealthcare</th><th>NoDocbcCost</th><th>GenHlth</th><th>MentHlth</th><th>PhysHlth</th><th>DiffWalk</th><th>Sex</th><th>Age</th><th>Education</th><th>Income</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.0</td><td>0.0</td><td>0.0</td><td>0.023362</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.067201</td><td>0.0</td><td>0.0</td><td>0.001369</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.011334</td><td>0.041928</td><td>0.0</td><td>0.0</td></tr></tbody></table></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">boxplot_matrix</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">diabetes_df_non_quant_NaNs</span><span class="p">,</span> <span class="n">n_cols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Boxplot Matrix - Quantitative variables&#39;</span><span class="p">,</span> <span class="n">title_fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">subtitles_fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
               <span class="n">n_xticks</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">auto_col</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">quant_col_names</span><span class="o">=</span><span class="n">quant_columns</span><span class="p">,</span> <span class="n">title_height</span><span class="o">=</span><span class="mf">1.05</span><span class="p">,</span>
               <span class="n">statistics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;median&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="n">color_stats</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">],</span>
               <span class="n">lines_width</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">legend_size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/a7d3ce1de810e2dc626c42b8494cccc71c787e18835e91abd4d35622489c4248.png" src="_images/a7d3ce1de810e2dc626c42b8494cccc71c787e18835e91abd4d35622489c4248.png" />
</div>
</div>
<p>After the data transformation to remove rows with null values in the quantitative variables, the boxplot matrix shows the following:</p>
<ul class="simple">
<li><p>For BMI, the distribution remains wide with several high-value outliers, indicating a varied body weight distribution. The median is lower than the mean, suggesting a right-skewed distribution.</p></li>
<li><p>Mental Health and Physical Health both show a concentration of values at the lower end of the scale, with the median and mean closely aligned. This suggests that most people report fewer issues, but there is a tail of respondents reporting higher values.</p></li>
<li><p>The presence of outliers is still noticeable in all three health indicators, implying that while most values cluster around a central point, there are individuals with significantly different reported outcomes.</p></li>
</ul>
<p>The absence of median and mean lines for the last two variables in the previous plot indicates the persisting presence of missing values.</p>
</section>
<section id="ecdfplot-matrix">
<h4><strong>ECDFplot matrix</strong><a class="headerlink" href="#ecdfplot-matrix" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ecdf_matrix</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">diabetes_df</span><span class="p">,</span> <span class="n">n_cols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;ECDFplot Matrix - Quantitative variables&#39;</span><span class="p">,</span> <span class="n">title_fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">subtitles_fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span>
            <span class="n">n_xticks</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">auto_col</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">quant_col_names</span><span class="o">=</span><span class="n">quant_columns</span><span class="p">,</span> <span class="n">title_height</span><span class="o">=</span><span class="mf">1.01</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/97709e71d46ca95cacc8e64455577a8ae2e3985ef8445ccdf863cea4abbdd86b.png" src="_images/97709e71d46ca95cacc8e64455577a8ae2e3985ef8445ccdf863cea4abbdd86b.png" />
</div>
</div>
<ul class="simple">
<li><p>The BMI ECDF suggests a rapid accumulation of values in the lower range, with the curve flattening as it approaches higher BMI values. This indicates that a large proportion of the population has a BMI within a lower range.</p></li>
<li><p>The Mental Health ECDF shows a more gradual slope, suggesting a more even distribution of responses across the range, but still, the majority have lower scores.</p></li>
<li><p>The Physical Health ECDF appears to have a similar shape to the Mental Health curve, indicating a similar distribution pattern of responses.</p></li>
</ul>
</section>
<section id="barplot-matrix">
<h4><strong>Barplot matrix</strong><a class="headerlink" href="#barplot-matrix" title="Link to this heading">#</a></h4>
<p>For showing the distribution of the variables, specially the binary ones.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">barplot_matrix</span><span class="p">(</span><span class="n">diabetes_df</span><span class="p">,</span> <span class="n">n_cols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Barplot Matrix - Categorical variables&#39;</span><span class="p">,</span> 
               <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">20</span><span class="p">),</span> <span class="n">auto_col</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cat_col_list</span><span class="o">=</span><span class="n">cat_columns</span><span class="p">,</span> <span class="n">title_height</span><span class="o">=</span><span class="mf">0.92</span><span class="p">,</span>
               <span class="n">hspace</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">x_rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">title_fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">subtitles_fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">n_yticks</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/490886dca230d426077a962e702f3191474f1634e9cf4c3800066db4b78dac7e.png" src="_images/490886dca230d426077a962e702f3191474f1634e9cf4c3800066db4b78dac7e.png" />
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;Diabetes_binary&quot;</span></code> shows the distribution between individuals with and without diabetes, indicating the proportion affected by the condition.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;HighBP&quot;</span></code> reveals how many individuals report having high blood pressure, while <code class="docutils literal notranslate"><span class="pre">&quot;HighChol&quot;</span></code> does the same for high cholesterol, both critical factors in diabetes risk.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;CholCheck&quot;</span></code> indicates high compliance with cholesterol monitoring among the survey participants, suggesting awareness or prevalence of cholesterol issues.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;Smoker&quot;</span></code> shows a considerable number of individuals identify as smokers, a risk factor for many health issues including diabetes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;Stroke&quot;</span></code> gives an indication of the prevalence of stroke among the respondents.</p></li>
<li><p>Lifestyle habits are captured in <code class="docutils literal notranslate"><span class="pre">&quot;PhysActivity&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;Fruits&quot;</span></code>, and <code class="docutils literal notranslate"><span class="pre">&quot;Veggies&quot;</span></code>, displaying the proportion of individuals who are physically active and those who consume fruits and vegetables regularly.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;HvyAlcoholConsump&quot;</span></code> shows how many participants consume alcohol heavily, which can impact health.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;AnyHealthcare&quot;</span></code> reflects on the participants’ access to healthcare, which is a crucial factor for managing chronic diseases.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;NoDocbcCost&quot;</span></code> suggests economic barriers to healthcare, as it shows the proportion of people who have foregone medical care due to cost.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;GenHlth&quot;</span></code> represents the participants’ self-assessed health status, with categories ranging from excellent to poor.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;DiffWalk&quot;</span></code> highlights the number of individuals with difficulties in walking, which may relate to physical health status or disabilities.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">&quot;Sex&quot;</span></code> distribution graph indicates the relative proportions of female and male participants.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;Age&quot;</span></code> shows a declining number of participants with increasing age, while <code class="docutils literal notranslate"><span class="pre">&quot;Education&quot;</span></code> indicates the distribution of educational attainment levels across the sample.</p></li>
<li><p>Finally, <code class="docutils literal notranslate"><span class="pre">&quot;Income&quot;</span></code> illustrates the financial distribution among the survey participants, providing insight into the economic diversity of the sample.</p></li>
</ul>
<p>Each bar graph provides insight into the frequency and distribution of each category, which is vital for understanding factors associated with diabetes within the population.</p>
</section>
</section>
<section id="response-vs-predictors-analysis">
<h3><strong>Response vs Predictors Analysis</strong><a class="headerlink" href="#response-vs-predictors-analysis" title="Link to this heading">#</a></h3>
<p>First, we start differentiating between the response and the predictors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">response</span> <span class="o">=</span> <span class="s1">&#39;Diabetes_binary&#39;</span>
<span class="n">predictors</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">diabetes_df</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">col</span> <span class="o">!=</span> <span class="n">response</span><span class="p">]</span>
<span class="n">quant_predictors</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">predictors</span> <span class="k">if</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">quant_columns</span><span class="p">]</span>
<span class="n">cat_predictors</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">predictors</span> <span class="k">if</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">cat_columns</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<section id="response-vs-quantitative-predictors">
<h4><strong>Response vs Quantitative Predictors</strong><a class="headerlink" href="#response-vs-quantitative-predictors" title="Link to this heading">#</a></h4>
<p>Now let’s compare the distribution of the quantitative predictor variables against the binary response variable for diabetes <code class="docutils literal notranslate"><span class="pre">Diabetes_binary</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">boxplot_2D_matrix</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">diabetes_df</span><span class="p">,</span> <span class="n">n_cols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">tittle</span><span class="o">=</span><span class="s1">&#39;Diabetes vs Quantitative Predictors&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> 
                 <span class="n">cat_col_names</span><span class="o">=</span><span class="p">[</span><span class="n">response</span><span class="p">],</span> <span class="n">quant_col_names</span><span class="o">=</span><span class="n">quant_predictors</span><span class="p">,</span>
                 <span class="n">n_yticks</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">title_height</span><span class="o">=</span><span class="mf">1.02</span><span class="p">,</span> <span class="n">title_fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">subtitles_fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span>
                 <span class="n">statistics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="n">lines_width</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">0.53</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.15</span><span class="p">),</span> 
                 <span class="n">legend_size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">color_stats</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;red&#39;</span><span class="p">],</span> <span class="n">showfliers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">hspace</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">x_rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">auto_col</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8ac9c65e68e32e667fc09d003086a5380d5de02eddb8cbe59e06d4e408b6a923.png" src="_images/8ac9c65e68e32e667fc09d003086a5380d5de02eddb8cbe59e06d4e408b6a923.png" />
</div>
</div>
<ul class="simple">
<li><p>For BMI, individuals with diabetes (<code class="docutils literal notranslate"><span class="pre">&quot;yes&quot;</span></code>) show a higher mean BMI (indicated by the <strong>red</strong>
dashed line) than those without diabetes (<code class="docutils literal notranslate"><span class="pre">&quot;no&quot;</span></code>), and the distribution has a larger spread, suggesting a correlation between higher BMI and diabetes presence.</p></li>
<li><p>The Mental Health (<code class="docutils literal notranslate"><span class="pre">MentHlth</span></code>) boxplot indicates a slightly higher mean for those with diabetes compared to those without, but the difference is not as pronounced as with BMI.</p></li>
<li><p>Physical Health (<code class="docutils literal notranslate"><span class="pre">PhysHlth</span></code>) shows a notably higher mean for diabetic individuals, implying that those with diabetes might have worse physical health than those without.</p></li>
</ul>
<section id="cross-quant-cat-descriptive-summary">
<h5><strong>Cross quant-cat descriptive summary</strong><a class="headerlink" href="#cross-quant-cat-descriptive-summary" title="Link to this heading">#</a></h5>
<p>This step in the code is generating and displaying a cross-tabulation summary between each quantitative predictor and the response variable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">quant_predictors</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---------------------------------------------------------------------------------&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">col</span><span class="p">)</span>
    <span class="n">display</span><span class="p">(</span><span class="n">cross_quant_cat_summary</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">diabetes_df</span><span class="p">,</span> <span class="n">cat_col</span><span class="o">=</span><span class="n">response</span><span class="p">,</span> <span class="n">quant_col</span><span class="o">=</span><span class="n">col</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>---------------------------------------------------------------------------------
BMI
</pre></div>
</div>
<div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 15)</small><table border="1" class="dataframe"><thead><tr><th>Diabetes_binary</th><th>prop_BMI</th><th>mean_BMI</th><th>std_BMI</th><th>min_BMI</th><th>Q10_BMI</th><th>Q25_BMI</th><th>median_BMI</th><th>Q75_BMI</th><th>Q90_BMI</th><th>max_price</th><th>kurtosis_BMI</th><th>skew_BMI</th><th>prop_outliers_BMI</th><th>prop_nan_BMI</th></tr><tr><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>Yes</td><td>0.139</td><td>31.944</td><td>7.363</td><td>13.0</td><td>24.0</td><td>27.0</td><td>31.0</td><td>35.0</td><td>41.0</td><td>98.0</td><td>5.716</td><td>1.527</td><td>0.029679</td><td>0.0</td></tr><tr><td>No</td><td>0.861</td><td>27.806</td><td>6.291</td><td>12.0</td><td>21.0</td><td>24.0</td><td>27.0</td><td>31.0</td><td>35.0</td><td>98.0</td><td>13.617</td><td>2.331</td><td>0.034035</td><td>0.0</td></tr></tbody></table></div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>---------------------------------------------------------------------------------
MentHlth
</pre></div>
</div>
<div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 15)</small><table border="1" class="dataframe"><thead><tr><th>Diabetes_binary</th><th>prop_MentHlth</th><th>mean_MentHlth</th><th>std_MentHlth</th><th>min_MentHlth</th><th>Q10_MentHlth</th><th>Q25_MentHlth</th><th>median_MentHlth</th><th>Q75_MentHlth</th><th>Q90_MentHlth</th><th>max_price</th><th>kurtosis_MentHlth</th><th>skew_MentHlth</th><th>prop_outliers_MentHlth</th><th>prop_nan_MentHlth</th></tr><tr><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>No</td><td>0.835</td><td>2.979</td><td>7.113</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.0</td><td>10.0</td><td>30.0</td><td>7.364</td><td>2.862</td><td>0.129906</td><td>0.02956</td></tr><tr><td>Yes</td><td>0.135</td><td>4.467</td><td>8.952</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>3.0</td><td>20.0</td><td>30.0</td><td>2.864</td><td>2.063</td><td>0.176342</td><td>0.030385</td></tr></tbody></table></div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>---------------------------------------------------------------------------------
PhysHlth
</pre></div>
</div>
<div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 15)</small><table border="1" class="dataframe"><thead><tr><th>Diabetes_binary</th><th>prop_PhysHlth</th><th>mean_PhysHlth</th><th>std_PhysHlth</th><th>min_PhysHlth</th><th>Q10_PhysHlth</th><th>Q25_PhysHlth</th><th>median_PhysHlth</th><th>Q75_PhysHlth</th><th>Q90_PhysHlth</th><th>max_price</th><th>kurtosis_PhysHlth</th><th>skew_PhysHlth</th><th>prop_outliers_PhysHlth</th><th>prop_nan_PhysHlth</th></tr><tr><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>No</td><td>0.838</td><td>3.644</td><td>8.069</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.0</td><td>15.0</td><td>30.0</td><td>4.993</td><td>2.495</td><td>0.0</td><td>0.026764</td></tr><tr><td>Yes</td><td>0.136</td><td>7.95</td><td>11.297</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>15.0</td><td>30.0</td><td>30.0</td><td>-0.338</td><td>1.151</td><td>0.155212</td><td>0.026533</td></tr></tbody></table></div></div></div>
</div>
</section>
</section>
<section id="response-vs-categorical-predictors">
<h4><strong>Response vs Categorical Predictors</strong><a class="headerlink" href="#response-vs-categorical-predictors" title="Link to this heading">#</a></h4>
<section id="conditional-contingence-table">
<h5><strong>Conditional contingence table</strong><a class="headerlink" href="#conditional-contingence-table" title="Link to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">cat_predictors</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">display</span><span class="p">(</span><span class="n">contingency_table_2D</span><span class="p">(</span><span class="n">diabetes_df</span><span class="p">,</span> <span class="n">cat1_name</span><span class="o">=</span><span class="n">response</span><span class="p">,</span> <span class="n">cat2_name</span><span class="o">=</span><span class="n">col</span><span class="p">,</span> 
                                     <span class="n">conditional</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">except</span><span class="p">:</span> 
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-------------------------------&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Computation failed for </span><span class="si">{</span><span class="n">col</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-------------------------------&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(HighBP | Diabetes_binary) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>82225</td><td>0.3766</td></tr><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>136109</td><td>0.6234</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>26604</td><td>0.7527</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>8742</td><td>0.2473</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(HighChol | Diabetes_binary) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>83905</td><td>0.3843</td></tr><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>134429</td><td>0.6157</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>23686</td><td>0.6701</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>11660</td><td>0.3299</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (6, 3)</small><table border="1" class="dataframe"><thead><tr><th>(CholCheck | Diabetes_binary) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>204181</td><td>0.9352</td></tr><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>9009</td><td>0.0413</td></tr><tr><td>[null, &quot;No&quot;]</td><td>5144</td><td>0.0236</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>34326</td><td>0.9711</td></tr><tr><td>[null, &quot;Yes&quot;]</td><td>785</td><td>0.0222</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>235</td><td>0.0066</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Smoker | Diabetes_binary) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>94106</td><td>0.431</td></tr><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>124228</td><td>0.569</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>18317</td><td>0.5182</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>17029</td><td>0.4818</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Stroke | Diabetes_binary) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>211310</td><td>0.9678</td></tr><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>7024</td><td>0.0322</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>32078</td><td>0.9075</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>3268</td><td>0.0925</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(HeartDiseaseorAttack | Diabetes_binary) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>202319</td><td>0.9266</td></tr><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>16015</td><td>0.0734</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>7878</td><td>0.2229</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>27468</td><td>0.7771</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(PhysActivity | Diabetes_binary) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>48701</td><td>0.2231</td></tr><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>169633</td><td>0.7769</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>13059</td><td>0.3695</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>22287</td><td>0.6305</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Fruits | Diabetes_binary) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>78129</td><td>0.3578</td></tr><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>140205</td><td>0.6422</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>20693</td><td>0.5854</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>14653</td><td>0.4146</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (6, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Veggies | Diabetes_binary) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>166980</td><td>0.7648</td></tr><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>36635</td><td>0.1678</td></tr><tr><td>[null, &quot;No&quot;]</td><td>14719</td><td>0.0674</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>24923</td><td>0.7051</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>8041</td><td>0.2275</td></tr><tr><td>[null, &quot;Yes&quot;]</td><td>2382</td><td>0.0674</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(HvyAlcoholConsump | Diabetes_binary) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>204910</td><td>0.9385</td></tr><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>13424</td><td>0.0615</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>34514</td><td>0.9765</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>832</td><td>0.0235</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(AnyHealthcare | Diabetes_binary) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>207339</td><td>0.9496</td></tr><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>10995</td><td>0.0504</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>33924</td><td>0.9598</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>1422</td><td>0.0402</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (6, 3)</small><table border="1" class="dataframe"><thead><tr><th>(NoDocbcCost | Diabetes_binary) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>200454</td><td>0.9181</td></tr><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>17584</td><td>0.0805</td></tr><tr><td>[null, &quot;No&quot;]</td><td>296</td><td>0.0014</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>31567</td><td>0.8931</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>3736</td><td>0.1057</td></tr><tr><td>[null, &quot;Yes&quot;]</td><td>43</td><td>0.0012</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (10, 3)</small><table border="1" class="dataframe"><thead><tr><th>(GenHlth | Diabetes_binary) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;5.0&quot;, &quot;No&quot;]</td><td>7503</td><td>0.0344</td></tr><tr><td>[&quot;Fair&quot;, &quot;No&quot;]</td><td>62189</td><td>0.2848</td></tr><tr><td>[&quot;Good&quot;, &quot;No&quot;]</td><td>82703</td><td>0.3788</td></tr><tr><td>[&quot;Poor&quot;, &quot;No&quot;]</td><td>21780</td><td>0.0998</td></tr><tr><td>[&quot;VeryGood&quot;, &quot;No&quot;]</td><td>44159</td><td>0.2023</td></tr><tr><td>[&quot;5.0&quot;, &quot;Yes&quot;]</td><td>4578</td><td>0.1295</td></tr><tr><td>[&quot;Fair&quot;, &quot;Yes&quot;]</td><td>13457</td><td>0.3807</td></tr><tr><td>[&quot;Poor&quot;, &quot;Yes&quot;]</td><td>9790</td><td>0.277</td></tr><tr><td>[&quot;Good&quot;, &quot;Yes&quot;]</td><td>6381</td><td>0.1805</td></tr><tr><td>[&quot;VeryGood&quot;, &quot;Yes&quot;]</td><td>1140</td><td>0.0323</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(DiffWalk | Diabetes_binary) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>29554</td><td>0.1354</td></tr><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>188780</td><td>0.8646</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>13121</td><td>0.3712</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>22225</td><td>0.6288</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (6, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Sex | Diabetes_binary) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;Female&quot;, &quot;No&quot;]</td><td>122166</td><td>0.5595</td></tr><tr><td>[null, &quot;No&quot;]</td><td>2489</td><td>0.0114</td></tr><tr><td>[&quot;Male&quot;, &quot;No&quot;]</td><td>93679</td><td>0.4291</td></tr><tr><td>[&quot;Female&quot;, &quot;Yes&quot;]</td><td>18214</td><td>0.5153</td></tr><tr><td>[&quot;Male&quot;, &quot;Yes&quot;]</td><td>16745</td><td>0.4737</td></tr><tr><td>[null, &quot;Yes&quot;]</td><td>387</td><td>0.0109</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (28, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Age | Diabetes_binary) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;[60,65)&quot;, &quot;No&quot;]</td><td>26340</td><td>0.1206</td></tr><tr><td>[&quot;[50,55)&quot;, &quot;No&quot;]</td><td>22253</td><td>0.1019</td></tr><tr><td>[&quot;[70,75)&quot;, &quot;No&quot;]</td><td>17624</td><td>0.0807</td></tr><tr><td>[&quot;[65,70)&quot;, &quot;No&quot;]</td><td>24537</td><td>0.1124</td></tr><tr><td>[&quot;[55,60)&quot;, &quot;No&quot;]</td><td>25478</td><td>0.1167</td></tr><tr><td>[&quot;[35,40)&quot;, &quot;No&quot;]</td><td>12655</td><td>0.058</td></tr><tr><td>[&quot;[45,50)&quot;, &quot;No&quot;]</td><td>17323</td><td>0.0793</td></tr><tr><td>[null, &quot;No&quot;]</td><td>9159</td><td>0.0419</td></tr><tr><td>[&quot;[75,80)&quot;, &quot;No&quot;]</td><td>12064</td><td>0.0553</td></tr><tr><td>[&quot;[80,inf)&quot;, &quot;No&quot;]</td><td>13524</td><td>0.0619</td></tr><tr><td>[&quot;[40,45)&quot;, &quot;No&quot;]</td><td>14508</td><td>0.0664</td></tr><tr><td>[&quot;[18, 25)&quot;, &quot;No&quot;]</td><td>5378</td><td>0.0246</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>[&quot;[70,75)&quot;, &quot;Yes&quot;]</td><td>4925</td><td>0.1393</td></tr><tr><td>[&quot;[50,55)&quot;, &quot;Yes&quot;]</td><td>2956</td><td>0.0836</td></tr><tr><td>[&quot;[65,70)&quot;, &quot;Yes&quot;]</td><td>6287</td><td>0.1779</td></tr><tr><td>[&quot;[75,80)&quot;, &quot;Yes&quot;]</td><td>3258</td><td>0.0922</td></tr><tr><td>[null, &quot;Yes&quot;]</td><td>1465</td><td>0.0414</td></tr><tr><td>[&quot;[55,60)&quot;, &quot;Yes&quot;]</td><td>4098</td><td>0.1159</td></tr><tr><td>[&quot;[30,35)&quot;, &quot;Yes&quot;]</td><td>295</td><td>0.0083</td></tr><tr><td>[&quot;[45,50)&quot;, &quot;Yes&quot;]</td><td>1677</td><td>0.0474</td></tr><tr><td>[&quot;[35,40)&quot;, &quot;Yes&quot;]</td><td>596</td><td>0.0169</td></tr><tr><td>[&quot;[40,45)&quot;, &quot;Yes&quot;]</td><td>1004</td><td>0.0284</td></tr><tr><td>[&quot;[25,30)&quot;, &quot;Yes&quot;]</td><td>139</td><td>0.0039</td></tr><tr><td>[&quot;[18, 25)&quot;, &quot;Yes&quot;]</td><td>77</td><td>0.0022</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (12, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Education | Diabetes_binary) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;HighSchool&quot;, &quot;No&quot;]</td><td>51684</td><td>0.2367</td></tr><tr><td>[&quot;CollegeGraduate&quot;, &quot;No&quot;]</td><td>96925</td><td>0.4439</td></tr><tr><td>[&quot;SomeHighSchool&quot;, &quot;No&quot;]</td><td>7182</td><td>0.0329</td></tr><tr><td>[&quot;SomeCollege&quot;, &quot;No&quot;]</td><td>59556</td><td>0.2728</td></tr><tr><td>[&quot;Elementary&quot;, &quot;No&quot;]</td><td>2860</td><td>0.0131</td></tr><tr><td>[&quot;Never&quot;, &quot;No&quot;]</td><td>127</td><td>0.0006</td></tr><tr><td>[&quot;SomeCollege&quot;, &quot;Yes&quot;]</td><td>10354</td><td>0.2929</td></tr><tr><td>[&quot;CollegeGraduate&quot;, &quot;Yes&quot;]</td><td>10400</td><td>0.2942</td></tr><tr><td>[&quot;HighSchool&quot;, &quot;Yes&quot;]</td><td>11066</td><td>0.3131</td></tr><tr><td>[&quot;Elementary&quot;, &quot;Yes&quot;]</td><td>1183</td><td>0.0335</td></tr><tr><td>[&quot;SomeHighSchool&quot;, &quot;Yes&quot;]</td><td>2296</td><td>0.065</td></tr><tr><td>[&quot;Never&quot;, &quot;Yes&quot;]</td><td>47</td><td>0.0013</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (16, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Income | Diabetes_binary) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;[15k, 20k)&quot;, &quot;No&quot;]</td><td>12426</td><td>0.0569</td></tr><tr><td>[&quot;[0, 10k)&quot;, &quot;No&quot;]</td><td>7428</td><td>0.034</td></tr><tr><td>[&quot;[75k, inf)&quot;, &quot;No&quot;]</td><td>83190</td><td>0.381</td></tr><tr><td>[&quot;[35k,50k)&quot;, &quot;No&quot;]</td><td>31179</td><td>0.1428</td></tr><tr><td>[&quot;[20k,25k)&quot;, &quot;No&quot;]</td><td>16081</td><td>0.0737</td></tr><tr><td>[&quot;[50k,75k)&quot;, &quot;No&quot;]</td><td>37954</td><td>0.1738</td></tr><tr><td>[&quot;[10k,15k)&quot;, &quot;No&quot;]</td><td>8697</td><td>0.0398</td></tr><tr><td>[&quot;[25k,35k)&quot;, &quot;No&quot;]</td><td>21379</td><td>0.0979</td></tr><tr><td>[&quot;[0, 10k)&quot;, &quot;Yes&quot;]</td><td>2383</td><td>0.0674</td></tr><tr><td>[&quot;[75k, inf)&quot;, &quot;Yes&quot;]</td><td>7195</td><td>0.2036</td></tr><tr><td>[&quot;[35k,50k)&quot;, &quot;Yes&quot;]</td><td>5291</td><td>0.1497</td></tr><tr><td>[&quot;[20k,25k)&quot;, &quot;Yes&quot;]</td><td>4054</td><td>0.1147</td></tr><tr><td>[&quot;[25k,35k)&quot;, &quot;Yes&quot;]</td><td>4504</td><td>0.1274</td></tr><tr><td>[&quot;[50k,75k)&quot;, &quot;Yes&quot;]</td><td>5265</td><td>0.149</td></tr><tr><td>[&quot;[15k, 20k)&quot;, &quot;Yes&quot;]</td><td>3568</td><td>0.1009</td></tr><tr><td>[&quot;[10k,15k)&quot;, &quot;Yes&quot;]</td><td>3086</td><td>0.0873</td></tr></tbody></table></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">cat_predictors</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">display</span><span class="p">(</span><span class="n">contingency_table_2D</span><span class="p">(</span><span class="n">diabetes_df_non_cat_NaNs</span><span class="p">,</span> <span class="n">cat1_name</span><span class="o">=</span><span class="n">response</span><span class="p">,</span> <span class="n">cat2_name</span><span class="o">=</span><span class="n">col</span><span class="p">,</span> 
                                     <span class="n">conditional</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">except</span><span class="p">:</span> 
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-------------------------------&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Computation failed for </span><span class="si">{</span><span class="n">col</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-------------------------------&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Diabetes_binary | HighBP) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>117270</td><td>0.9395</td></tr><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>7548</td><td>0.0605</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>70731</td><td>0.7547</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>22985</td><td>0.2453</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Diabetes_binary | HighChol) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>115719</td><td>0.9199</td></tr><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>10077</td><td>0.0801</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>72282</td><td>0.7794</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>20456</td><td>0.2206</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Diabetes_binary | CholCheck) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>7940</td><td>0.9742</td></tr><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>210</td><td>0.0258</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>180061</td><td>0.8559</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>30323</td><td>0.1441</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Diabetes_binary | Smoker) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>107039</td><td>0.879</td></tr><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>14735</td><td>0.121</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>80962</td><td>0.8367</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>15798</td><td>0.1633</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Diabetes_binary | Stroke) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>181918</td><td>0.8678</td></tr><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>27714</td><td>0.1322</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>6083</td><td>0.6833</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>2819</td><td>0.3167</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Diabetes_binary | HeartDiseaseorAttack) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>174260</td><td>0.8802</td></tr><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>23711</td><td>0.1198</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>6822</td><td>0.3318</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>13741</td><td>0.6682</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Diabetes_binary | PhysActivity) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>42008</td><td>0.7881</td></tr><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>11294</td><td>0.2119</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>145993</td><td>0.8836</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>19239</td><td>0.1164</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Diabetes_binary | Fruits) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>67237</td><td>0.8419</td></tr><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>12629</td><td>0.1581</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>120764</td><td>0.8709</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>17904</td><td>0.1291</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Diabetes_binary | Veggies) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>33904</td><td>0.8205</td></tr><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>7416</td><td>0.1795</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>154097</td><td>0.8696</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>23117</td><td>0.1304</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Diabetes_binary | HvyAlcoholConsump) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>176441</td><td>0.8555</td></tr><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>29813</td><td>0.1445</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>11560</td><td>0.9414</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>720</td><td>0.0586</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Diabetes_binary | AnyHealthcare) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>9477</td><td>0.8853</td></tr><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>1228</td><td>0.1147</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>178524</td><td>0.859</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>29305</td><td>0.141</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Diabetes_binary | NoDocbcCost) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>172849</td><td>0.8636</td></tr><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>27297</td><td>0.1364</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>15152</td><td>0.824</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>3236</td><td>0.176</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (10, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Diabetes_binary | GenHlth) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;5.0&quot;]</td><td>6447</td><td>0.6213</td></tr><tr><td>[&quot;Yes&quot;, &quot;5.0&quot;]</td><td>3930</td><td>0.3787</td></tr><tr><td>[&quot;No&quot;, &quot;Fair&quot;]</td><td>53548</td><td>0.822</td></tr><tr><td>[&quot;Yes&quot;, &quot;Fair&quot;]</td><td>11596</td><td>0.178</td></tr><tr><td>[&quot;No&quot;, &quot;Good&quot;]</td><td>71286</td><td>0.9281</td></tr><tr><td>[&quot;Yes&quot;, &quot;Good&quot;]</td><td>5519</td><td>0.0719</td></tr><tr><td>[&quot;Yes&quot;, &quot;Poor&quot;]</td><td>8503</td><td>0.3128</td></tr><tr><td>[&quot;No&quot;, &quot;Poor&quot;]</td><td>18680</td><td>0.6872</td></tr><tr><td>[&quot;Yes&quot;, &quot;VeryGood&quot;]</td><td>985</td><td>0.0252</td></tr><tr><td>[&quot;No&quot;, &quot;VeryGood&quot;]</td><td>38040</td><td>0.9748</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Diabetes_binary | DiffWalk) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;No&quot;]</td><td>162551</td><td>0.8943</td></tr><tr><td>[&quot;Yes&quot;, &quot;No&quot;]</td><td>19207</td><td>0.1057</td></tr><tr><td>[&quot;No&quot;, &quot;Yes&quot;]</td><td>25450</td><td>0.692</td></tr><tr><td>[&quot;Yes&quot;, &quot;Yes&quot;]</td><td>11326</td><td>0.308</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Diabetes_binary | Sex) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;Female&quot;]</td><td>106426</td><td>0.8696</td></tr><tr><td>[&quot;Yes&quot;, &quot;Female&quot;]</td><td>15962</td><td>0.1304</td></tr><tr><td>[&quot;No&quot;, &quot;Male&quot;]</td><td>81575</td><td>0.8484</td></tr><tr><td>[&quot;Yes&quot;, &quot;Male&quot;]</td><td>14571</td><td>0.1516</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (26, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Diabetes_binary | Age) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;[18, 25)&quot;]</td><td>4851</td><td>0.986</td></tr><tr><td>[&quot;Yes&quot;, &quot;[18, 25)&quot;]</td><td>69</td><td>0.014</td></tr><tr><td>[&quot;No&quot;, &quot;[25,30)&quot;]</td><td>6377</td><td>0.98</td></tr><tr><td>[&quot;Yes&quot;, &quot;[25,30)&quot;]</td><td>130</td><td>0.02</td></tr><tr><td>[&quot;No&quot;, &quot;[30,35)&quot;]</td><td>9338</td><td>0.9724</td></tr><tr><td>[&quot;Yes&quot;, &quot;[30,35)&quot;]</td><td>265</td><td>0.0276</td></tr><tr><td>[&quot;No&quot;, &quot;[35,40)&quot;]</td><td>11380</td><td>0.9549</td></tr><tr><td>[&quot;Yes&quot;, &quot;[35,40)&quot;]</td><td>537</td><td>0.0451</td></tr><tr><td>[&quot;No&quot;, &quot;[40,45)&quot;]</td><td>13010</td><td>0.935</td></tr><tr><td>[&quot;Yes&quot;, &quot;[40,45)&quot;]</td><td>904</td><td>0.065</td></tr><tr><td>[&quot;No&quot;, &quot;[45,50)&quot;]</td><td>15586</td><td>0.9112</td></tr><tr><td>[&quot;Yes&quot;, &quot;[45,50)&quot;]</td><td>1518</td><td>0.0888</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>[&quot;No&quot;, &quot;[55,60)&quot;]</td><td>22830</td><td>0.8602</td></tr><tr><td>[&quot;Yes&quot;, &quot;[55,60)&quot;]</td><td>3711</td><td>0.1398</td></tr><tr><td>[&quot;No&quot;, &quot;[60,65)&quot;]</td><td>23707</td><td>0.8282</td></tr><tr><td>[&quot;Yes&quot;, &quot;[60,65)&quot;]</td><td>4917</td><td>0.1718</td></tr><tr><td>[&quot;No&quot;, &quot;[65,70)&quot;]</td><td>22036</td><td>0.7952</td></tr><tr><td>[&quot;Yes&quot;, &quot;[65,70)&quot;]</td><td>5675</td><td>0.2048</td></tr><tr><td>[&quot;No&quot;, &quot;[70,75)&quot;]</td><td>15819</td><td>0.7815</td></tr><tr><td>[&quot;Yes&quot;, &quot;[70,75)&quot;]</td><td>4422</td><td>0.2185</td></tr><tr><td>[&quot;No&quot;, &quot;[75,80)&quot;]</td><td>10864</td><td>0.7866</td></tr><tr><td>[&quot;Yes&quot;, &quot;[75,80)&quot;]</td><td>2947</td><td>0.2134</td></tr><tr><td>[&quot;Yes&quot;, &quot;[80,inf)&quot;]</td><td>2794</td><td>0.1866</td></tr><tr><td>[&quot;No&quot;, &quot;[80,inf)&quot;]</td><td>12181</td><td>0.8134</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (12, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Diabetes_binary | Education) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;CollegeGraduate&quot;]</td><td>83433</td><td>0.9025</td></tr><tr><td>[&quot;Yes&quot;, &quot;CollegeGraduate&quot;]</td><td>9009</td><td>0.0975</td></tr><tr><td>[&quot;Yes&quot;, &quot;Elementary&quot;]</td><td>1020</td><td>0.2952</td></tr><tr><td>[&quot;No&quot;, &quot;Elementary&quot;]</td><td>2435</td><td>0.7048</td></tr><tr><td>[&quot;No&quot;, &quot;HighSchool&quot;]</td><td>44446</td><td>0.823</td></tr><tr><td>[&quot;Yes&quot;, &quot;HighSchool&quot;]</td><td>9557</td><td>0.177</td></tr><tr><td>[&quot;Yes&quot;, &quot;Never&quot;]</td><td>37</td><td>0.2534</td></tr><tr><td>[&quot;No&quot;, &quot;Never&quot;]</td><td>109</td><td>0.7466</td></tr><tr><td>[&quot;No&quot;, &quot;SomeCollege&quot;]</td><td>51308</td><td>0.8518</td></tr><tr><td>[&quot;Yes&quot;, &quot;SomeCollege&quot;]</td><td>8930</td><td>0.1482</td></tr><tr><td>[&quot;No&quot;, &quot;SomeHighSchool&quot;]</td><td>6270</td><td>0.76</td></tr><tr><td>[&quot;Yes&quot;, &quot;SomeHighSchool&quot;]</td><td>1980</td><td>0.24</td></tr></tbody></table></div></div><div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (16, 3)</small><table border="1" class="dataframe"><thead><tr><th>(Diabetes_binary | Income) : unique values</th><th>abs_freq</th><th>rel_freq</th></tr><tr><td>list[str]</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>[&quot;No&quot;, &quot;[0, 10k)&quot;]</td><td>6431</td><td>0.7573</td></tr><tr><td>[&quot;Yes&quot;, &quot;[0, 10k)&quot;]</td><td>2061</td><td>0.2427</td></tr><tr><td>[&quot;No&quot;, &quot;[10k,15k)&quot;]</td><td>7479</td><td>0.7368</td></tr><tr><td>[&quot;Yes&quot;, &quot;[10k,15k)&quot;]</td><td>2672</td><td>0.2632</td></tr><tr><td>[&quot;No&quot;, &quot;[15k, 20k)&quot;]</td><td>10695</td><td>0.7764</td></tr><tr><td>[&quot;Yes&quot;, &quot;[15k, 20k)&quot;]</td><td>3080</td><td>0.2236</td></tr><tr><td>[&quot;No&quot;, &quot;[20k,25k)&quot;]</td><td>13792</td><td>0.796</td></tr><tr><td>[&quot;Yes&quot;, &quot;[20k,25k)&quot;]</td><td>3534</td><td>0.204</td></tr><tr><td>[&quot;No&quot;, &quot;[25k,35k)&quot;]</td><td>18457</td><td>0.8256</td></tr><tr><td>[&quot;Yes&quot;, &quot;[25k,35k)&quot;]</td><td>3898</td><td>0.1744</td></tr><tr><td>[&quot;No&quot;, &quot;[35k,50k)&quot;]</td><td>26838</td><td>0.8551</td></tr><tr><td>[&quot;Yes&quot;, &quot;[35k,50k)&quot;]</td><td>4546</td><td>0.1449</td></tr><tr><td>[&quot;No&quot;, &quot;[50k,75k)&quot;]</td><td>32756</td><td>0.8788</td></tr><tr><td>[&quot;Yes&quot;, &quot;[50k,75k)&quot;]</td><td>4519</td><td>0.1212</td></tr><tr><td>[&quot;No&quot;, &quot;[75k, inf)&quot;]</td><td>71553</td><td>0.92</td></tr><tr><td>[&quot;Yes&quot;, &quot;[75k, inf)&quot;]</td><td>6223</td><td>0.08</td></tr></tbody></table></div></div></div>
</div>
</section>
<section id="visualization-of-conditional-contingence-table">
<h5><strong>Visualization of conditional contingence table</strong><a class="headerlink" href="#visualization-of-conditional-contingence-table" title="Link to this heading">#</a></h5>
<p>In this section, we will graphically represent the information from the previous tables to facilitate the extraction of insights.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">response_conditioned_barplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">cat_predictor</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">n_rows</span><span class="p">,</span> <span class="n">figsize</span><span class="p">,</span> <span class="n">title_size</span><span class="p">,</span> <span class="n">subtitles_size</span><span class="p">,</span> <span class="n">title_height</span><span class="p">,</span> 
                            <span class="n">xlabel_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">xticks_size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s1">&#39;tab10&#39;</span><span class="p">,</span> <span class="n">ylabel_size</span><span class="o">=</span><span class="mi">11</span><span class="p">):</span>
    
    <span class="n">cond_prop_response</span> <span class="o">=</span> <span class="p">{</span><span class="n">cat_predictor</span><span class="p">:</span> <span class="p">{}}</span>
    <span class="k">for</span> <span class="n">cat</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="n">cat_predictor</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">():</span>

        <span class="n">Y_cond</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="n">cat_predictor</span><span class="p">)</span> <span class="o">==</span> <span class="n">cat</span><span class="p">)[</span><span class="n">response</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
        <span class="n">unique_values</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">Y_cond</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">prop</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">counts</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y_cond</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">cond_prop_response</span><span class="p">[</span><span class="n">cat_predictor</span><span class="p">][</span><span class="n">cat</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">unique_values</span><span class="p">,</span> <span class="n">prop</span><span class="p">))</span>

    <span class="n">n_categories</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cond_prop_response</span><span class="p">[</span><span class="n">cat_predictor</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">n_cols</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">n_categories</span> <span class="o">/</span> <span class="n">n_rows</span><span class="p">))</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">n_rows</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="n">palette</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cat</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cond_prop_response</span><span class="p">[</span><span class="n">cat_predictor</span><span class="p">]):</span>

        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">cond_prop_response</span><span class="p">[</span><span class="n">cat_predictor</span><span class="p">][</span><span class="n">cat</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span> 
                    <span class="n">y</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">cond_prop_response</span><span class="p">[</span><span class="n">cat_predictor</span><span class="p">][</span><span class="n">cat</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span> 
                    <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">cat</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">subtitles_size</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">axes</span><span class="p">)):</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">xlabel_size</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="n">xticks_size</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Proportion&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">ylabel_size</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="s1"> | </span><span class="si">{</span><span class="n">cat_predictor</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">title_size</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">title_height</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="n">hspace</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="n">wspace</span><span class="p">)</span> 
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_categories</span><span class="p">,</span> <span class="n">n_rows</span> <span class="o">*</span> <span class="n">n_cols</span><span class="p">):</span>
        <span class="n">fig</span><span class="o">.</span><span class="n">delaxes</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_unique</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">cat_predictors</span> <span class="p">:</span>
    <span class="n">n_unique</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">diabetes_df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">cat_predictors</span><span class="p">:</span>
    
    <span class="k">if</span> <span class="n">n_unique</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">n_rows</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">title_height</span> <span class="o">=</span> <span class="mf">1.05</span>
    <span class="k">elif</span> <span class="n">n_unique</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">n_unique</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">8</span><span class="p">:</span>
        <span class="n">n_rows</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mf">4.5</span><span class="p">)</span>
        <span class="n">title_height</span> <span class="o">=</span> <span class="mf">1.02</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">n_rows</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span>
        <span class="n">title_height</span> <span class="o">=</span> <span class="mf">0.99</span>


    <span class="n">response_conditioned_barplot</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">diabetes_df_non_cat_NaNs</span><span class="p">,</span> <span class="n">cat_predictor</span><span class="o">=</span><span class="n">col</span><span class="p">,</span> <span class="n">response</span><span class="o">=</span><span class="n">response</span><span class="p">,</span> <span class="n">n_rows</span><span class="o">=</span><span class="n">n_rows</span><span class="p">,</span> 
                             <span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">,</span> <span class="n">title_size</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">subtitles_size</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">title_height</span><span class="o">=</span><span class="n">title_height</span><span class="p">,</span> 
                             <span class="n">xlabel_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">xticks_size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s1">&#39;tab10&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/936f19b5f069f2940650f4647edb0abfa43a2f00b44ee3658e0ce987056b1dd2.png" src="_images/936f19b5f069f2940650f4647edb0abfa43a2f00b44ee3658e0ce987056b1dd2.png" />
<img alt="_images/bc6ef37761654eb52ab065c7a4a7317dc49614d72d1699da6649782a618bc67c.png" src="_images/bc6ef37761654eb52ab065c7a4a7317dc49614d72d1699da6649782a618bc67c.png" />
<img alt="_images/f7437c585149ff17306e410dbaadc92c60f0c27ce7c5f8001e5861fb40a034ef.png" src="_images/f7437c585149ff17306e410dbaadc92c60f0c27ce7c5f8001e5861fb40a034ef.png" />
<img alt="_images/bde64b8e19a612e596cdb9ee67fcaba7e4e2f45ca8f494d8aa1350ddd1356c0a.png" src="_images/bde64b8e19a612e596cdb9ee67fcaba7e4e2f45ca8f494d8aa1350ddd1356c0a.png" />
<img alt="_images/538d764c29b734ba977f695d8a8325ac887f462829f9fc89dd4f5b7261c68559.png" src="_images/538d764c29b734ba977f695d8a8325ac887f462829f9fc89dd4f5b7261c68559.png" />
<img alt="_images/a13b086d5cedc8eca7422c1a1976e3dc8e3d260217492557d6c152094afc25c9.png" src="_images/a13b086d5cedc8eca7422c1a1976e3dc8e3d260217492557d6c152094afc25c9.png" />
<img alt="_images/289670a68a85777fa08730ed22d58cc8654c07eb392a45c22ee244edf204ee8d.png" src="_images/289670a68a85777fa08730ed22d58cc8654c07eb392a45c22ee244edf204ee8d.png" />
<img alt="_images/fe25531aecda428abc59b3f66b076632e50fdda8f5027238eadbd24fb4a6b24a.png" src="_images/fe25531aecda428abc59b3f66b076632e50fdda8f5027238eadbd24fb4a6b24a.png" />
<img alt="_images/d3aa0066060749274fd40dc3fecf0b33980a9d1ce70cfc032e34151fa243b3be.png" src="_images/d3aa0066060749274fd40dc3fecf0b33980a9d1ce70cfc032e34151fa243b3be.png" />
<img alt="_images/6a6b096ecbb5eda41a0c457e7fa3946f7be06f551e33276fb2cb40d0e7661c9a.png" src="_images/6a6b096ecbb5eda41a0c457e7fa3946f7be06f551e33276fb2cb40d0e7661c9a.png" />
<img alt="_images/baa7876c4d87c75f11d52576431a43312c045fdff775d04925a269faa82bed77.png" src="_images/baa7876c4d87c75f11d52576431a43312c045fdff775d04925a269faa82bed77.png" />
<img alt="_images/d9cbcf4b53a5391d823789d31a7805d6fe813889c24cacbffa405468279402f8.png" src="_images/d9cbcf4b53a5391d823789d31a7805d6fe813889c24cacbffa405468279402f8.png" />
<img alt="_images/33464a2e05722af3a6793e369582ffe0394ee9414929525a7c8d4f373c7b35f7.png" src="_images/33464a2e05722af3a6793e369582ffe0394ee9414929525a7c8d4f373c7b35f7.png" />
<img alt="_images/e7f75acab2a23d6570ab17bd48917cbc022d84079adb66958064ae68c3124bcd.png" src="_images/e7f75acab2a23d6570ab17bd48917cbc022d84079adb66958064ae68c3124bcd.png" />
<img alt="_images/1cdcc18fb5a3c813c96bd14f3f8db9b573c2f3dd34f3d9b14e2908cbe369fc7c.png" src="_images/1cdcc18fb5a3c813c96bd14f3f8db9b573c2f3dd34f3d9b14e2908cbe369fc7c.png" />
<img alt="_images/ca044f9cbca18f7ecc4686f432d90340cfd0464817e7a0a92e2eb3b758214229.png" src="_images/ca044f9cbca18f7ecc4686f432d90340cfd0464817e7a0a92e2eb3b758214229.png" />
<img alt="_images/3c5f12bf2e6bd880b6ff187850737820667fe14c38bd8d8b9fa41b16329da803.png" src="_images/3c5f12bf2e6bd880b6ff187850737820667fe14c38bd8d8b9fa41b16329da803.png" />
<img alt="_images/ba4578428012284094d99ccd9e20b16d4ab4ae00bfa23dcf12ff8f242627e84e.png" src="_images/ba4578428012284094d99ccd9e20b16d4ab4ae00bfa23dcf12ff8f242627e84e.png" />
</div>
</div>
<p>These plots help to see the relation of some variables that, in principle, should be related. We are focusing specially in the relation of the response variable with other ones, this analysis could help us in the upcoming parts of the project.</p>
<p>In these last graphs we can see the different relationships that exist between some predictor variables and the response variable. Looking at the distribution of the binary variable in each case we can notice differences. Nor can these relationships be taken literally because some may be due to chance and others to causality.</p>
<p>In the case of age, if it has been proven in other studies that it is a risk factor, there is a greater proportion of cases of diabetes at certain ages, in this case the most notorious would be the highest. Then there are others that are related to this as the state of health, with age your health worsens and these ailments usually indicate a greater presence of diabetes.</p>
<p>In the case of income, to highlight some, there is perhaps no significant change, but there seems to be a higher proportion in people with lower income, this may be due to a poorer quality of food, higher amount of processed foods and higher consumption of sugar because of its lower cost. This is a relationship that should be reviewed if the change is really significant, possibly a test will help us to find out.</p>
<p>A lot of information can be obtained from these relationships, but as I said before, many times they can be due to coincidences that are not too significant. But it is worth bearing them in mind to study them more closely.</p>
<p>More key insights from the plots include:</p>
<ul class="simple">
<li><p>A higher proportion of individuals with high blood pressure (<code class="docutils literal notranslate"><span class="pre">&quot;HighBP&quot;</span></code>) and high cholesterol (<code class="docutils literal notranslate"><span class="pre">&quot;HighChol&quot;</span></code>) are diabetic compared to non-diabetic.</p></li>
<li><p>A substantial majority of both diabetic and non-diabetic individuals have had cholesterol checks (<code class="docutils literal notranslate"><span class="pre">&quot;CholCheck&quot;</span></code>), but the proportion is slightly higher for diabetics.</p></li>
<li><p>Smoking (<code class="docutils literal notranslate"><span class="pre">&quot;Smoker&quot;</span></code>) is more prevalent among diabetics than non-diabetics.</p></li>
<li><p>Stroke (<code class="docutils literal notranslate"><span class="pre">&quot;Stroke&quot;</span></code>) and heart disease or attack (<code class="docutils literal notranslate"><span class="pre">&quot;HeartDiseaseorAttack&quot;</span></code>) occurrences are notably higher in diabetics.</p></li>
<li><p>Diabetics are less likely to engage in physical activity (<code class="docutils literal notranslate"><span class="pre">&quot;PhysActivity&quot;</span></code>) than non-diabetics.</p></li>
<li><p>Diabetics have a lower consumption of fruits (<code class="docutils literal notranslate"><span class="pre">&quot;Fruits&quot;</span></code>) and vegetables (<code class="docutils literal notranslate"><span class="pre">&quot;Veggies&quot;</span></code>), and lower heavy alcohol consumption (<code class="docutils literal notranslate"><span class="pre">&quot;HvyAlcoholConsump&quot;</span></code>).</p></li>
<li><p>Most individuals, regardless of diabetes status, have some form of healthcare (<code class="docutils literal notranslate"><span class="pre">&quot;AnyHealthcare&quot;</span></code>), but the proportion is marginally higher in diabetics.</p></li>
<li><p>Diabetics are more likely to report cost as a barrier to seeing a doctor (<code class="docutils literal notranslate"><span class="pre">&quot;NoDocbcCost&quot;</span></code>).</p></li>
<li><p>General health (<code class="docutils literal notranslate"><span class="pre">&quot;GenHlth&quot;</span></code>) ratings are poorer among diabetics, with higher frequencies of <code class="docutils literal notranslate"><span class="pre">Fair</span></code> and <code class="docutils literal notranslate"><span class="pre">Poor</span></code> ratings.</p></li>
<li><p>Difficulty walking (<code class="docutils literal notranslate"><span class="pre">&quot;DiffWalk&quot;</span></code>) is more frequently reported by diabetics.</p></li>
<li><p>Gender (<code class="docutils literal notranslate"><span class="pre">&quot;Sex&quot;</span></code>) shows a balanced distribution across diabetes status, with a slight female preponderance.</p></li>
<li><p>Age shows an increasing trend of diabetes prevalence with advancing age.</p></li>
<li><p>Education and income show varied distribution, with a tendency towards lower education and income levels among diabetics.</p></li>
</ul>
<hr class="docutils" />
</section>
</section>
</section>
</section>
<hr class="docutils" />
<section id="intro-to-machine-learning-with-sklearn">
<h2><strong>Intro to Machine Learning with <code class="docutils literal notranslate"><span class="pre">Sklearn</span></code></strong><a class="headerlink" href="#intro-to-machine-learning-with-sklearn" title="Link to this heading">#</a></h2>
<section id="defining-the-response-and-predictors">
<h3><strong>Defining the response and predictors</strong><a class="headerlink" href="#defining-the-response-and-predictors" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">quant_columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">diabetes_df</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">diabetes_df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">pl</span><span class="o">.</span><span class="n">Float64</span><span class="p">]</span>
<span class="n">cat_columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">diabetes_df</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">diabetes_df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">pl</span><span class="o">.</span><span class="n">Utf8</span><span class="p">]</span>

<span class="n">response</span> <span class="o">=</span> <span class="s1">&#39;Diabetes_binary&#39;</span>
<span class="n">predictors</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">diabetes_df</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">col</span> <span class="o">!=</span> <span class="n">response</span><span class="p">]</span>
<span class="n">quant_predictors</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">predictors</span> <span class="k">if</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">quant_columns</span><span class="p">]</span>
<span class="n">cat_predictors</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">predictors</span> <span class="k">if</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">cat_columns</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">diabetes_df</span><span class="p">[</span><span class="n">response</span><span class="p">]</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">diabetes_df</span><span class="p">[</span><span class="n">predictors</span><span class="p">]</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0          No
1          No
2          No
3          No
4          No
         ... 
253675     No
253676    Yes
253677     No
253678     No
253679    Yes
Name: Diabetes_binary, Length: 253680, dtype: object
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>HighBP</th>
      <th>HighChol</th>
      <th>CholCheck</th>
      <th>BMI</th>
      <th>Smoker</th>
      <th>Stroke</th>
      <th>HeartDiseaseorAttack</th>
      <th>PhysActivity</th>
      <th>Fruits</th>
      <th>Veggies</th>
      <th>...</th>
      <th>AnyHealthcare</th>
      <th>NoDocbcCost</th>
      <th>GenHlth</th>
      <th>MentHlth</th>
      <th>PhysHlth</th>
      <th>DiffWalk</th>
      <th>Sex</th>
      <th>Age</th>
      <th>Education</th>
      <th>Income</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Yes</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>40.0</td>
      <td>Yes</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>...</td>
      <td>Yes</td>
      <td>No</td>
      <td>5.0</td>
      <td>18.0</td>
      <td>15.0</td>
      <td>Yes</td>
      <td>Female</td>
      <td>[60,65)</td>
      <td>HighSchool</td>
      <td>[15k, 20k)</td>
    </tr>
    <tr>
      <th>1</th>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>25.0</td>
      <td>Yes</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>No</td>
      <td>No</td>
      <td>...</td>
      <td>No</td>
      <td>Yes</td>
      <td>Fair</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>No</td>
      <td>Female</td>
      <td>[50,55)</td>
      <td>CollegeGraduate</td>
      <td>[0, 10k)</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Yes</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>28.0</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>No</td>
      <td>...</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>5.0</td>
      <td>30.0</td>
      <td>30.0</td>
      <td>Yes</td>
      <td>Female</td>
      <td>[60,65)</td>
      <td>HighSchool</td>
      <td>[75k, inf)</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Yes</td>
      <td>No</td>
      <td>Yes</td>
      <td>27.0</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>...</td>
      <td>Yes</td>
      <td>No</td>
      <td>Good</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>No</td>
      <td>Female</td>
      <td>[70,75)</td>
      <td>SomeHighSchool</td>
      <td>[35k,50k)</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Yes</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>24.0</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>...</td>
      <td>Yes</td>
      <td>No</td>
      <td>Good</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>No</td>
      <td>Female</td>
      <td>[70,75)</td>
      <td>SomeCollege</td>
      <td>[20k,25k)</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>253675</th>
      <td>Yes</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>45.0</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>...</td>
      <td>Yes</td>
      <td>No</td>
      <td>Fair</td>
      <td>0.0</td>
      <td>5.0</td>
      <td>No</td>
      <td>Male</td>
      <td>[40,45)</td>
      <td>CollegeGraduate</td>
      <td>[50k,75k)</td>
    </tr>
    <tr>
      <th>253676</th>
      <td>Yes</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>18.0</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>...</td>
      <td>Yes</td>
      <td>No</td>
      <td>Poor</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>Yes</td>
      <td>Female</td>
      <td>[70,75)</td>
      <td>Elementary</td>
      <td>[20k,25k)</td>
    </tr>
    <tr>
      <th>253677</th>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>28.0</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>None</td>
      <td>...</td>
      <td>Yes</td>
      <td>No</td>
      <td>VeryGood</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>No</td>
      <td>Female</td>
      <td>[25,30)</td>
      <td>SomeCollege</td>
      <td>[10k,15k)</td>
    </tr>
    <tr>
      <th>253678</th>
      <td>Yes</td>
      <td>No</td>
      <td>Yes</td>
      <td>23.0</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>...</td>
      <td>Yes</td>
      <td>No</td>
      <td>Fair</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>No</td>
      <td>Male</td>
      <td>[50,55)</td>
      <td>SomeCollege</td>
      <td>[0, 10k)</td>
    </tr>
    <tr>
      <th>253679</th>
      <td>Yes</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>25.0</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>No</td>
      <td>...</td>
      <td>Yes</td>
      <td>No</td>
      <td>Good</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>No</td>
      <td>Female</td>
      <td>[60,65)</td>
      <td>CollegeGraduate</td>
      <td>[10k,15k)</td>
    </tr>
  </tbody>
</table>
<p>253680 rows × 21 columns</p>
</div></div></div>
</div>
<p>Things to consider to be able to use the predictors and response properly with <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> estimators:</p>
<ul class="simple">
<li><p>They have to be made of numerical type objects (int or float).</p></li>
<li><p>They don’t have to contain missing values (NaN)</p></li>
</ul>
<p>Otherwise an error will arise.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Encoding the categorical variables using an ordinal encoder</span>
<span class="n">enc</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">()</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">X_cat</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cat_predictors</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_quant</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">quant_predictors</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">X_quant</span><span class="p">,</span> <span class="n">X_cat</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[40., 18., 15., ...,  8.,  2.,  2.],
       [25.,  0.,  0., ...,  6.,  0.,  0.],
       [28., 30., 30., ...,  8.,  2.,  7.],
       ...,
       [28.,  0.,  0., ...,  1.,  4.,  1.],
       [23.,  0.,  0., ...,  6.,  4.,  0.],
       [25.,  0., nan, ...,  8.,  0.,  1.]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Filling the missing values using a simple imputer</span>
<span class="n">simple_imputer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">missing_values</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">simple_imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0
</pre></div>
</div>
</div>
</div>
<p>This is a naive approach since we are imputing the categorical variables also with the mean.</p>
<p>The most correct approach is to apply this separately for que quant and cat variables, applying one strategy for the first an another for the second. We will see this approach later.</p>
</section>
<section id="defining-outer-evaluation-train-test-split">
<h3><strong>Defining outer evaluation: train-test split</strong><a class="headerlink" href="#defining-outer-evaluation-train-test-split" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Defining the outer-evaluation: train-test split.</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(190260, 21)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(63420, 21)
</pre></div>
</div>
</div>
</div>
</section>
<section id="defining-inner-evaluation-k-fold-cross-validation">
<h3><strong>Defining inner evaluation: K-Fold Cross Validation</strong><a class="headerlink" href="#defining-inner-evaluation-k-fold-cross-validation" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Defining the inner-evaluation: k-fold cross </span>
<span class="n">inner</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="c1"># inner = Fold(n_splits=3, shuffle=True, random_state=123)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="defining-some-models">
<h3><strong>Defining some models</strong><a class="headerlink" href="#defining-some-models" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">)</span>
<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">RF</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">knn</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">RF</span><span class="p">]</span>
<span class="n">model_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;knn&#39;</span><span class="p">,</span> <span class="s1">&#39;tree&#39;</span><span class="p">,</span> <span class="s1">&#39;RF&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><strong>Training a model (fit method)</strong></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">RF</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>RandomForestClassifier(max_depth=10, max_features=0.8, min_samples_leaf=3,
                       n_estimators=40, random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">RandomForestClassifier</label><div class="sk-toggleable__content"><pre>RandomForestClassifier(max_depth=10, max_features=0.8, min_samples_leaf=3,
                       n_estimators=40, random_state=123)</pre></div></div></div></div></div></div></div>
</div>
<ul class="simple">
<li><p><strong>Predicting using a model (predict method)</strong></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training predictions</span>
<span class="n">Y_train_hat</span> <span class="o">=</span> <span class="n">RF</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Testing predictions</span>
<span class="n">Y_test_hat</span> <span class="o">=</span> <span class="n">RF</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><strong>Computing a score</strong></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training score</span>
<span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_train_hat</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5716832507119044
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Testing score</span>
<span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_test_hat</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5574103382707574
</pre></div>
</div>
</div>
</div>
</section>
<section id="applying-inner-evaluation">
<h3><strong>Applying inner evaluation</strong><a class="headerlink" href="#applying-inner-evaluation" title="Link to this heading">#</a></h3>
<section id="without-hpo">
<h4><strong>Without HPO</strong><a class="headerlink" href="#without-hpo" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scores_arr</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">,</span> <span class="n">estimator</span><span class="o">=</span><span class="n">RF</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">inner</span><span class="p">)</span>
<span class="n">scores_arr</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.55845631, 0.5590863 , 0.55492997])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores_arr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5574908591434647
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">model</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">model_names</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">scores_arr</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">,</span> <span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">inner</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">scores_arr</span><span class="p">)</span>
    <span class="n">scores</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores_arr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>KNeighborsClassifier(metric=&#39;euclidean&#39;)
[0.56353619 0.5624154  0.55945893]
DecisionTreeClassifier(max_depth=10, min_samples_leaf=3, random_state=123)
[0.56811021 0.5637455  0.55970761]
RandomForestClassifier(max_depth=10, max_features=0.8, min_samples_leaf=3,
                       n_estimators=40, random_state=123)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.55845631 0.5590863  0.55492997]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;knn&#39;: 0.5618035076772085,
 &#39;tree&#39;: 0.5638544376891831,
 &#39;RF&#39;: 0.5574908591434647}
</pre></div>
</div>
</div>
</div>
</section>
<section id="with-hpo">
<h4><strong>With HPO</strong><a class="headerlink" href="#with-hpo" title="Link to this heading">#</a></h4>
<section id="grid-search">
<h5><strong>Grid Search</strong><a class="headerlink" href="#grid-search" title="Link to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">40</span><span class="p">],</span>
              <span class="s1">&#39;max_features&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>
              <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> 
              <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span>
              <span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">]}</span>

<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">RF</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>   
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitting 3 folds for each of 8 candidates, totalling 24 fits
</pre></div>
</div>
<div class="output text_html"><style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=123, shuffle=True),
             estimator=RandomForestClassifier(max_depth=10, max_features=0.8,
                                              min_samples_leaf=3,
                                              n_estimators=40,
                                              random_state=123),
             param_grid={&#x27;max_depth&#x27;: [2, 5], &#x27;max_features&#x27;: [0.5, 0.8],
                         &#x27;min_samples_leaf&#x27;: [3], &#x27;min_samples_split&#x27;: [2],
                         &#x27;n_estimators&#x27;: [25, 40]},
             scoring=&#x27;balanced_accuracy&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" ><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">GridSearchCV</label><div class="sk-toggleable__content"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=123, shuffle=True),
             estimator=RandomForestClassifier(max_depth=10, max_features=0.8,
                                              min_samples_leaf=3,
                                              n_estimators=40,
                                              random_state=123),
             param_grid={&#x27;max_depth&#x27;: [2, 5], &#x27;max_features&#x27;: [0.5, 0.8],
                         &#x27;min_samples_leaf&#x27;: [3], &#x27;min_samples_split&#x27;: [2],
                         &#x27;n_estimators&#x27;: [25, 40]},
             scoring=&#x27;balanced_accuracy&#x27;, verbose=True)</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" ><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: RandomForestClassifier</label><div class="sk-toggleable__content"><pre>RandomForestClassifier(max_depth=10, max_features=0.8, min_samples_leaf=3,
                       n_estimators=40, random_state=123)</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox" ><label for="sk-estimator-id-4" class="sk-toggleable__label sk-toggleable__label-arrow">RandomForestClassifier</label><div class="sk-toggleable__content"><pre>RandomForestClassifier(max_depth=10, max_features=0.8, min_samples_leaf=3,
                       n_estimators=40, random_state=123)</pre></div></div></div></div></div></div></div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid_search</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[{&#39;max_depth&#39;: 2,
  &#39;max_features&#39;: 0.5,
  &#39;min_samples_leaf&#39;: 3,
  &#39;min_samples_split&#39;: 2,
  &#39;n_estimators&#39;: 25},
 {&#39;max_depth&#39;: 2,
  &#39;max_features&#39;: 0.5,
  &#39;min_samples_leaf&#39;: 3,
  &#39;min_samples_split&#39;: 2,
  &#39;n_estimators&#39;: 40},
 {&#39;max_depth&#39;: 2,
  &#39;max_features&#39;: 0.8,
  &#39;min_samples_leaf&#39;: 3,
  &#39;min_samples_split&#39;: 2,
  &#39;n_estimators&#39;: 25},
 {&#39;max_depth&#39;: 2,
  &#39;max_features&#39;: 0.8,
  &#39;min_samples_leaf&#39;: 3,
  &#39;min_samples_split&#39;: 2,
  &#39;n_estimators&#39;: 40},
 {&#39;max_depth&#39;: 5,
  &#39;max_features&#39;: 0.5,
  &#39;min_samples_leaf&#39;: 3,
  &#39;min_samples_split&#39;: 2,
  &#39;n_estimators&#39;: 25},
 {&#39;max_depth&#39;: 5,
  &#39;max_features&#39;: 0.5,
  &#39;min_samples_leaf&#39;: 3,
  &#39;min_samples_split&#39;: 2,
  &#39;n_estimators&#39;: 40},
 {&#39;max_depth&#39;: 5,
  &#39;max_features&#39;: 0.8,
  &#39;min_samples_leaf&#39;: 3,
  &#39;min_samples_split&#39;: 2,
  &#39;n_estimators&#39;: 25},
 {&#39;max_depth&#39;: 5,
  &#39;max_features&#39;: 0.8,
  &#39;min_samples_leaf&#39;: 3,
  &#39;min_samples_split&#39;: 2,
  &#39;n_estimators&#39;: 40}]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid_search</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">&#39;mean_test_score&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.5       , 0.5       , 0.5       , 0.5       , 0.52701078,
       0.52812129, 0.53724633, 0.54013562])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;max_depth&#39;: 5,
 &#39;max_features&#39;: 0.8,
 &#39;min_samples_leaf&#39;: 3,
 &#39;min_samples_split&#39;: 2,
 &#39;n_estimators&#39;: 40}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid_search</span><span class="o">.</span><span class="n">best_score_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5401356236358016
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid_search</span><span class="o">.</span><span class="n">best_estimator_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>RandomForestClassifier(max_depth=5, max_features=0.8, min_samples_leaf=3,
                       n_estimators=40, random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox" checked><label for="sk-estimator-id-5" class="sk-toggleable__label sk-toggleable__label-arrow">RandomForestClassifier</label><div class="sk-toggleable__content"><pre>RandomForestClassifier(max_depth=5, max_features=0.8, min_samples_leaf=3,
                       n_estimators=40, random_state=123)</pre></div></div></div></div></div></div></div>
</div>
</section>
<section id="random-search">
<h5><strong>Random Search</strong><a class="headerlink" href="#random-search" title="Link to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">75</span><span class="p">],</span>
              <span class="s1">&#39;max_features&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
              <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span> 
              <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">12</span><span class="p">],</span>
              <span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">]}</span>

<span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">RF</span><span class="p">,</span> <span class="n">param_distributions</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                            <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                            <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span> 
                            <span class="p">)</span>   
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitting 3 folds for each of 5 candidates, totalling 15 fits
</pre></div>
</div>
<div class="output text_html"><style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-4" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>RandomizedSearchCV(cv=StratifiedKFold(n_splits=3, random_state=123, shuffle=True),
                   estimator=RandomForestClassifier(max_depth=10,
                                                    max_features=0.8,
                                                    min_samples_leaf=3,
                                                    n_estimators=40,
                                                    random_state=123),
                   n_iter=5,
                   param_distributions={&#x27;max_depth&#x27;: [2, 5, 15, 30],
                                        &#x27;max_features&#x27;: [0.5, 0.8, 1],
                                        &#x27;min_samples_leaf&#x27;: [3, 10],
                                        &#x27;min_samples_split&#x27;: [2, 7, 12],
                                        &#x27;n_estimators&#x27;: [25, 40, 75]},
                   random_state=123, scoring=&#x27;balanced_accuracy&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-6" type="checkbox" ><label for="sk-estimator-id-6" class="sk-toggleable__label sk-toggleable__label-arrow">RandomizedSearchCV</label><div class="sk-toggleable__content"><pre>RandomizedSearchCV(cv=StratifiedKFold(n_splits=3, random_state=123, shuffle=True),
                   estimator=RandomForestClassifier(max_depth=10,
                                                    max_features=0.8,
                                                    min_samples_leaf=3,
                                                    n_estimators=40,
                                                    random_state=123),
                   n_iter=5,
                   param_distributions={&#x27;max_depth&#x27;: [2, 5, 15, 30],
                                        &#x27;max_features&#x27;: [0.5, 0.8, 1],
                                        &#x27;min_samples_leaf&#x27;: [3, 10],
                                        &#x27;min_samples_split&#x27;: [2, 7, 12],
                                        &#x27;n_estimators&#x27;: [25, 40, 75]},
                   random_state=123, scoring=&#x27;balanced_accuracy&#x27;, verbose=True)</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-7" type="checkbox" ><label for="sk-estimator-id-7" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: RandomForestClassifier</label><div class="sk-toggleable__content"><pre>RandomForestClassifier(max_depth=10, max_features=0.8, min_samples_leaf=3,
                       n_estimators=40, random_state=123)</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-8" type="checkbox" ><label for="sk-estimator-id-8" class="sk-toggleable__label sk-toggleable__label-arrow">RandomForestClassifier</label><div class="sk-toggleable__content"><pre>RandomForestClassifier(max_depth=10, max_features=0.8, min_samples_leaf=3,
                       n_estimators=40, random_state=123)</pre></div></div></div></div></div></div></div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random_search</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[{&#39;n_estimators&#39;: 25,
  &#39;min_samples_split&#39;: 2,
  &#39;min_samples_leaf&#39;: 10,
  &#39;max_features&#39;: 0.5,
  &#39;max_depth&#39;: 30},
 {&#39;n_estimators&#39;: 25,
  &#39;min_samples_split&#39;: 7,
  &#39;min_samples_leaf&#39;: 3,
  &#39;max_features&#39;: 1,
  &#39;max_depth&#39;: 30},
 {&#39;n_estimators&#39;: 40,
  &#39;min_samples_split&#39;: 7,
  &#39;min_samples_leaf&#39;: 10,
  &#39;max_features&#39;: 0.8,
  &#39;max_depth&#39;: 2},
 {&#39;n_estimators&#39;: 75,
  &#39;min_samples_split&#39;: 2,
  &#39;min_samples_leaf&#39;: 3,
  &#39;max_features&#39;: 0.8,
  &#39;max_depth&#39;: 15},
 {&#39;n_estimators&#39;: 25,
  &#39;min_samples_split&#39;: 7,
  &#39;min_samples_leaf&#39;: 3,
  &#39;max_features&#39;: 1,
  &#39;max_depth&#39;: 5}]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random_search</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">&#39;mean_test_score&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.56483805, 0.52558002, 0.5       , 0.56783233, 0.5       ])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;n_estimators&#39;: 75,
 &#39;min_samples_split&#39;: 2,
 &#39;min_samples_leaf&#39;: 3,
 &#39;max_features&#39;: 0.8,
 &#39;max_depth&#39;: 15}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random_search</span><span class="o">.</span><span class="n">best_score_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5678323261825794
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random_search</span><span class="o">.</span><span class="n">best_estimator_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-5" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>RandomForestClassifier(max_depth=15, max_features=0.8, min_samples_leaf=3,
                       n_estimators=75, random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-9" type="checkbox" checked><label for="sk-estimator-id-9" class="sk-toggleable__label sk-toggleable__label-arrow">RandomForestClassifier</label><div class="sk-toggleable__content"><pre>RandomForestClassifier(max_depth=15, max_features=0.8, min_samples_leaf=3,
                       n_estimators=75, random_state=123)</pre></div></div></div></div></div></div></div>
</div>
</section>
<section id="optuna">
<h5><strong>Optuna</strong><a class="headerlink" href="#optuna" title="Link to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">param_grid</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;n_estimators&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">75</span><span class="p">]),</span>
        <span class="s1">&#39;max_features&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s1">&#39;max_features&#39;</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
        <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;max_depth&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">]),</span>
        <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;min_samples_split&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span>
        <span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optuna_search</span> <span class="o">=</span> <span class="n">OptunaSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">RF</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                               <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                               <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>   
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optuna_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-25 18:59:14,351] A new study created in memory with name: no-name-e299d9e8-120a-4155-a0c4-c1da29c986fc
[I 2024-03-25 18:59:24,191] Trial 0 finished with value: 0.5440858669032845 and parameters: {&#39;n_estimators&#39;: 25, &#39;max_features&#39;: 0.8, &#39;max_depth&#39;: 7, &#39;min_samples_split&#39;: 11, &#39;min_samples_leaf&#39;: 10}. Best is trial 0 with value: 0.5440858669032845.
[I 2024-03-25 18:59:28,520] Trial 1 finished with value: 0.5 and parameters: {&#39;n_estimators&#39;: 25, &#39;max_features&#39;: 0.7, &#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 17, &#39;min_samples_leaf&#39;: 22}. Best is trial 0 with value: 0.5440858669032845.
[I 2024-03-25 18:59:39,503] Trial 2 finished with value: 0.5540121521305189 and parameters: {&#39;n_estimators&#39;: 25, &#39;max_features&#39;: 0.6, &#39;max_depth&#39;: 10, &#39;min_samples_split&#39;: 12, &#39;min_samples_leaf&#39;: 12}. Best is trial 2 with value: 0.5540121521305189.
[I 2024-03-25 18:59:46,402] Trial 3 finished with value: 0.5363257606753301 and parameters: {&#39;n_estimators&#39;: 25, &#39;max_features&#39;: 0.7, &#39;max_depth&#39;: 5, &#39;min_samples_split&#39;: 9, &#39;min_samples_leaf&#39;: 11}. Best is trial 2 with value: 0.5540121521305189.
[W 2024-03-25 18:59:49,829] Trial 4 failed with parameters: {&#39;n_estimators&#39;: 25, &#39;max_features&#39;: 1.0, &#39;max_depth&#39;: 10, &#39;min_samples_split&#39;: 15, &#39;min_samples_leaf&#39;: 10} because of the following error: KeyboardInterrupt().
Traceback (most recent call last):
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\optuna\study\_optimize.py&quot;, line 200, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File &quot;C:\Users\fscielzo\Documents\DataScience-GitHub\Regression\ML\PyML.py&quot;, line 46, in &lt;lambda&gt;
  File &quot;C:\Users\fscielzo\Documents\DataScience-GitHub\Regression\ML\PyML.py&quot;, line 39, in objective
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\model_selection\_validation.py&quot;, line 562, in cross_val_score
    cv_results = cross_validate(
                 ^^^^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\_param_validation.py&quot;, line 211, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\model_selection\_validation.py&quot;, line 309, in cross_validate
    results = parallel(
              ^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\parallel.py&quot;, line 65, in __call__
    return super().__call__(iterable_with_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\joblib\parallel.py&quot;, line 1863, in __call__
    return output if self.return_generator else list(output)
                                                ^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\joblib\parallel.py&quot;, line 1792, in _get_sequential_output
    res = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\parallel.py&quot;, line 127, in __call__
    return self.function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\model_selection\_validation.py&quot;, line 729, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\base.py&quot;, line 1152, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\ensemble\_forest.py&quot;, line 456, in fit
    trees = Parallel(
            ^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\parallel.py&quot;, line 65, in __call__
    return super().__call__(iterable_with_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\joblib\parallel.py&quot;, line 1863, in __call__
    return output if self.return_generator else list(output)
                                                ^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\joblib\parallel.py&quot;, line 1792, in _get_sequential_output
    res = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\parallel.py&quot;, line 127, in __call__
    return self.function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\ensemble\_forest.py&quot;, line 188, in _parallel_build_trees
    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\base.py&quot;, line 1152, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\tree\_classes.py&quot;, line 959, in fit
    super()._fit(
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\tree\_classes.py&quot;, line 443, in _fit
    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)
KeyboardInterrupt
[W 2024-03-25 18:59:49,848] Trial 4 failed with value None.
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">119</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">optuna_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

<span class="nn">File ~\Documents\DataScience-GitHub\Regression\ML\PyML.py:46,</span> in <span class="ni">fit</span><span class="nt">(self, X, y)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\optuna\study\study.py:451,</span> in <span class="ni">Study.optimize</span><span class="nt">(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)</span>
<span class="g g-Whitespace">    </span><span class="mi">348</span> <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">349</span>     <span class="bp">self</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">350</span>     <span class="n">func</span><span class="p">:</span> <span class="n">ObjectiveFuncType</span><span class="p">,</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">357</span>     <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">358</span> <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">359</span><span class="w">     </span><span class="sd">&quot;&quot;&quot;Optimize an objective function.</span>
<span class="g g-Whitespace">    </span><span class="mi">360</span><span class="sd"> </span>
<span class="g g-Whitespace">    </span><span class="mi">361</span><span class="sd">     Optimization is done by choosing a suitable set of hyperparameter values from a given</span>
<span class="sd">   (...)</span>
<span class="g g-Whitespace">    </span><span class="mi">449</span><span class="sd">             If nested invocation of this method occurs.</span>
<span class="g g-Whitespace">    </span><span class="mi">450</span><span class="sd">     &quot;&quot;&quot;</span>
<span class="ne">--&gt; </span><span class="mi">451</span>     <span class="n">_optimize</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">452</span>         <span class="n">study</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">453</span>         <span class="n">func</span><span class="o">=</span><span class="n">func</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">454</span>         <span class="n">n_trials</span><span class="o">=</span><span class="n">n_trials</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">455</span>         <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">456</span>         <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">457</span>         <span class="n">catch</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">catch</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">catch</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="n">catch</span><span class="p">,),</span>
<span class="g g-Whitespace">    </span><span class="mi">458</span>         <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">459</span>         <span class="n">gc_after_trial</span><span class="o">=</span><span class="n">gc_after_trial</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">460</span>         <span class="n">show_progress_bar</span><span class="o">=</span><span class="n">show_progress_bar</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">461</span>     <span class="p">)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\optuna\study\_optimize.py:66,</span> in <span class="ni">_optimize</span><span class="nt">(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)</span>
<span class="g g-Whitespace">     </span><span class="mi">64</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">65</span>     <span class="k">if</span> <span class="n">n_jobs</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
<span class="ne">---&gt; </span><span class="mi">66</span>         <span class="n">_optimize_sequential</span><span class="p">(</span>
<span class="g g-Whitespace">     </span><span class="mi">67</span>             <span class="n">study</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">68</span>             <span class="n">func</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">69</span>             <span class="n">n_trials</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">70</span>             <span class="n">timeout</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">71</span>             <span class="n">catch</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">72</span>             <span class="n">callbacks</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">73</span>             <span class="n">gc_after_trial</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">74</span>             <span class="n">reseed_sampler_rng</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">75</span>             <span class="n">time_start</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">76</span>             <span class="n">progress_bar</span><span class="o">=</span><span class="n">progress_bar</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">77</span>         <span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">78</span>     <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">79</span>         <span class="k">if</span> <span class="n">n_jobs</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\optuna\study\_optimize.py:163,</span> in <span class="ni">_optimize_sequential</span><span class="nt">(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)</span>
<span class="g g-Whitespace">    </span><span class="mi">160</span>         <span class="k">break</span>
<span class="g g-Whitespace">    </span><span class="mi">162</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">163</span>     <span class="n">frozen_trial</span> <span class="o">=</span> <span class="n">_run_trial</span><span class="p">(</span><span class="n">study</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">catch</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">164</span> <span class="k">finally</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">165</span>     <span class="c1"># The following line mitigates memory problems that can be occurred in some</span>
<span class="g g-Whitespace">    </span><span class="mi">166</span>     <span class="c1"># environments (e.g., services that use computing containers such as GitHub Actions).</span>
<span class="g g-Whitespace">    </span><span class="mi">167</span>     <span class="c1"># Please refer to the following PR for further details:</span>
<span class="g g-Whitespace">    </span><span class="mi">168</span>     <span class="c1"># https://github.com/optuna/optuna/pull/325.</span>
<span class="g g-Whitespace">    </span><span class="mi">169</span>     <span class="k">if</span> <span class="n">gc_after_trial</span><span class="p">:</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\optuna\study\_optimize.py:251,</span> in <span class="ni">_run_trial</span><span class="nt">(study, func, catch)</span>
<span class="g g-Whitespace">    </span><span class="mi">244</span>         <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;Should not reach.&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">246</span> <span class="k">if</span> <span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">247</span>     <span class="n">frozen_trial</span><span class="o">.</span><span class="n">state</span> <span class="o">==</span> <span class="n">TrialState</span><span class="o">.</span><span class="n">FAIL</span>
<span class="g g-Whitespace">    </span><span class="mi">248</span>     <span class="ow">and</span> <span class="n">func_err</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span class="g g-Whitespace">    </span><span class="mi">249</span>     <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func_err</span><span class="p">,</span> <span class="n">catch</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">250</span> <span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">251</span>     <span class="k">raise</span> <span class="n">func_err</span>
<span class="g g-Whitespace">    </span><span class="mi">252</span> <span class="k">return</span> <span class="n">frozen_trial</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\optuna\study\_optimize.py:200,</span> in <span class="ni">_run_trial</span><span class="nt">(study, func, catch)</span>
<span class="g g-Whitespace">    </span><span class="mi">198</span> <span class="k">with</span> <span class="n">get_heartbeat_thread</span><span class="p">(</span><span class="n">trial</span><span class="o">.</span><span class="n">_trial_id</span><span class="p">,</span> <span class="n">study</span><span class="o">.</span><span class="n">_storage</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">199</span>     <span class="k">try</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">200</span>         <span class="n">value_or_values</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">201</span>     <span class="k">except</span> <span class="n">exceptions</span><span class="o">.</span><span class="n">TrialPruned</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">202</span>         <span class="c1"># TODO(mamu): Handle multi-objective cases.</span>
<span class="g g-Whitespace">    </span><span class="mi">203</span>         <span class="n">state</span> <span class="o">=</span> <span class="n">TrialState</span><span class="o">.</span><span class="n">PRUNED</span>

<span class="nn">File ~\Documents\DataScience-GitHub\Regression\ML\PyML.py:46,</span> in <span class="ni">&lt;lambda&gt;</span><span class="nt">(trial)</span>

<span class="nn">File ~\Documents\DataScience-GitHub\Regression\ML\PyML.py:39,</span> in <span class="ni">objective</span><span class="nt">(self, trial, X, y)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\model_selection\_validation.py:562,</span> in <span class="ni">cross_val_score</span><span class="nt">(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)</span>
<span class="g g-Whitespace">    </span><span class="mi">559</span> <span class="c1"># To ensure multimetric format is not supported</span>
<span class="g g-Whitespace">    </span><span class="mi">560</span> <span class="n">scorer</span> <span class="o">=</span> <span class="n">check_scoring</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">562</span> <span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">563</span>     <span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">564</span>     <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">565</span>     <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">566</span>     <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">567</span>     <span class="n">scoring</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;score&quot;</span><span class="p">:</span> <span class="n">scorer</span><span class="p">},</span>
<span class="g g-Whitespace">    </span><span class="mi">568</span>     <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">569</span>     <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">570</span>     <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">571</span>     <span class="n">fit_params</span><span class="o">=</span><span class="n">fit_params</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">572</span>     <span class="n">pre_dispatch</span><span class="o">=</span><span class="n">pre_dispatch</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">573</span>     <span class="n">error_score</span><span class="o">=</span><span class="n">error_score</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">574</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">575</span> <span class="k">return</span> <span class="n">cv_results</span><span class="p">[</span><span class="s2">&quot;test_score&quot;</span><span class="p">]</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\_param_validation.py:211,</span> in <span class="ni">validate_params.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">205</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">206</span>     <span class="k">with</span> <span class="n">config_context</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">207</span>         <span class="n">skip_parameter_validation</span><span class="o">=</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">208</span>             <span class="n">prefer_skip_nested_validation</span> <span class="ow">or</span> <span class="n">global_skip_validation</span>
<span class="g g-Whitespace">    </span><span class="mi">209</span>         <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">210</span>     <span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">211</span>         <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">212</span> <span class="k">except</span> <span class="n">InvalidParameterError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">213</span>     <span class="c1"># When the function is just a wrapper around an estimator, we allow</span>
<span class="g g-Whitespace">    </span><span class="mi">214</span>     <span class="c1"># the function to delegate validation to the estimator, but we replace</span>
<span class="g g-Whitespace">    </span><span class="mi">215</span>     <span class="c1"># the name of the estimator by the name of the function in the error</span>
<span class="g g-Whitespace">    </span><span class="mi">216</span>     <span class="c1"># message to avoid confusion.</span>
<span class="g g-Whitespace">    </span><span class="mi">217</span>     <span class="n">msg</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">218</span>         <span class="sa">r</span><span class="s2">&quot;parameter of \w+ must be&quot;</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">219</span>         <span class="sa">f</span><span class="s2">&quot;parameter of </span><span class="si">{</span><span class="n">func</span><span class="o">.</span><span class="vm">__qualname__</span><span class="si">}</span><span class="s2"> must be&quot;</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">220</span>         <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">221</span>     <span class="p">)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\model_selection\_validation.py:309,</span> in <span class="ni">cross_validate</span><span class="nt">(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)</span>
<span class="g g-Whitespace">    </span><span class="mi">306</span> <span class="c1"># We clone the estimator to make sure that all the folds are</span>
<span class="g g-Whitespace">    </span><span class="mi">307</span> <span class="c1"># independent, and that it is pickle-able.</span>
<span class="g g-Whitespace">    </span><span class="mi">308</span> <span class="n">parallel</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span> <span class="n">pre_dispatch</span><span class="o">=</span><span class="n">pre_dispatch</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">309</span> <span class="n">results</span> <span class="o">=</span> <span class="n">parallel</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">310</span>     <span class="n">delayed</span><span class="p">(</span><span class="n">_fit_and_score</span><span class="p">)(</span>
<span class="g g-Whitespace">    </span><span class="mi">311</span>         <span class="n">clone</span><span class="p">(</span><span class="n">estimator</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">312</span>         <span class="n">X</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">313</span>         <span class="n">y</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">314</span>         <span class="n">scorers</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">315</span>         <span class="n">train</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">316</span>         <span class="n">test</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">317</span>         <span class="n">verbose</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">318</span>         <span class="kc">None</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">319</span>         <span class="n">fit_params</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">320</span>         <span class="n">return_train_score</span><span class="o">=</span><span class="n">return_train_score</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">321</span>         <span class="n">return_times</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">322</span>         <span class="n">return_estimator</span><span class="o">=</span><span class="n">return_estimator</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">323</span>         <span class="n">error_score</span><span class="o">=</span><span class="n">error_score</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">324</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">325</span>     <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">indices</span>
<span class="g g-Whitespace">    </span><span class="mi">326</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">328</span> <span class="n">_warn_or_raise_about_fit_failures</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">error_score</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">330</span> <span class="c1"># For callable scoring, the return type is only know after calling. If the</span>
<span class="g g-Whitespace">    </span><span class="mi">331</span> <span class="c1"># return type is a dictionary, the error scores can now be inserted with</span>
<span class="g g-Whitespace">    </span><span class="mi">332</span> <span class="c1"># the correct key.</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\parallel.py:65,</span> in <span class="ni">Parallel.__call__</span><span class="nt">(self, iterable)</span>
<span class="g g-Whitespace">     </span><span class="mi">60</span> <span class="n">config</span> <span class="o">=</span> <span class="n">get_config</span><span class="p">()</span>
<span class="g g-Whitespace">     </span><span class="mi">61</span> <span class="n">iterable_with_config</span> <span class="o">=</span> <span class="p">(</span>
<span class="g g-Whitespace">     </span><span class="mi">62</span>     <span class="p">(</span><span class="n">_with_config</span><span class="p">(</span><span class="n">delayed_func</span><span class="p">,</span> <span class="n">config</span><span class="p">),</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">63</span>     <span class="k">for</span> <span class="n">delayed_func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="n">iterable</span>
<span class="g g-Whitespace">     </span><span class="mi">64</span> <span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">65</span> <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">iterable_with_config</span><span class="p">)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\joblib\parallel.py:1863,</span> in <span class="ni">Parallel.__call__</span><span class="nt">(self, iterable)</span>
<span class="g g-Whitespace">   </span><span class="mi">1861</span>     <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_sequential_output</span><span class="p">(</span><span class="n">iterable</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1862</span>     <span class="nb">next</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1863</span>     <span class="k">return</span> <span class="n">output</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_generator</span> <span class="k">else</span> <span class="nb">list</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1865</span> <span class="c1"># Let&#39;s create an ID that uniquely identifies the current call. If the</span>
<span class="g g-Whitespace">   </span><span class="mi">1866</span> <span class="c1"># call is interrupted early and that the same instance is immediately</span>
<span class="g g-Whitespace">   </span><span class="mi">1867</span> <span class="c1"># re-used, this id will be used to prevent workers that were</span>
<span class="g g-Whitespace">   </span><span class="mi">1868</span> <span class="c1"># concurrently finalizing a task from the previous call to run the</span>
<span class="g g-Whitespace">   </span><span class="mi">1869</span> <span class="c1"># callback.</span>
<span class="g g-Whitespace">   </span><span class="mi">1870</span> <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\joblib\parallel.py:1792,</span> in <span class="ni">Parallel._get_sequential_output</span><span class="nt">(self, iterable)</span>
<span class="g g-Whitespace">   </span><span class="mi">1790</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dispatched_batches</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="g g-Whitespace">   </span><span class="mi">1791</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dispatched_tasks</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="ne">-&gt; </span><span class="mi">1792</span> <span class="n">res</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1793</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_completed_tasks</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="g g-Whitespace">   </span><span class="mi">1794</span> <span class="bp">self</span><span class="o">.</span><span class="n">print_progress</span><span class="p">()</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\parallel.py:127,</span> in <span class="ni">_FuncWrapper.__call__</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">125</span>     <span class="n">config</span> <span class="o">=</span> <span class="p">{}</span>
<span class="g g-Whitespace">    </span><span class="mi">126</span> <span class="k">with</span> <span class="n">config_context</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">127</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\model_selection\_validation.py:729,</span> in <span class="ni">_fit_and_score</span><span class="nt">(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)</span>
<span class="g g-Whitespace">    </span><span class="mi">727</span>         <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">728</span>     <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">729</span>         <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">731</span> <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">732</span>     <span class="c1"># Note fit time as time until error</span>
<span class="g g-Whitespace">    </span><span class="mi">733</span>     <span class="n">fit_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\base.py:1152,</span> in <span class="ni">_fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper</span><span class="nt">(estimator, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1145</span>     <span class="n">estimator</span><span class="o">.</span><span class="n">_validate_params</span><span class="p">()</span>
<span class="g g-Whitespace">   </span><span class="mi">1147</span> <span class="k">with</span> <span class="n">config_context</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1148</span>     <span class="n">skip_parameter_validation</span><span class="o">=</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1149</span>         <span class="n">prefer_skip_nested_validation</span> <span class="ow">or</span> <span class="n">global_skip_validation</span>
<span class="g g-Whitespace">   </span><span class="mi">1150</span>     <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1151</span> <span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1152</span>     <span class="k">return</span> <span class="n">fit_method</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\ensemble\_forest.py:456,</span> in <span class="ni">BaseForest.fit</span><span class="nt">(self, X, y, sample_weight)</span>
<span class="g g-Whitespace">    </span><span class="mi">445</span> <span class="n">trees</span> <span class="o">=</span> <span class="p">[</span>
<span class="g g-Whitespace">    </span><span class="mi">446</span>     <span class="bp">self</span><span class="o">.</span><span class="n">_make_estimator</span><span class="p">(</span><span class="n">append</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
<span class="nn">    447     for i</span> in <span class="ni">range</span><span class="nt">(n_more_estimators)</span>
<span class="g g-Whitespace">    </span><span class="mi">448</span> <span class="p">]</span>
<span class="g g-Whitespace">    </span><span class="mi">450</span> <span class="c1"># Parallel loop: we prefer the threading backend as the Cython code</span>
<span class="g g-Whitespace">    </span><span class="mi">451</span> <span class="c1"># for fitting the trees is internally releasing the Python GIL</span>
<span class="g g-Whitespace">    </span><span class="mi">452</span> <span class="c1"># making threading more efficient than multiprocessing in</span>
<span class="g g-Whitespace">    </span><span class="mi">453</span> <span class="c1"># that case. However, for joblib 0.12+ we respect any</span>
<span class="g g-Whitespace">    </span><span class="mi">454</span> <span class="c1"># parallel_backend contexts set at a higher level,</span>
<span class="g g-Whitespace">    </span><span class="mi">455</span> <span class="c1"># since correctness does not rely on using threads.</span>
<span class="ne">--&gt; </span><span class="mi">456</span> <span class="n">trees</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">457</span>     <span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">458</span>     <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">459</span>     <span class="n">prefer</span><span class="o">=</span><span class="s2">&quot;threads&quot;</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">460</span> <span class="p">)(</span>
<span class="g g-Whitespace">    </span><span class="mi">461</span>     <span class="n">delayed</span><span class="p">(</span><span class="n">_parallel_build_trees</span><span class="p">)(</span>
<span class="g g-Whitespace">    </span><span class="mi">462</span>         <span class="n">t</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">463</span>         <span class="bp">self</span><span class="o">.</span><span class="n">bootstrap</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">464</span>         <span class="n">X</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">465</span>         <span class="n">y</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">466</span>         <span class="n">sample_weight</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">467</span>         <span class="n">i</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">468</span>         <span class="nb">len</span><span class="p">(</span><span class="n">trees</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">469</span>         <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">470</span>         <span class="n">class_weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">471</span>         <span class="n">n_samples_bootstrap</span><span class="o">=</span><span class="n">n_samples_bootstrap</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">472</span>     <span class="p">)</span>
<span class="nn">    473     for i, t</span> in <span class="ni">enumerate</span><span class="nt">(trees)</span>
<span class="g g-Whitespace">    </span><span class="mi">474</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">476</span> <span class="c1"># Collect newly grown trees</span>
<span class="g g-Whitespace">    </span><span class="mi">477</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">trees</span><span class="p">)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\parallel.py:65,</span> in <span class="ni">Parallel.__call__</span><span class="nt">(self, iterable)</span>
<span class="g g-Whitespace">     </span><span class="mi">60</span> <span class="n">config</span> <span class="o">=</span> <span class="n">get_config</span><span class="p">()</span>
<span class="g g-Whitespace">     </span><span class="mi">61</span> <span class="n">iterable_with_config</span> <span class="o">=</span> <span class="p">(</span>
<span class="g g-Whitespace">     </span><span class="mi">62</span>     <span class="p">(</span><span class="n">_with_config</span><span class="p">(</span><span class="n">delayed_func</span><span class="p">,</span> <span class="n">config</span><span class="p">),</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">63</span>     <span class="k">for</span> <span class="n">delayed_func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="n">iterable</span>
<span class="g g-Whitespace">     </span><span class="mi">64</span> <span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">65</span> <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">iterable_with_config</span><span class="p">)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\joblib\parallel.py:1863,</span> in <span class="ni">Parallel.__call__</span><span class="nt">(self, iterable)</span>
<span class="g g-Whitespace">   </span><span class="mi">1861</span>     <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_sequential_output</span><span class="p">(</span><span class="n">iterable</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1862</span>     <span class="nb">next</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1863</span>     <span class="k">return</span> <span class="n">output</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_generator</span> <span class="k">else</span> <span class="nb">list</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1865</span> <span class="c1"># Let&#39;s create an ID that uniquely identifies the current call. If the</span>
<span class="g g-Whitespace">   </span><span class="mi">1866</span> <span class="c1"># call is interrupted early and that the same instance is immediately</span>
<span class="g g-Whitespace">   </span><span class="mi">1867</span> <span class="c1"># re-used, this id will be used to prevent workers that were</span>
<span class="g g-Whitespace">   </span><span class="mi">1868</span> <span class="c1"># concurrently finalizing a task from the previous call to run the</span>
<span class="g g-Whitespace">   </span><span class="mi">1869</span> <span class="c1"># callback.</span>
<span class="g g-Whitespace">   </span><span class="mi">1870</span> <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\joblib\parallel.py:1792,</span> in <span class="ni">Parallel._get_sequential_output</span><span class="nt">(self, iterable)</span>
<span class="g g-Whitespace">   </span><span class="mi">1790</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dispatched_batches</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="g g-Whitespace">   </span><span class="mi">1791</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dispatched_tasks</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="ne">-&gt; </span><span class="mi">1792</span> <span class="n">res</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1793</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_completed_tasks</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="g g-Whitespace">   </span><span class="mi">1794</span> <span class="bp">self</span><span class="o">.</span><span class="n">print_progress</span><span class="p">()</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\parallel.py:127,</span> in <span class="ni">_FuncWrapper.__call__</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">125</span>     <span class="n">config</span> <span class="o">=</span> <span class="p">{}</span>
<span class="g g-Whitespace">    </span><span class="mi">126</span> <span class="k">with</span> <span class="n">config_context</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">127</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\ensemble\_forest.py:188,</span> in <span class="ni">_parallel_build_trees</span><span class="nt">(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)</span>
<span class="g g-Whitespace">    </span><span class="mi">185</span>     <span class="k">elif</span> <span class="n">class_weight</span> <span class="o">==</span> <span class="s2">&quot;balanced_subsample&quot;</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">186</span>         <span class="n">curr_sample_weight</span> <span class="o">*=</span> <span class="n">compute_sample_weight</span><span class="p">(</span><span class="s2">&quot;balanced&quot;</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">188</span>     <span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">curr_sample_weight</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">189</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">190</span>     <span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\base.py:1152,</span> in <span class="ni">_fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper</span><span class="nt">(estimator, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1145</span>     <span class="n">estimator</span><span class="o">.</span><span class="n">_validate_params</span><span class="p">()</span>
<span class="g g-Whitespace">   </span><span class="mi">1147</span> <span class="k">with</span> <span class="n">config_context</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1148</span>     <span class="n">skip_parameter_validation</span><span class="o">=</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1149</span>         <span class="n">prefer_skip_nested_validation</span> <span class="ow">or</span> <span class="n">global_skip_validation</span>
<span class="g g-Whitespace">   </span><span class="mi">1150</span>     <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1151</span> <span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1152</span>     <span class="k">return</span> <span class="n">fit_method</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\tree\_classes.py:959,</span> in <span class="ni">DecisionTreeClassifier.fit</span><span class="nt">(self, X, y, sample_weight, check_input)</span>
<span class="g g-Whitespace">    </span><span class="mi">928</span> <span class="nd">@_fit_context</span><span class="p">(</span><span class="n">prefer_skip_nested_validation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">929</span> <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">930</span><span class="w">     </span><span class="sd">&quot;&quot;&quot;Build a decision tree classifier from the training set (X, y).</span>
<span class="g g-Whitespace">    </span><span class="mi">931</span><span class="sd"> </span>
<span class="g g-Whitespace">    </span><span class="mi">932</span><span class="sd">     Parameters</span>
<span class="sd">   (...)</span>
<span class="g g-Whitespace">    </span><span class="mi">956</span><span class="sd">         Fitted estimator.</span>
<span class="g g-Whitespace">    </span><span class="mi">957</span><span class="sd">     &quot;&quot;&quot;</span>
<span class="ne">--&gt; </span><span class="mi">959</span>     <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">960</span>         <span class="n">X</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">961</span>         <span class="n">y</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">962</span>         <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">963</span>         <span class="n">check_input</span><span class="o">=</span><span class="n">check_input</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">964</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">965</span>     <span class="k">return</span> <span class="bp">self</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\tree\_classes.py:443,</span> in <span class="ni">BaseDecisionTree._fit</span><span class="nt">(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)</span>
<span class="g g-Whitespace">    </span><span class="mi">432</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">433</span>     <span class="n">builder</span> <span class="o">=</span> <span class="n">BestFirstTreeBuilder</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">434</span>         <span class="n">splitter</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">435</span>         <span class="n">min_samples_split</span><span class="p">,</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">440</span>         <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">441</span>     <span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">443</span> <span class="n">builder</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree_</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">missing_values_in_feature_mask</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">445</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">446</span>     <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optuna_search</span><span class="o">.</span><span class="n">results</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>n_estimators</th>
      <th>max_features</th>
      <th>max_depth</th>
      <th>min_samples_split</th>
      <th>min_samples_leaf</th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>5</th>
      <td>75.0</td>
      <td>1.0</td>
      <td>30.0</td>
      <td>22.0</td>
      <td>3.0</td>
      <td>0.571658</td>
    </tr>
    <tr>
      <th>4</th>
      <td>25.0</td>
      <td>1.0</td>
      <td>10.0</td>
      <td>15.0</td>
      <td>10.0</td>
      <td>0.558439</td>
    </tr>
    <tr>
      <th>7</th>
      <td>75.0</td>
      <td>0.6</td>
      <td>10.0</td>
      <td>10.0</td>
      <td>3.0</td>
      <td>0.554656</td>
    </tr>
    <tr>
      <th>2</th>
      <td>25.0</td>
      <td>0.6</td>
      <td>10.0</td>
      <td>12.0</td>
      <td>12.0</td>
      <td>0.554012</td>
    </tr>
    <tr>
      <th>9</th>
      <td>75.0</td>
      <td>0.5</td>
      <td>10.0</td>
      <td>22.0</td>
      <td>11.0</td>
      <td>0.552342</td>
    </tr>
    <tr>
      <th>8</th>
      <td>75.0</td>
      <td>1.0</td>
      <td>5.0</td>
      <td>11.0</td>
      <td>7.0</td>
      <td>0.546045</td>
    </tr>
    <tr>
      <th>0</th>
      <td>25.0</td>
      <td>0.8</td>
      <td>7.0</td>
      <td>11.0</td>
      <td>10.0</td>
      <td>0.544086</td>
    </tr>
    <tr>
      <th>6</th>
      <td>25.0</td>
      <td>0.8</td>
      <td>5.0</td>
      <td>16.0</td>
      <td>15.0</td>
      <td>0.537246</td>
    </tr>
    <tr>
      <th>3</th>
      <td>25.0</td>
      <td>0.7</td>
      <td>5.0</td>
      <td>9.0</td>
      <td>11.0</td>
      <td>0.536326</td>
    </tr>
    <tr>
      <th>1</th>
      <td>25.0</td>
      <td>0.7</td>
      <td>2.0</td>
      <td>17.0</td>
      <td>22.0</td>
      <td>0.500000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optuna_search</span><span class="o">.</span><span class="n">best_params_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;n_estimators&#39;: 75,
 &#39;max_features&#39;: 1.0,
 &#39;max_depth&#39;: 30,
 &#39;min_samples_split&#39;: 22,
 &#39;min_samples_leaf&#39;: 3}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optuna_search</span><span class="o">.</span><span class="n">best_score_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5716577312956537
</pre></div>
</div>
</div>
</div>
</section>
<section id="applying-hpo-to-all-the-models-together">
<h5><strong>Applying HPO to all the models together</strong><a class="headerlink" href="#applying-hpo-to-all-the-models-together" title="Link to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_scores</span><span class="p">,</span> <span class="n">best_params</span> <span class="o">=</span> <span class="p">{},</span> <span class="p">{}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">param_grid_RF</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;n_estimators&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">75</span><span class="p">]),</span>
        <span class="s1">&#39;max_features&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s1">&#39;max_features&#39;</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
        <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;max_depth&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">]),</span>
        <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;min_samples_split&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span>
        <span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">param_grid_trees</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;max_depth&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">]),</span>
        <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;min_samples_split&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span>
        <span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">param_grid_knn</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;n_neighbors&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;n_neighbors&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
        <span class="s1">&#39;metric&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;minkowski&#39;</span><span class="p">,</span> <span class="s1">&#39;euclidean&#39;</span><span class="p">]),</span>
    <span class="p">}</span>

    <span class="k">if</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;metric&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;minkowski&#39;</span><span class="p">:</span>

        <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;p&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])})</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">knn</span>
<span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;knn&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_knn</span>

<span class="n">optuna_search</span> <span class="o">=</span> <span class="n">OptunaSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                               <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                               <span class="n">n_iter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">111</span><span class="p">)</span>   

<span class="n">optuna_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

<span class="n">best_scores</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">optuna_search</span><span class="o">.</span><span class="n">best_score_</span>
<span class="n">best_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">optuna_search</span><span class="o">.</span><span class="n">best_params_</span>
<span class="c1"># Time: 2 mins</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-13 01:00:43,931] A new study created in memory with name: no-name-f8f25b59-79fe-4bf8-953e-22e870030d1d
[I 2024-03-13 01:01:15,787] Trial 0 finished with value: 0.5716124226792975 and parameters: {&#39;n_neighbors&#39;: 3, &#39;metric&#39;: &#39;euclidean&#39;}. Best is trial 0 with value: 0.5716124226792975.
[I 2024-03-13 01:01:46,091] Trial 1 finished with value: 0.5361065939877293 and parameters: {&#39;n_neighbors&#39;: 4, &#39;metric&#39;: &#39;minkowski&#39;, &#39;p&#39;: 2}. Best is trial 0 with value: 0.5716124226792975.
[I 2024-03-13 01:02:17,608] Trial 2 finished with value: 0.5342243145920306 and parameters: {&#39;n_neighbors&#39;: 2, &#39;metric&#39;: &#39;euclidean&#39;}. Best is trial 0 with value: 0.5716124226792975.
[I 2024-03-13 01:02:49,092] Trial 3 finished with value: 0.5342243145920306 and parameters: {&#39;n_neighbors&#39;: 2, &#39;metric&#39;: &#39;euclidean&#39;}. Best is trial 0 with value: 0.5716124226792975.
[I 2024-03-13 01:03:20,440] Trial 4 finished with value: 0.5716124226792975 and parameters: {&#39;n_neighbors&#39;: 3, &#39;metric&#39;: &#39;euclidean&#39;}. Best is trial 0 with value: 0.5716124226792975.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">tree</span>
<span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;trees&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_trees</span>

<span class="n">optuna_search</span> <span class="o">=</span> <span class="n">OptunaSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                               <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                               <span class="n">n_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>   

<span class="n">optuna_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

<span class="n">best_scores</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">optuna_search</span><span class="o">.</span><span class="n">best_score_</span>
<span class="n">best_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">optuna_search</span><span class="o">.</span><span class="n">best_params_</span>
<span class="c1"># Time: 17 secs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-13 01:12:02,191] A new study created in memory with name: no-name-26fcad2c-f2d7-408a-b023-8624016b6bf1
[I 2024-03-13 01:12:03,354] Trial 0 finished with value: 0.5793985727406281 and parameters: {&#39;max_depth&#39;: 30, &#39;min_samples_split&#39;: 12, &#39;min_samples_leaf&#39;: 25}. Best is trial 0 with value: 0.5793985727406281.
[I 2024-03-13 01:12:04,909] Trial 1 finished with value: 0.5955955998132402 and parameters: {&#39;max_depth&#39;: 30, &#39;min_samples_split&#39;: 12, &#39;min_samples_leaf&#39;: 3}. Best is trial 1 with value: 0.5955955998132402.
[I 2024-03-13 01:12:05,490] Trial 2 finished with value: 0.5462235197836873 and parameters: {&#39;max_depth&#39;: 5, &#39;min_samples_split&#39;: 14, &#39;min_samples_leaf&#39;: 17}. Best is trial 1 with value: 0.5955955998132402.
[I 2024-03-13 01:12:05,822] Trial 3 finished with value: 0.5 and parameters: {&#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 10, &#39;min_samples_leaf&#39;: 7}. Best is trial 1 with value: 0.5955955998132402.
[I 2024-03-13 01:12:06,408] Trial 4 finished with value: 0.5462235197836873 and parameters: {&#39;max_depth&#39;: 5, &#39;min_samples_split&#39;: 13, &#39;min_samples_leaf&#39;: 12}. Best is trial 1 with value: 0.5955955998132402.
[I 2024-03-13 01:12:07,422] Trial 5 finished with value: 0.5637478614778239 and parameters: {&#39;max_depth&#39;: 10, &#39;min_samples_split&#39;: 16, &#39;min_samples_leaf&#39;: 4}. Best is trial 1 with value: 0.5955955998132402.
[I 2024-03-13 01:12:08,196] Trial 6 finished with value: 0.5571063894680863 and parameters: {&#39;max_depth&#39;: 7, &#39;min_samples_split&#39;: 25, &#39;min_samples_leaf&#39;: 14}. Best is trial 1 with value: 0.5955955998132402.
[I 2024-03-13 01:12:08,905] Trial 7 finished with value: 0.5570961410952525 and parameters: {&#39;max_depth&#39;: 7, &#39;min_samples_split&#39;: 10, &#39;min_samples_leaf&#39;: 9}. Best is trial 1 with value: 0.5955955998132402.
[I 2024-03-13 01:12:09,654] Trial 8 finished with value: 0.5569483317532736 and parameters: {&#39;max_depth&#39;: 7, &#39;min_samples_split&#39;: 16, &#39;min_samples_leaf&#39;: 16}. Best is trial 1 with value: 0.5955955998132402.
[I 2024-03-13 01:12:10,359] Trial 9 finished with value: 0.5462235197836873 and parameters: {&#39;max_depth&#39;: 5, &#39;min_samples_split&#39;: 6, &#39;min_samples_leaf&#39;: 15}. Best is trial 1 with value: 0.5955955998132402.
[I 2024-03-13 01:12:12,362] Trial 10 finished with value: 0.5956727778249408 and parameters: {&#39;max_depth&#39;: 30, &#39;min_samples_split&#39;: 2, &#39;min_samples_leaf&#39;: 3}. Best is trial 10 with value: 0.5956727778249408.
[I 2024-03-13 01:12:14,403] Trial 11 finished with value: 0.5883230610996064 and parameters: {&#39;max_depth&#39;: 30, &#39;min_samples_split&#39;: 2, &#39;min_samples_leaf&#39;: 2}. Best is trial 10 with value: 0.5956727778249408.
[I 2024-03-13 01:12:16,089] Trial 12 finished with value: 0.5914377493474388 and parameters: {&#39;max_depth&#39;: 30, &#39;min_samples_split&#39;: 20, &#39;min_samples_leaf&#39;: 6}. Best is trial 10 with value: 0.5956727778249408.
[I 2024-03-13 01:12:18,059] Trial 13 finished with value: 0.5883230610996064 and parameters: {&#39;max_depth&#39;: 30, &#39;min_samples_split&#39;: 4, &#39;min_samples_leaf&#39;: 2}. Best is trial 10 with value: 0.5956727778249408.
[I 2024-03-13 01:12:19,670] Trial 14 finished with value: 0.5885903821725328 and parameters: {&#39;max_depth&#39;: 30, &#39;min_samples_split&#39;: 6, &#39;min_samples_leaf&#39;: 11}. Best is trial 10 with value: 0.5956727778249408.
[I 2024-03-13 01:12:20,755] Trial 15 finished with value: 0.5611354702533786 and parameters: {&#39;max_depth&#39;: 10, &#39;min_samples_split&#39;: 21, &#39;min_samples_leaf&#39;: 21}. Best is trial 10 with value: 0.5956727778249408.
[I 2024-03-13 01:12:21,221] Trial 16 finished with value: 0.5 and parameters: {&#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 8, &#39;min_samples_leaf&#39;: 6}. Best is trial 10 with value: 0.5956727778249408.
[I 2024-03-13 01:12:22,874] Trial 17 finished with value: 0.5915406490794547 and parameters: {&#39;max_depth&#39;: 30, &#39;min_samples_split&#39;: 3, &#39;min_samples_leaf&#39;: 9}. Best is trial 10 with value: 0.5956727778249408.
[I 2024-03-13 01:12:24,556] Trial 18 finished with value: 0.5926689171906961 and parameters: {&#39;max_depth&#39;: 30, &#39;min_samples_split&#39;: 19, &#39;min_samples_leaf&#39;: 4}. Best is trial 10 with value: 0.5956727778249408.
[I 2024-03-13 01:12:26,039] Trial 19 finished with value: 0.5813079874792636 and parameters: {&#39;max_depth&#39;: 30, &#39;min_samples_split&#39;: 25, &#39;min_samples_leaf&#39;: 19}. Best is trial 10 with value: 0.5956727778249408.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">RF</span>
<span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;RF&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_RF</span>

<span class="n">optuna_search</span> <span class="o">=</span> <span class="n">OptunaSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                               <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span>
                               <span class="n">n_iter</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>   

<span class="n">optuna_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

<span class="n">best_scores</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">optuna_search</span><span class="o">.</span><span class="n">best_score_</span>
<span class="n">best_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">optuna_search</span><span class="o">.</span><span class="n">best_params_</span>
<span class="c1"># Time: 1.35 mins</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-13 01:12:26,056] A new study created in memory with name: no-name-209540ce-6138-4f10-8ab5-222053295e21
[I 2024-03-13 01:12:37,588] Trial 0 finished with value: 0.5440858669032845 and parameters: {&#39;n_estimators&#39;: 25, &#39;max_features&#39;: 0.8, &#39;max_depth&#39;: 7, &#39;min_samples_split&#39;: 11, &#39;min_samples_leaf&#39;: 10}. Best is trial 0 with value: 0.5440858669032845.
[I 2024-03-13 01:12:42,189] Trial 1 finished with value: 0.5 and parameters: {&#39;n_estimators&#39;: 25, &#39;max_features&#39;: 0.7, &#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 17, &#39;min_samples_leaf&#39;: 22}. Best is trial 0 with value: 0.5440858669032845.
[I 2024-03-13 01:12:55,492] Trial 2 finished with value: 0.5540121521305189 and parameters: {&#39;n_estimators&#39;: 25, &#39;max_features&#39;: 0.6, &#39;max_depth&#39;: 10, &#39;min_samples_split&#39;: 12, &#39;min_samples_leaf&#39;: 12}. Best is trial 2 with value: 0.5540121521305189.
[I 2024-03-13 01:13:04,289] Trial 3 finished with value: 0.5363257606753301 and parameters: {&#39;n_estimators&#39;: 25, &#39;max_features&#39;: 0.7, &#39;max_depth&#39;: 5, &#39;min_samples_split&#39;: 9, &#39;min_samples_leaf&#39;: 11}. Best is trial 2 with value: 0.5540121521305189.
[I 2024-03-13 01:13:22,973] Trial 4 finished with value: 0.5584389987620727 and parameters: {&#39;n_estimators&#39;: 25, &#39;max_features&#39;: 1.0, &#39;max_depth&#39;: 10, &#39;min_samples_split&#39;: 15, &#39;min_samples_leaf&#39;: 10}. Best is trial 4 with value: 0.5584389987620727.
[I 2024-03-13 01:15:00,910] Trial 5 finished with value: 0.5716577312956537 and parameters: {&#39;n_estimators&#39;: 75, &#39;max_features&#39;: 1.0, &#39;max_depth&#39;: 30, &#39;min_samples_split&#39;: 22, &#39;min_samples_leaf&#39;: 3}. Best is trial 5 with value: 0.5716577312956537.
[I 2024-03-13 01:15:07,722] Trial 6 finished with value: 0.5372463260963158 and parameters: {&#39;n_estimators&#39;: 25, &#39;max_features&#39;: 0.8, &#39;max_depth&#39;: 5, &#39;min_samples_split&#39;: 16, &#39;min_samples_leaf&#39;: 15}. Best is trial 5 with value: 0.5716577312956537.
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="selecting-the-best-model">
<h4><strong>Selecting the best model</strong><a class="headerlink" href="#selecting-the-best-model" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_scores</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;knn&#39;: 0.5716124226792975,
 &#39;trees&#39;: 0.5956727778249408,
 &#39;RF&#39;: 0.5716577312956537}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_scores_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">best_scores</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The best model, according to the inner evaluation is: </span><span class="si">{</span><span class="n">model_names</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">best_scores_values</span><span class="p">)]</span><span class="si">}</span><span class="s1"> </span><span class="se">\n</span><span class="s1">with an inner score of </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">best_scores_values</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The best model, according to the inner evaluation is: tree 
with an inner score of 0.5956727778249408
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="applying-outer-evaluation">
<h3><strong>Applying outer evaluation</strong><a class="headerlink" href="#applying-outer-evaluation" title="Link to this heading">#</a></h3>
<p>Outer evaluation means to estimate the future performance of a model, in this case we are going to apply that to the best model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tree</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-6" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>DecisionTreeClassifier(max_depth=30, min_samples_leaf=19, min_samples_split=25,
                       random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-10" type="checkbox" checked><label for="sk-estimator-id-10" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeClassifier</label><div class="sk-toggleable__content"><pre>DecisionTreeClassifier(max_depth=30, min_samples_leaf=19, min_samples_split=25,
                       random_state=123)</pre></div></div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_params</span><span class="p">[</span><span class="s1">&#39;trees&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;max_depth&#39;: 30, &#39;min_samples_split&#39;: 2, &#39;min_samples_leaf&#39;: 3}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tree</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">best_params</span><span class="p">[</span><span class="s1">&#39;trees&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-7" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>DecisionTreeClassifier(max_depth=30, min_samples_leaf=3, random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-11" type="checkbox" checked><label for="sk-estimator-id-11" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeClassifier</label><div class="sk-toggleable__content"><pre>DecisionTreeClassifier(max_depth=30, min_samples_leaf=3, random_state=123)</pre></div></div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tree</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;ccp_alpha&#39;: 0.0,
 &#39;class_weight&#39;: None,
 &#39;criterion&#39;: &#39;gini&#39;,
 &#39;max_depth&#39;: 30,
 &#39;max_features&#39;: None,
 &#39;max_leaf_nodes&#39;: None,
 &#39;min_impurity_decrease&#39;: 0.0,
 &#39;min_samples_leaf&#39;: 3,
 &#39;min_samples_split&#39;: 2,
 &#39;min_weight_fraction_leaf&#39;: 0.0,
 &#39;random_state&#39;: 123,
 &#39;splitter&#39;: &#39;best&#39;}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_test_hat</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">future_performance</span> <span class="o">=</span> <span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_test_hat</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="p">)</span>
<span class="n">future_performance</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5959711429110877
</pre></div>
</div>
</div>
</div>
</section>
<section id="ml-with-pipelines">
<h3><strong>ML with Pipelines</strong><a class="headerlink" href="#ml-with-pipelines" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">diabetes_df</span><span class="p">[</span><span class="n">response</span><span class="p">]</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">diabetes_df</span><span class="p">[</span><span class="n">predictors</span><span class="p">]</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span>

<span class="c1"># Since pipelines transformers are a always applied to X (in sklearn) </span>
<span class="c1"># we need to apply the needed transformers to Y individually and as a first step.</span>
<span class="n">enc</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">()</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="c1"># Defining the outer-evaluation: train-test split.</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>

<span class="c1"># Defining the inner-evaluation: k-fold cross </span>
<span class="n">inner</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># To use this X must be a pandas data-frame, in order to select the column by names</span>
<span class="n">quant_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;imputer&#39;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">missing_values</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">(</span><span class="n">with_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_std</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="p">])</span>

<span class="n">cat_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;encoder&#39;</span><span class="p">,</span> <span class="n">OrdinalEncoder</span><span class="p">()),</span> <span class="c1"># encoding the categorical variables is needed for some imputers</span>
    <span class="p">(</span><span class="s1">&#39;imputer&#39;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">missing_values</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;most_frequent&#39;</span><span class="p">))</span>
    <span class="p">])</span>

<span class="n">quant_cat_processing</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span><span class="n">transformers</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;quant&#39;</span><span class="p">,</span> <span class="n">quant_pipeline</span><span class="p">,</span> <span class="n">quant_predictors</span><span class="p">),</span>
                                                       <span class="p">(</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">cat_pipeline</span><span class="p">,</span> <span class="n">cat_predictors</span><span class="p">)])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipelines</span> <span class="o">=</span> <span class="p">{}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;trees&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
             <span class="p">(</span><span class="s1">&#39;preprocessing&#39;</span><span class="p">,</span> <span class="n">quant_cat_processing</span><span class="p">),</span>
             <span class="p">(</span><span class="s1">&#39;trees&#39;</span><span class="p">,</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">))</span> 
            <span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;trees&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-8" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,
                 ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                                  Pipeline(steps=[(&#x27;imputer&#x27;,
                                                                   SimpleImputer()),
                                                                  (&#x27;scaler&#x27;,
                                                                   StandardScaler())]),
                                                  [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;,
                                                   &#x27;PhysHlth&#x27;]),
                                                 (&#x27;cat&#x27;,
                                                  Pipeline(steps=[(&#x27;encoder&#x27;,
                                                                   OrdinalEncoder()),
                                                                  (&#x27;imputer&#x27;,
                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;))]),
                                                  [&#x27;HighBP&#x27;, &#x27;HighChol&#x27;,
                                                   &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;,
                                                   &#x27;Stroke&#x27;,
                                                   &#x27;HeartDiseaseorAttack&#x27;,
                                                   &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;,
                                                   &#x27;Veggies&#x27;,
                                                   &#x27;HvyAlcoholConsump&#x27;,
                                                   &#x27;AnyHealthcare&#x27;,
                                                   &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;,
                                                   &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;,
                                                   &#x27;Education&#x27;, &#x27;Income&#x27;])])),
                (&#x27;trees&#x27;,
                 DecisionTreeClassifier(max_depth=10, min_samples_leaf=3,
                                        random_state=123))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-12" type="checkbox" ><label for="sk-estimator-id-12" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,
                 ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                                  Pipeline(steps=[(&#x27;imputer&#x27;,
                                                                   SimpleImputer()),
                                                                  (&#x27;scaler&#x27;,
                                                                   StandardScaler())]),
                                                  [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;,
                                                   &#x27;PhysHlth&#x27;]),
                                                 (&#x27;cat&#x27;,
                                                  Pipeline(steps=[(&#x27;encoder&#x27;,
                                                                   OrdinalEncoder()),
                                                                  (&#x27;imputer&#x27;,
                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;))]),
                                                  [&#x27;HighBP&#x27;, &#x27;HighChol&#x27;,
                                                   &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;,
                                                   &#x27;Stroke&#x27;,
                                                   &#x27;HeartDiseaseorAttack&#x27;,
                                                   &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;,
                                                   &#x27;Veggies&#x27;,
                                                   &#x27;HvyAlcoholConsump&#x27;,
                                                   &#x27;AnyHealthcare&#x27;,
                                                   &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;,
                                                   &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;,
                                                   &#x27;Education&#x27;, &#x27;Income&#x27;])])),
                (&#x27;trees&#x27;,
                 DecisionTreeClassifier(max_depth=10, min_samples_leaf=3,
                                        random_state=123))])</pre></div></div></div><div class="sk-serial"><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-13" type="checkbox" ><label for="sk-estimator-id-13" class="sk-toggleable__label sk-toggleable__label-arrow">preprocessing: ColumnTransformer</label><div class="sk-toggleable__content"><pre>ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                 Pipeline(steps=[(&#x27;imputer&#x27;, SimpleImputer()),
                                                 (&#x27;scaler&#x27;, StandardScaler())]),
                                 [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;, &#x27;PhysHlth&#x27;]),
                                (&#x27;cat&#x27;,
                                 Pipeline(steps=[(&#x27;encoder&#x27;, OrdinalEncoder()),
                                                 (&#x27;imputer&#x27;,
                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;))]),
                                 [&#x27;HighBP&#x27;, &#x27;HighChol&#x27;, &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;,
                                  &#x27;Stroke&#x27;, &#x27;HeartDiseaseorAttack&#x27;,
                                  &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;, &#x27;Veggies&#x27;,
                                  &#x27;HvyAlcoholConsump&#x27;, &#x27;AnyHealthcare&#x27;,
                                  &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;, &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;,
                                  &#x27;Age&#x27;, &#x27;Education&#x27;, &#x27;Income&#x27;])])</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-14" type="checkbox" ><label for="sk-estimator-id-14" class="sk-toggleable__label sk-toggleable__label-arrow">quant</label><div class="sk-toggleable__content"><pre>[&#x27;BMI&#x27;, &#x27;MentHlth&#x27;, &#x27;PhysHlth&#x27;]</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-15" type="checkbox" ><label for="sk-estimator-id-15" class="sk-toggleable__label sk-toggleable__label-arrow">SimpleImputer</label><div class="sk-toggleable__content"><pre>SimpleImputer()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-16" type="checkbox" ><label for="sk-estimator-id-16" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-17" type="checkbox" ><label for="sk-estimator-id-17" class="sk-toggleable__label sk-toggleable__label-arrow">cat</label><div class="sk-toggleable__content"><pre>[&#x27;HighBP&#x27;, &#x27;HighChol&#x27;, &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;, &#x27;Stroke&#x27;, &#x27;HeartDiseaseorAttack&#x27;, &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;, &#x27;Veggies&#x27;, &#x27;HvyAlcoholConsump&#x27;, &#x27;AnyHealthcare&#x27;, &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;, &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;, &#x27;Education&#x27;, &#x27;Income&#x27;]</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-18" type="checkbox" ><label for="sk-estimator-id-18" class="sk-toggleable__label sk-toggleable__label-arrow">OrdinalEncoder</label><div class="sk-toggleable__content"><pre>OrdinalEncoder()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-19" type="checkbox" ><label for="sk-estimator-id-19" class="sk-toggleable__label sk-toggleable__label-arrow">SimpleImputer</label><div class="sk-toggleable__content"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-20" type="checkbox" ><label for="sk-estimator-id-20" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeClassifier</label><div class="sk-toggleable__content"><pre>DecisionTreeClassifier(max_depth=10, min_samples_leaf=3, random_state=123)</pre></div></div></div></div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;trees&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">steps</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;preprocessing&#39;,
  ColumnTransformer(transformers=[(&#39;quant&#39;,
                                   Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()),
                                                   (&#39;scaler&#39;, StandardScaler())]),
                                   [&#39;BMI&#39;, &#39;MentHlth&#39;, &#39;PhysHlth&#39;]),
                                  (&#39;cat&#39;,
                                   Pipeline(steps=[(&#39;encoder&#39;, OrdinalEncoder()),
                                                   (&#39;imputer&#39;,
                                                    SimpleImputer(strategy=&#39;most_frequent&#39;))]),
                                   [&#39;HighBP&#39;, &#39;HighChol&#39;, &#39;CholCheck&#39;, &#39;Smoker&#39;,
                                    &#39;Stroke&#39;, &#39;HeartDiseaseorAttack&#39;,
                                    &#39;PhysActivity&#39;, &#39;Fruits&#39;, &#39;Veggies&#39;,
                                    &#39;HvyAlcoholConsump&#39;, &#39;AnyHealthcare&#39;,
                                    &#39;NoDocbcCost&#39;, &#39;GenHlth&#39;, &#39;DiffWalk&#39;, &#39;Sex&#39;,
                                    &#39;Age&#39;, &#39;Education&#39;, &#39;Income&#39;])])),
 (&#39;trees&#39;,
  DecisionTreeClassifier(max_depth=10, min_samples_leaf=3, random_state=123))]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;trees&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>
<span class="c1"># Transformers: fit and transform with X_train (equivalent to fit_transform)</span>
<span class="c1"># Estimator: fit with X_train and Y_train</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-9" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,
                 ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                                  Pipeline(steps=[(&#x27;imputer&#x27;,
                                                                   SimpleImputer()),
                                                                  (&#x27;scaler&#x27;,
                                                                   StandardScaler())]),
                                                  [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;,
                                                   &#x27;PhysHlth&#x27;]),
                                                 (&#x27;cat&#x27;,
                                                  Pipeline(steps=[(&#x27;encoder&#x27;,
                                                                   OrdinalEncoder()),
                                                                  (&#x27;imputer&#x27;,
                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;))]),
                                                  [&#x27;HighBP&#x27;, &#x27;HighChol&#x27;,
                                                   &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;,
                                                   &#x27;Stroke&#x27;,
                                                   &#x27;HeartDiseaseorAttack&#x27;,
                                                   &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;,
                                                   &#x27;Veggies&#x27;,
                                                   &#x27;HvyAlcoholConsump&#x27;,
                                                   &#x27;AnyHealthcare&#x27;,
                                                   &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;,
                                                   &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;,
                                                   &#x27;Education&#x27;, &#x27;Income&#x27;])])),
                (&#x27;trees&#x27;,
                 DecisionTreeClassifier(max_depth=10, min_samples_leaf=3,
                                        random_state=123))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-21" type="checkbox" ><label for="sk-estimator-id-21" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,
                 ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                                  Pipeline(steps=[(&#x27;imputer&#x27;,
                                                                   SimpleImputer()),
                                                                  (&#x27;scaler&#x27;,
                                                                   StandardScaler())]),
                                                  [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;,
                                                   &#x27;PhysHlth&#x27;]),
                                                 (&#x27;cat&#x27;,
                                                  Pipeline(steps=[(&#x27;encoder&#x27;,
                                                                   OrdinalEncoder()),
                                                                  (&#x27;imputer&#x27;,
                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;))]),
                                                  [&#x27;HighBP&#x27;, &#x27;HighChol&#x27;,
                                                   &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;,
                                                   &#x27;Stroke&#x27;,
                                                   &#x27;HeartDiseaseorAttack&#x27;,
                                                   &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;,
                                                   &#x27;Veggies&#x27;,
                                                   &#x27;HvyAlcoholConsump&#x27;,
                                                   &#x27;AnyHealthcare&#x27;,
                                                   &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;,
                                                   &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;,
                                                   &#x27;Education&#x27;, &#x27;Income&#x27;])])),
                (&#x27;trees&#x27;,
                 DecisionTreeClassifier(max_depth=10, min_samples_leaf=3,
                                        random_state=123))])</pre></div></div></div><div class="sk-serial"><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-22" type="checkbox" ><label for="sk-estimator-id-22" class="sk-toggleable__label sk-toggleable__label-arrow">preprocessing: ColumnTransformer</label><div class="sk-toggleable__content"><pre>ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                 Pipeline(steps=[(&#x27;imputer&#x27;, SimpleImputer()),
                                                 (&#x27;scaler&#x27;, StandardScaler())]),
                                 [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;, &#x27;PhysHlth&#x27;]),
                                (&#x27;cat&#x27;,
                                 Pipeline(steps=[(&#x27;encoder&#x27;, OrdinalEncoder()),
                                                 (&#x27;imputer&#x27;,
                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;))]),
                                 [&#x27;HighBP&#x27;, &#x27;HighChol&#x27;, &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;,
                                  &#x27;Stroke&#x27;, &#x27;HeartDiseaseorAttack&#x27;,
                                  &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;, &#x27;Veggies&#x27;,
                                  &#x27;HvyAlcoholConsump&#x27;, &#x27;AnyHealthcare&#x27;,
                                  &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;, &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;,
                                  &#x27;Age&#x27;, &#x27;Education&#x27;, &#x27;Income&#x27;])])</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-23" type="checkbox" ><label for="sk-estimator-id-23" class="sk-toggleable__label sk-toggleable__label-arrow">quant</label><div class="sk-toggleable__content"><pre>[&#x27;BMI&#x27;, &#x27;MentHlth&#x27;, &#x27;PhysHlth&#x27;]</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-24" type="checkbox" ><label for="sk-estimator-id-24" class="sk-toggleable__label sk-toggleable__label-arrow">SimpleImputer</label><div class="sk-toggleable__content"><pre>SimpleImputer()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-25" type="checkbox" ><label for="sk-estimator-id-25" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-26" type="checkbox" ><label for="sk-estimator-id-26" class="sk-toggleable__label sk-toggleable__label-arrow">cat</label><div class="sk-toggleable__content"><pre>[&#x27;HighBP&#x27;, &#x27;HighChol&#x27;, &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;, &#x27;Stroke&#x27;, &#x27;HeartDiseaseorAttack&#x27;, &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;, &#x27;Veggies&#x27;, &#x27;HvyAlcoholConsump&#x27;, &#x27;AnyHealthcare&#x27;, &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;, &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;, &#x27;Education&#x27;, &#x27;Income&#x27;]</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-27" type="checkbox" ><label for="sk-estimator-id-27" class="sk-toggleable__label sk-toggleable__label-arrow">OrdinalEncoder</label><div class="sk-toggleable__content"><pre>OrdinalEncoder()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-28" type="checkbox" ><label for="sk-estimator-id-28" class="sk-toggleable__label sk-toggleable__label-arrow">SimpleImputer</label><div class="sk-toggleable__content"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-29" type="checkbox" ><label for="sk-estimator-id-29" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeClassifier</label><div class="sk-toggleable__content"><pre>DecisionTreeClassifier(max_depth=10, min_samples_leaf=3, random_state=123)</pre></div></div></div></div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y_test_hat</span> <span class="o">=</span> <span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;trees&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">Y_test_hat</span>
<span class="c1"># Transformers: transform with X_test (already fitted with X_train) </span>
<span class="c1"># Estimator: predict with X_test</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0., 0., 0., ..., 0., 0., 0.])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_test_hat</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5694157240863605
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;trees&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
             <span class="p">(</span><span class="s1">&#39;preprocessing&#39;</span><span class="p">,</span> <span class="n">quant_cat_processing</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;features_selector&#39;</span><span class="p">,</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">f_classif</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)),</span>
            <span class="p">(</span><span class="s1">&#39;trees&#39;</span><span class="p">,</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">))</span> 
            <span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;trees&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-10 {color: black;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-10" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,
                 ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                                  Pipeline(steps=[(&#x27;imputer&#x27;,
                                                                   SimpleImputer()),
                                                                  (&#x27;scaler&#x27;,
                                                                   StandardScaler())]),
                                                  [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;,
                                                   &#x27;PhysHlth&#x27;]),
                                                 (&#x27;cat&#x27;,
                                                  Pipeline(steps=[(&#x27;encoder&#x27;,
                                                                   OrdinalEncoder()),
                                                                  (&#x27;imputer&#x27;,
                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;))]),
                                                  [&#x27;HighBP&#x27;, &#x27;HighChol&#x27;,
                                                   &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;,
                                                   &#x27;Stroke&#x27;,
                                                   &#x27;HeartDiseaseorAttack&#x27;,
                                                   &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;,
                                                   &#x27;Veggies&#x27;,
                                                   &#x27;HvyAlcoholConsump&#x27;,
                                                   &#x27;AnyHealthcare&#x27;,
                                                   &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;,
                                                   &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;,
                                                   &#x27;Education&#x27;, &#x27;Income&#x27;])])),
                (&#x27;features_selector&#x27;, SelectKBest(k=5)),
                (&#x27;trees&#x27;,
                 DecisionTreeClassifier(max_depth=10, min_samples_leaf=3,
                                        random_state=123))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-30" type="checkbox" ><label for="sk-estimator-id-30" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,
                 ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                                  Pipeline(steps=[(&#x27;imputer&#x27;,
                                                                   SimpleImputer()),
                                                                  (&#x27;scaler&#x27;,
                                                                   StandardScaler())]),
                                                  [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;,
                                                   &#x27;PhysHlth&#x27;]),
                                                 (&#x27;cat&#x27;,
                                                  Pipeline(steps=[(&#x27;encoder&#x27;,
                                                                   OrdinalEncoder()),
                                                                  (&#x27;imputer&#x27;,
                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;))]),
                                                  [&#x27;HighBP&#x27;, &#x27;HighChol&#x27;,
                                                   &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;,
                                                   &#x27;Stroke&#x27;,
                                                   &#x27;HeartDiseaseorAttack&#x27;,
                                                   &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;,
                                                   &#x27;Veggies&#x27;,
                                                   &#x27;HvyAlcoholConsump&#x27;,
                                                   &#x27;AnyHealthcare&#x27;,
                                                   &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;,
                                                   &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;,
                                                   &#x27;Education&#x27;, &#x27;Income&#x27;])])),
                (&#x27;features_selector&#x27;, SelectKBest(k=5)),
                (&#x27;trees&#x27;,
                 DecisionTreeClassifier(max_depth=10, min_samples_leaf=3,
                                        random_state=123))])</pre></div></div></div><div class="sk-serial"><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-31" type="checkbox" ><label for="sk-estimator-id-31" class="sk-toggleable__label sk-toggleable__label-arrow">preprocessing: ColumnTransformer</label><div class="sk-toggleable__content"><pre>ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                 Pipeline(steps=[(&#x27;imputer&#x27;, SimpleImputer()),
                                                 (&#x27;scaler&#x27;, StandardScaler())]),
                                 [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;, &#x27;PhysHlth&#x27;]),
                                (&#x27;cat&#x27;,
                                 Pipeline(steps=[(&#x27;encoder&#x27;, OrdinalEncoder()),
                                                 (&#x27;imputer&#x27;,
                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;))]),
                                 [&#x27;HighBP&#x27;, &#x27;HighChol&#x27;, &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;,
                                  &#x27;Stroke&#x27;, &#x27;HeartDiseaseorAttack&#x27;,
                                  &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;, &#x27;Veggies&#x27;,
                                  &#x27;HvyAlcoholConsump&#x27;, &#x27;AnyHealthcare&#x27;,
                                  &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;, &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;,
                                  &#x27;Age&#x27;, &#x27;Education&#x27;, &#x27;Income&#x27;])])</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-32" type="checkbox" ><label for="sk-estimator-id-32" class="sk-toggleable__label sk-toggleable__label-arrow">quant</label><div class="sk-toggleable__content"><pre>[&#x27;BMI&#x27;, &#x27;MentHlth&#x27;, &#x27;PhysHlth&#x27;]</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-33" type="checkbox" ><label for="sk-estimator-id-33" class="sk-toggleable__label sk-toggleable__label-arrow">SimpleImputer</label><div class="sk-toggleable__content"><pre>SimpleImputer()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-34" type="checkbox" ><label for="sk-estimator-id-34" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-35" type="checkbox" ><label for="sk-estimator-id-35" class="sk-toggleable__label sk-toggleable__label-arrow">cat</label><div class="sk-toggleable__content"><pre>[&#x27;HighBP&#x27;, &#x27;HighChol&#x27;, &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;, &#x27;Stroke&#x27;, &#x27;HeartDiseaseorAttack&#x27;, &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;, &#x27;Veggies&#x27;, &#x27;HvyAlcoholConsump&#x27;, &#x27;AnyHealthcare&#x27;, &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;, &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;, &#x27;Education&#x27;, &#x27;Income&#x27;]</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-36" type="checkbox" ><label for="sk-estimator-id-36" class="sk-toggleable__label sk-toggleable__label-arrow">OrdinalEncoder</label><div class="sk-toggleable__content"><pre>OrdinalEncoder()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-37" type="checkbox" ><label for="sk-estimator-id-37" class="sk-toggleable__label sk-toggleable__label-arrow">SimpleImputer</label><div class="sk-toggleable__content"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-38" type="checkbox" ><label for="sk-estimator-id-38" class="sk-toggleable__label sk-toggleable__label-arrow">SelectKBest</label><div class="sk-toggleable__content"><pre>SelectKBest(k=5)</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-39" type="checkbox" ><label for="sk-estimator-id-39" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeClassifier</label><div class="sk-toggleable__content"><pre>DecisionTreeClassifier(max_depth=10, min_samples_leaf=3, random_state=123)</pre></div></div></div></div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;trees&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_test_hat</span> <span class="o">=</span> <span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;trees&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_test_hat</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5478283308531597
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">selected_features</span> <span class="o">=</span> <span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;trees&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_support</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">selected_features</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 0,  3,  4,  8, 16], dtype=int64)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">diabetes_df</span><span class="p">[</span><span class="n">predictors</span><span class="p">]</span><span class="o">.</span><span class="n">columns</span><span class="p">)[</span><span class="n">selected_features</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;HighBP&#39;, &#39;BMI&#39;, &#39;Smoker&#39;, &#39;Fruits&#39;, &#39;DiffWalk&#39;], dtype=&#39;&lt;U20&#39;)
</pre></div>
</div>
</div>
</div>
<p>We can do both inner and outer evaluation using pipelines, and we will see that in the next section.</p>
<hr class="docutils" />
</section>
</section>
<hr class="docutils" />
<section id="advanced-machine-learning-workflow">
<h2><strong>Advanced Machine Learning Workflow</strong><a class="headerlink" href="#advanced-machine-learning-workflow" title="Link to this heading">#</a></h2>
<section id="id1">
<h3><strong>Defining the response and predictors</strong><a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">quant_columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">diabetes_df</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">diabetes_df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">pl</span><span class="o">.</span><span class="n">Float64</span><span class="p">]</span>
<span class="n">cat_columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">diabetes_df</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">diabetes_df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">pl</span><span class="o">.</span><span class="n">Utf8</span><span class="p">]</span>

<span class="n">response</span> <span class="o">=</span> <span class="s1">&#39;Diabetes_binary&#39;</span>
<span class="n">predictors</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">diabetes_df</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">col</span> <span class="o">!=</span> <span class="n">response</span><span class="p">]</span>
<span class="n">quant_predictors</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">predictors</span> <span class="k">if</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">quant_columns</span><span class="p">]</span>
<span class="n">cat_predictors</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">predictors</span> <span class="k">if</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">cat_columns</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>In order to make the below part simpler (specially in terms of time) we are going to use a quarter of the total dataset. It will be chosen randomly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># In order to make the below part simpler (specially in terms of time)</span>
<span class="n">frac_sample</span> <span class="o">=</span> <span class="mf">0.25</span>
<span class="nb">print</span><span class="p">(</span><span class="n">frac_sample</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">diabetes_df</span><span class="p">))</span>
<span class="n">diabetes_df_sample</span> <span class="o">=</span> <span class="n">diabetes_df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">fraction</span><span class="o">=</span><span class="n">frac_sample</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>63420.0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">diabetes_df_sample</span><span class="p">[</span><span class="n">response</span><span class="p">]</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">diabetes_df_sample</span><span class="p">[</span><span class="n">predictors</span><span class="p">]</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span>
<span class="c1"># The Null values of the Polars columns that are define as Object type by Pandas are treated as None and not as NaN (what we would like)</span>
<span class="c1"># The avoid this behavior the next step is necessary</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Necessary step to apply sklearn later</span>
<span class="n">enc</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">()</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id2">
<h3><strong>Defining outer evaluation: train-test split</strong><a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Defining the outer-evaluation: train-test split.</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(47565, 21)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(15855, 21)
</pre></div>
</div>
</div>
</div>
</section>
<section id="id3">
<h3><strong>Defining inner evaluation: K-Fold Cross Validation</strong><a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Defining the inner-evaluation: k-fold cross </span>
<span class="c1"># We use StratifiedKFold to ensure the same classes distribution in training and testing partitions</span>
<span class="n">inner</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>
<span class="c1"># inner = Fold(n_splits=3, shuffle=True, random_state=123)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="defining-the-pipelines">
<h3><strong>Defining the Pipelines</strong><a class="headerlink" href="#defining-the-pipelines" title="Link to this heading">#</a></h3>
<p>We define the preprocessing pipeline using <code class="docutils literal notranslate"><span class="pre">ColumnTransformer</span></code> that allows to define different preprocessing pipelines for different set of variables, which is very useful since allow us to apply certain preprocessing method to the categorical variables an another to the quantitative ones.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">quant_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;imputer&#39;</span><span class="p">,</span> <span class="n">imputer</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">scaler</span><span class="p">())</span>
    <span class="p">])</span>

<span class="n">cat_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;encoder&#39;</span><span class="p">,</span> <span class="n">encoder</span><span class="p">()),</span> <span class="c1"># encoding the categorical variables is needed by some imputers</span>
    <span class="p">(</span><span class="s1">&#39;imputer&#39;</span><span class="p">,</span> <span class="n">imputer</span><span class="p">())</span>
    <span class="p">])</span>

<span class="n">quant_cat_processing</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span><span class="n">transformers</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;quant&#39;</span><span class="p">,</span> <span class="n">quant_pipeline</span><span class="p">,</span> <span class="n">quant_predictors</span><span class="p">),</span>
                                                       <span class="p">(</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">cat_pipeline</span><span class="p">,</span> <span class="n">cat_predictors</span><span class="p">)])</span>
</pre></div>
</div>
</div>
</div>
<p>Here we define the complete pipelines (preprocessing + models = transformers + estimators), using the below preprocessing pipelines defined for the categorical and quantitative variables.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Defining dictionaries to save important objects:</span>
<span class="n">inner_score</span><span class="p">,</span> <span class="n">best_params</span><span class="p">,</span> <span class="n">inner_results</span><span class="p">,</span> <span class="n">pipelines</span> <span class="o">=</span> <span class="p">{},</span> <span class="p">{},</span> <span class="p">{},</span> <span class="p">{}</span>

<span class="n">model_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;knn&#39;</span><span class="p">,</span> <span class="s1">&#39;trees&#39;</span><span class="p">,</span> <span class="s1">&#39;extra_trees&#39;</span><span class="p">,</span>
              <span class="s1">&#39;RF&#39;</span><span class="p">,</span> <span class="s1">&#39;HGB&#39;</span><span class="p">,</span> <span class="s1">&#39;NN&#39;</span><span class="p">,</span> <span class="s1">&#39;SVM&#39;</span><span class="p">,</span> <span class="s1">&#39;GB&#39;</span><span class="p">,</span> 
              <span class="s1">&#39;XGB&#39;</span><span class="p">,</span> <span class="s1">&#39;logistic_reg&#39;</span><span class="p">,</span> <span class="s1">&#39;bagging_knn&#39;</span><span class="p">,</span> 
              <span class="p">]</span>

<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> 
          <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span> 
          <span class="n">ExtraTreesClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span>
          <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span> 
          <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span> 
          <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span>
          <span class="n">LinearSVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span>  
          <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span> 
          <span class="n">XGBClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span>
          <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;saga&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span>
          <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">KNeighborsClassifier</span><span class="p">(),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span>
          <span class="p">]</span>

<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">model_keys</span><span class="p">,</span> <span class="n">models</span><span class="p">):</span>

    <span class="n">pipelines</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">inner_score</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">best_params</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">inner_results</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span> <span class="p">{},</span> <span class="p">{}</span>

    <span class="n">pipelines</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
            <span class="p">(</span><span class="s1">&#39;preprocessing&#39;</span><span class="p">,</span> <span class="n">quant_cat_processing</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;features_selector&#39;</span><span class="p">,</span> <span class="n">features_selector</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)),</span>
            <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span> 
            <span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>The following graph shows how the general pipeline looks like, in this case using logistic regression as estimator.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;logistic_reg&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,
                 ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                                  Pipeline(steps=[(&#x27;imputer&#x27;,
                                                                   imputer()),
                                                                  (&#x27;scaler&#x27;,
                                                                   scaler())]),
                                                  [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;,
                                                   &#x27;PhysHlth&#x27;]),
                                                 (&#x27;cat&#x27;,
                                                  Pipeline(steps=[(&#x27;encoder&#x27;,
                                                                   encoder()),
                                                                  (&#x27;imputer&#x27;,
                                                                   imputer())]),
                                                  [&#x27;HighBP&#x27;, &#x27;HighChol&#x27;,
                                                   &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;,
                                                   &#x27;Stroke&#x27;,
                                                   &#x27;HeartDiseaseorAttack&#x27;,
                                                   &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;,
                                                   &#x27;Veggies&#x27;,
                                                   &#x27;HvyAlcoholConsump&#x27;,
                                                   &#x27;AnyHealthcare&#x27;,
                                                   &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;,
                                                   &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;,
                                                   &#x27;Education&#x27;, &#x27;Income&#x27;])])),
                (&#x27;features_selector&#x27;, features_selector(cv=2, k=10, n_jobs=-1)),
                (&#x27;logistic_reg&#x27;,
                 LogisticRegression(max_iter=250, random_state=123,
                                    solver=&#x27;saga&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" ><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,
                 ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                                  Pipeline(steps=[(&#x27;imputer&#x27;,
                                                                   imputer()),
                                                                  (&#x27;scaler&#x27;,
                                                                   scaler())]),
                                                  [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;,
                                                   &#x27;PhysHlth&#x27;]),
                                                 (&#x27;cat&#x27;,
                                                  Pipeline(steps=[(&#x27;encoder&#x27;,
                                                                   encoder()),
                                                                  (&#x27;imputer&#x27;,
                                                                   imputer())]),
                                                  [&#x27;HighBP&#x27;, &#x27;HighChol&#x27;,
                                                   &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;,
                                                   &#x27;Stroke&#x27;,
                                                   &#x27;HeartDiseaseorAttack&#x27;,
                                                   &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;,
                                                   &#x27;Veggies&#x27;,
                                                   &#x27;HvyAlcoholConsump&#x27;,
                                                   &#x27;AnyHealthcare&#x27;,
                                                   &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;,
                                                   &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;,
                                                   &#x27;Education&#x27;, &#x27;Income&#x27;])])),
                (&#x27;features_selector&#x27;, features_selector(cv=2, k=10, n_jobs=-1)),
                (&#x27;logistic_reg&#x27;,
                 LogisticRegression(max_iter=250, random_state=123,
                                    solver=&#x27;saga&#x27;))])</pre></div></div></div><div class="sk-serial"><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" ><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">preprocessing: ColumnTransformer</label><div class="sk-toggleable__content"><pre>ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                 Pipeline(steps=[(&#x27;imputer&#x27;, imputer()),
                                                 (&#x27;scaler&#x27;, scaler())]),
                                 [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;, &#x27;PhysHlth&#x27;]),
                                (&#x27;cat&#x27;,
                                 Pipeline(steps=[(&#x27;encoder&#x27;, encoder()),
                                                 (&#x27;imputer&#x27;, imputer())]),
                                 [&#x27;HighBP&#x27;, &#x27;HighChol&#x27;, &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;,
                                  &#x27;Stroke&#x27;, &#x27;HeartDiseaseorAttack&#x27;,
                                  &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;, &#x27;Veggies&#x27;,
                                  &#x27;HvyAlcoholConsump&#x27;, &#x27;AnyHealthcare&#x27;,
                                  &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;, &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;,
                                  &#x27;Age&#x27;, &#x27;Education&#x27;, &#x27;Income&#x27;])])</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" ><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">quant</label><div class="sk-toggleable__content"><pre>[&#x27;BMI&#x27;, &#x27;MentHlth&#x27;, &#x27;PhysHlth&#x27;]</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox" ><label for="sk-estimator-id-4" class="sk-toggleable__label sk-toggleable__label-arrow">imputer</label><div class="sk-toggleable__content"><pre>imputer()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox" ><label for="sk-estimator-id-5" class="sk-toggleable__label sk-toggleable__label-arrow">scaler</label><div class="sk-toggleable__content"><pre>scaler()</pre></div></div></div></div></div></div></div></div><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-6" type="checkbox" ><label for="sk-estimator-id-6" class="sk-toggleable__label sk-toggleable__label-arrow">cat</label><div class="sk-toggleable__content"><pre>[&#x27;HighBP&#x27;, &#x27;HighChol&#x27;, &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;, &#x27;Stroke&#x27;, &#x27;HeartDiseaseorAttack&#x27;, &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;, &#x27;Veggies&#x27;, &#x27;HvyAlcoholConsump&#x27;, &#x27;AnyHealthcare&#x27;, &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;, &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;, &#x27;Education&#x27;, &#x27;Income&#x27;]</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-7" type="checkbox" ><label for="sk-estimator-id-7" class="sk-toggleable__label sk-toggleable__label-arrow">encoder</label><div class="sk-toggleable__content"><pre>encoder()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-8" type="checkbox" ><label for="sk-estimator-id-8" class="sk-toggleable__label sk-toggleable__label-arrow">imputer</label><div class="sk-toggleable__content"><pre>imputer()</pre></div></div></div></div></div></div></div></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-9" type="checkbox" ><label for="sk-estimator-id-9" class="sk-toggleable__label sk-toggleable__label-arrow">features_selector</label><div class="sk-toggleable__content"><pre>features_selector(cv=2, k=10, n_jobs=-1)</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-10" type="checkbox" ><label for="sk-estimator-id-10" class="sk-toggleable__label sk-toggleable__label-arrow">LogisticRegression</label><div class="sk-toggleable__content"><pre>LogisticRegression(max_iter=250, random_state=123, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div></div></div></div></div>
</div>
</section>
<section id="id4">
<h3><strong>Applying inner evaluation</strong><a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<section id="grids-for-hpo">
<h4><strong>Grids for HPO</strong><a class="headerlink" href="#grids-for-hpo" title="Link to this heading">#</a></h4>
<p>In this section we define all the grids to be used along the HPO part. This grids define the values of the different parameters that will be explored in the HPO process.</p>
<ul class="simple">
<li><p>Grid for the preprocessing methods (transformers)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">common_param_grid</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="c1"># Fix Grid</span>
    <span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;preprocessing__quant__scaler__apply&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__quant__scaler__apply&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]),</span>
        <span class="s1">&#39;preprocessing__cat__encoder__method&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__cat__encoder__method&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;ordinal&#39;</span><span class="p">,</span> <span class="s1">&#39;one-hot&#39;</span><span class="p">]),</span>
        <span class="s1">&#39;preprocessing__cat__imputer__apply&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__cat__imputer__apply&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">True</span><span class="p">]),</span>
        <span class="s1">&#39;preprocessing__quant__imputer__apply&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__quant__imputer__apply&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">True</span><span class="p">]),</span>
        <span class="s1">&#39;features_selector__apply&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;features_selector__apply&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">])</span>
    <span class="p">}</span>

    <span class="c1"># Conditioned Grid</span>
    <span class="k">if</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;features_selector__apply&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>

        <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;features_selector__method&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;features_selector__method&#39;</span><span class="p">,</span> 
                                                                                  <span class="p">[</span><span class="s1">&#39;Fpr_f_class&#39;</span><span class="p">,</span> <span class="s1">&#39;Fdr_f_class&#39;</span><span class="p">,</span> 
                                                                                   <span class="s1">&#39;KBest_mutual_class&#39;</span><span class="p">,</span>
                                                                                   <span class="s1">&#39;Percentile_mutual_class&#39;</span><span class="p">,</span>
                                                                                   <span class="c1">#&#39;backward_trees_class&#39;, # Takes too much time with one-hot (too many predictors)</span>
                                                                                   <span class="c1">#&#39;forward_trees_class&#39; # Takes too much time with one-hot (too many predictors)</span>
                                                                                   <span class="p">])})</span>

        <span class="c1"># This is not allowed in Optuna yet (suggest categorical dynamically)</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;        </span>
<span class="sd">        if param_grid[&#39;preprocessing__cat__encoder__method&#39;] == &#39;one-hot&#39;:</span>

<span class="sd">            param_grid.update({&#39;features_selector__method&#39;: trial.suggest_categorical(&#39;features_selector__method&#39;, </span>
<span class="sd">                                                                                  [&#39;Fpr_f_class&#39;, &#39;Fdr_f_class&#39;, </span>
<span class="sd">                                                                                   #&#39;Fpr_mutual_class&#39;, </span>
<span class="sd">                                                                                   #&#39;Fpr_mutual_class&#39;</span>
<span class="sd">                                                                                   #&#39;backward_trees_class&#39;, </span>
<span class="sd">                                                                                   #&#39;forward_trees_class&#39;</span>
<span class="sd">                                                                                   ])})</span>
<span class="sd">        elif param_grid[&#39;preprocessing__cat__encoder__method&#39;] == &#39;ordinal&#39;:</span>

<span class="sd">            param_grid.update({&#39;features_selector__method&#39;: trial.suggest_categorical(&#39;features_selector__method&#39;, </span>
<span class="sd">                                                                                  [&#39;Fpr_f_class&#39;, &#39;Fdr_f_class&#39;, </span>
<span class="sd">                                                                                   #&#39;Fpr_mutual_class&#39;, </span>
<span class="sd">                                                                                   #&#39;Fpr_mutual_class&#39;</span>
<span class="sd">                                                                                   #&#39;backward_trees_class&#39;, </span>
<span class="sd">                                                                                   &#39;forward_trees_class&#39;</span>
<span class="sd">                                                                                   ])})</span>
<span class="sd">        &#39;&#39;&#39;</span>

    <span class="k">if</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;preprocessing__quant__scaler__apply&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
    
        <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__quant__scaler__method&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__quant__scaler__method&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;standard&#39;</span><span class="p">,</span> <span class="s1">&#39;min-max&#39;</span><span class="p">])})</span>

    <span class="k">if</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;preprocessing__quant__imputer__apply&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>

        <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__quant__imputer__method&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__quant__imputer__method&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;simple_median&#39;</span><span class="p">,</span> <span class="s1">&#39;iterative_median&#39;</span><span class="p">,</span> <span class="s1">&#39;knn&#39;</span><span class="p">])})</span>
        <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__cat__imputer__method&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__cat__imputer__method&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;simple_most_frequent&#39;</span><span class="p">,</span> <span class="s1">&#39;iterative_most_frequent&#39;</span><span class="p">,</span> <span class="s1">&#39;knn&#39;</span><span class="p">])})</span>
        
        <span class="k">if</span>  <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;preprocessing__quant__imputer__method&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;knn&#39;</span><span class="p">:</span>
            <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__quant__imputer__n_neighbors&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;preprocessing__quant__imputer__n_neighbors&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)})</span>

        <span class="k">if</span>  <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;preprocessing__cat__imputer__method&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;knn&#39;</span><span class="p">:</span>
            <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__cat__imputer__n_neighbors&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;preprocessing__cat__imputer__n_neighbors&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)})</span>

        <span class="k">if</span> <span class="s1">&#39;iterative&#39;</span> <span class="ow">in</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;preprocessing__quant__imputer__method&#39;</span><span class="p">]:</span>
            <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__quant__imputer__n_nearest_features&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;preprocessing__quant__imputer__n_nearest_features&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">)})</span>

        <span class="k">if</span> <span class="s1">&#39;iterative&#39;</span> <span class="ow">in</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;preprocessing__cat__imputer__method&#39;</span><span class="p">]:</span>
            <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__cat__imputer__n_nearest_features&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;preprocessing__cat__imputer__n_nearest_features&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">)})</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Refined grid for the preprocessing methods</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Based on previous trials we refine the grid</span>
<span class="k">def</span> <span class="nf">common_param_grid_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="c1"># Fix Grid</span>
    <span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;preprocessing__quant__scaler__apply&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__quant__scaler__apply&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">False</span><span class="p">]),</span>
        <span class="s1">&#39;preprocessing__cat__encoder__method&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__cat__encoder__method&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;one-hot&#39;</span><span class="p">]),</span>
        <span class="s1">&#39;preprocessing__cat__imputer__apply&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__cat__imputer__apply&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">True</span><span class="p">]),</span>
        <span class="s1">&#39;preprocessing__quant__imputer__apply&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__quant__imputer__apply&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">True</span><span class="p">]),</span>
        <span class="s1">&#39;features_selector__apply&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;features_selector__apply&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">True</span><span class="p">])</span>
    <span class="p">}</span>

    <span class="c1"># Conditioned Grid</span>
    <span class="k">if</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;features_selector__apply&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>

        <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;features_selector__method&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;features_selector__method&#39;</span><span class="p">,</span> 
                                                                                  <span class="p">[</span><span class="s1">&#39;Fdr_f_class&#39;</span><span class="p">])})</span>

    <span class="k">if</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;preprocessing__quant__scaler__apply&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
    
        <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__quant__scaler__method&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__quant__scaler__method&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;standard&#39;</span><span class="p">,</span> <span class="s1">&#39;min-max&#39;</span><span class="p">])})</span>

    <span class="k">if</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;preprocessing__quant__imputer__apply&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>

        <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__quant__imputer__method&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__quant__imputer__method&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;iterative_median&#39;</span><span class="p">])})</span>
        <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__cat__imputer__method&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__cat__imputer__method&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;simple_most_frequent&#39;</span><span class="p">])})</span>
        
        <span class="k">if</span>  <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;preprocessing__quant__imputer__method&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;knn&#39;</span><span class="p">:</span>
            <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__quant__imputer__n_neighbors&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;preprocessing__quant__imputer__n_neighbors&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)})</span>

        <span class="k">if</span>  <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;preprocessing__cat__imputer__method&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;knn&#39;</span><span class="p">:</span>
            <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__cat__imputer__n_neighbors&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;preprocessing__cat__imputer__n_neighbors&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)})</span>

        <span class="k">if</span> <span class="s1">&#39;iterative&#39;</span> <span class="ow">in</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;preprocessing__quant__imputer__method&#39;</span><span class="p">]:</span>
            <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__quant__imputer__n_nearest_features&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__quant__imputer__n_nearest_features&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">6</span><span class="p">])})</span>

        <span class="k">if</span> <span class="s1">&#39;iterative&#39;</span> <span class="ow">in</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;preprocessing__cat__imputer__method&#39;</span><span class="p">]:</span>
            <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__cat__imputer__n_nearest_features&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;preprocessing__cat__imputer__n_nearest_features&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">)})</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Grid for KNN</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">param_grid_knn</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="n">common_param_grid</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>

    <span class="c1"># NoT conditioned grid for KNN</span>
    <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s1">&#39;knn__n_neighbors&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;knn__n_neighbors&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
        <span class="s1">&#39;knn__metric&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;knn__metric&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;cosine&#39;</span><span class="p">,</span> <span class="s1">&#39;minkowski&#39;</span><span class="p">,</span> <span class="s1">&#39;cityblock&#39;</span><span class="p">])</span>
    <span class="p">})</span>

    <span class="c1"># Conditioned grid for KNN</span>
    <span class="k">if</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;knn__metric&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;minkowski&#39;</span><span class="p">:</span>
        <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;knn__p&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;knn__p&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">param_grid_knn_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="n">common_param_grid_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>

    <span class="c1"># NoT conditioned grid for KNN</span>
    <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s1">&#39;knn__n_neighbors&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;knn__n_neighbors&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">),</span>
        <span class="s1">&#39;knn__metric&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;knn__metric&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;cosine&#39;</span><span class="p">,</span> <span class="s1">&#39;minkowski&#39;</span><span class="p">,</span> <span class="s1">&#39;cityblock&#39;</span><span class="p">])</span>
    <span class="p">})</span>

    <span class="c1"># Conditioned grid for KNN</span>
    <span class="k">if</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;knn__metric&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;minkowski&#39;</span><span class="p">:</span>
        <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;knn__p&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;knn__p&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Grid for Trees</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">param_grid_trees</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="n">common_param_grid</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>

    <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s1">&#39;trees__max_depth&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;trees__max_depth&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">]),</span>
        <span class="s1">&#39;trees__min_samples_split&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;trees__min_samples_split&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span>
        <span class="s1">&#39;trees__min_samples_leaf&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;trees__min_samples_leaf&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span>
        <span class="s1">&#39;trees__splitter&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;trees__splitter&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="s1">&#39;random&#39;</span><span class="p">]),</span>
        <span class="s1">&#39;trees__criterion&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;trees__criterion&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;log_loss&#39;</span><span class="p">,</span> <span class="s1">&#39;gini&#39;</span><span class="p">,</span> <span class="s1">&#39;entropy&#39;</span><span class="p">]),</span>
        <span class="c1">#&#39;trees__ccp_alpha&#39;: trial.suggest_categorical(&#39;trees__ccp_alpha&#39;, [0, 0.1, 0.3, 0.5, 0.8])</span>
    <span class="p">})</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">param_grid_trees_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="n">common_param_grid_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>

    <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s1">&#39;trees__max_depth&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;trees__max_depth&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">]),</span>
        <span class="s1">&#39;trees__min_samples_split&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;trees__min_samples_split&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span>
        <span class="s1">&#39;trees__min_samples_leaf&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;trees__min_samples_leaf&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span>
        <span class="s1">&#39;trees__splitter&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;trees__splitter&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;best&#39;</span><span class="p">]),</span>
        <span class="s1">&#39;trees__criterion&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;trees__criterion&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;entropy&#39;</span><span class="p">]),</span>
        <span class="c1">#&#39;trees__ccp_alpha&#39;: trial.suggest_categorical(&#39;trees__ccp_alpha&#39;, [0, 0.1, 0.3, 0.5, 0.8])</span>
    <span class="p">})</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Grid for Extra-trees</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">param_grid_extra_trees</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="n">common_param_grid</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>

    <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s1">&#39;extra_trees__n_estimators&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;extra_trees__n_estimators&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">75</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">120</span><span class="p">]),</span>
        <span class="s1">&#39;extra_trees__max_depth&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;extra_trees__max_depth&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">]),</span>
        <span class="s1">&#39;extra_trees__min_samples_split&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;extra_trees__min_samples_split&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
        <span class="s1">&#39;extra_trees__min_samples_leaf&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;extra_trees__min_samples_leaf&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
        <span class="s1">&#39;extra_trees__criterion&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;extra_trees__criterion&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;gini&#39;</span><span class="p">]),</span>
        <span class="s1">&#39;extra_trees__max_features&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;extra_trees__max_features&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
    <span class="p">})</span>
    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">param_grid_extra_trees_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="n">common_param_grid_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>

    <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s1">&#39;extra_trees__n_estimators&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;extra_trees__n_estimators&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">75</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">200</span><span class="p">]),</span>
        <span class="s1">&#39;extra_trees__max_depth&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;extra_trees__max_depth&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">]),</span>
        <span class="s1">&#39;extra_trees__min_samples_split&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;extra_trees__min_samples_split&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
        <span class="s1">&#39;extra_trees__min_samples_leaf&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;extra_trees__min_samples_leaf&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
        <span class="s1">&#39;extra_trees__criterion&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;extra_trees__criterion&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;gini&#39;</span><span class="p">]),</span>
        <span class="s1">&#39;extra_trees__max_features&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;extra_trees__max_features&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
    <span class="p">})</span>
    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Grid for Histogram Gradient Boosting</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Grid for Histogram Gradient Boosting </span>
<span class="k">def</span> <span class="nf">param_grid_HGB</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="n">common_param_grid</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>

    <span class="c1"># Specific logic for HGB</span>
    <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s1">&#39;HGB__max_depth&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;HGB__max_depth&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">]),</span>
        <span class="s1">&#39;HGB__l2_regularization&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s1">&#39;HGB__l2_regularization&#39;</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="s1">&#39;HGB__max_iter&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;HGB__max_iter&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">130</span><span class="p">,</span> <span class="mi">150</span><span class="p">])</span>
    <span class="p">})</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Grid for Histogram Gradient Boosting </span>
<span class="k">def</span> <span class="nf">param_grid_HGB_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="n">common_param_grid_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>

    <span class="c1"># Specific logic for HGB</span>
    <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s1">&#39;HGB__max_depth&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;HGB__max_depth&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">]),</span>
        <span class="s1">&#39;HGB__l2_regularization&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s1">&#39;HGB__l2_regularization&#39;</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="s1">&#39;HGB__max_iter&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;HGB__max_iter&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">130</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">175</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">250</span><span class="p">])</span>
    <span class="p">})</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Grid for XGBoost</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">param_grid_XGB</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="n">common_param_grid</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>

    <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s1">&#39;XGB__max_depth&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;XGB__max_depth&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">100</span><span class="p">]),</span>
        <span class="s1">&#39;XGB__reg_lambda&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s1">&#39;XGB__reg_lambda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="s1">&#39;XGB__n_estimators&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;XGB__n_estimators&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">130</span><span class="p">,</span> <span class="mi">150</span><span class="p">]),</span>
        <span class="s1">&#39;XGB__eta&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s1">&#39;XGB__eta&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="s1">&#39;XGB__alpha&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s1">&#39;XGB__alpha&#39;</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="p">})</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Based on previous trials we refine the grid</span>
<span class="k">def</span> <span class="nf">param_grid_XGB_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="n">common_param_grid_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>

    <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s1">&#39;XGB__max_depth&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;XGB__max_depth&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">]),</span>
        <span class="s1">&#39;XGB__reg_lambda&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s1">&#39;XGB__reg_lambda&#39;</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="s1">&#39;XGB__n_estimators&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;XGB__n_estimators&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">130</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">170</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">300</span><span class="p">]),</span>
        <span class="s1">&#39;XGB__eta&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s1">&#39;XGB__eta&#39;</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="s1">&#39;XGB__alpha&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s1">&#39;XGB__alpha&#39;</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">})</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Grid for Random Forest</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">param_grid_RF_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="n">common_param_grid_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>

    <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s1">&#39;RF__n_estimators&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;RF__n_estimators&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">75</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">250</span><span class="p">]),</span>
        <span class="s1">&#39;RF__max_depth&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;RF__max_depth&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">]),</span>
        <span class="s1">&#39;RF__min_samples_split&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;RF__min_samples_split&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
        <span class="s1">&#39;RF__min_samples_leaf&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;RF__min_samples_leaf&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
        <span class="s1">&#39;RF__criterion&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;RF__criterion&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;gini&#39;</span><span class="p">,</span> <span class="s1">&#39;entropy&#39;</span><span class="p">]),</span>
    <span class="p">})</span>
    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Grid for Linear SVM</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Grid for Linear SVM</span>
<span class="k">def</span> <span class="nf">param_grid_linear_SVM_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="n">common_param_grid_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>

    <span class="c1"># Specific logic for Lasso</span>
    <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s1">&#39;SVM__C&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s1">&#39;SVM__C&#39;</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="s1">&#39;SVM__class_weight&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;SVM__class_weight&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;balanced&#39;</span><span class="p">])</span>
    <span class="p">})</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Grid for Multi-Layer Perceptron (NN)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Grid for Multi-Layer Perceptron</span>
<span class="k">def</span> <span class="nf">param_grid_MLP_NN_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="n">common_param_grid_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>

    <span class="c1"># Specific logic for Lasso</span>
    <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s1">&#39;NN__learning_rate_init&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s1">&#39;NN__learning_rate_init&#39;</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="s1">&#39;NN__alpha&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s1">&#39;NN__alpha&#39;</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">})</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Grid for Bagging KNN</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Grid for bagging knn</span>
<span class="k">def</span> <span class="nf">param_grid_bagging_knn_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="n">common_param_grid_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>

    <span class="c1"># Not conditioned grid for bagging KNN</span>
    <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s1">&#39;bagging_knn__estimator__n_neighbors&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;bagging_knn__estimator__n_neighbors&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span>
        <span class="s1">&#39;bagging_knn__estimator__metric&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;bagging_knn__estimator__metric&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;cosine&#39;</span><span class="p">,</span> <span class="s1">&#39;minkowski&#39;</span><span class="p">,</span> <span class="s1">&#39;cityblock&#39;</span><span class="p">]),</span>
        <span class="s1">&#39;bagging_knn__n_estimators&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;bagging_knn__n_estimators&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">100</span><span class="p">]),</span>
        <span class="s1">&#39;bagging_knn__max_features&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;bagging_knn__max_features&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.70</span><span class="p">,</span> <span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.90</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]),</span>
        <span class="s1">&#39;bagging_knn__max_samples&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;bagging_knn__max_samples&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.70</span><span class="p">,</span> <span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.90</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>

    <span class="p">})</span>

    <span class="c1"># Conditioned grid for bagging KNN</span>
    <span class="k">if</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;bagging_knn__estimator__metric&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;minkowski&#39;</span><span class="p">:</span>
        <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;bagging_knn__estimator__p&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;bagging_knn__estimator__p&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Grid for Gradient Boosting</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Grid for GB</span>
<span class="k">def</span> <span class="nf">param_grid_GB_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="n">common_param_grid_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>

    <span class="c1"># Specific logic for GB</span>
    <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s1">&#39;GB__n_estimators&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;GB__n_estimators&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">130</span><span class="p">,</span> <span class="mi">150</span><span class="p">]),</span>
        <span class="s1">&#39;GB__max_depth&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;GB__max_depth&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">]),</span>
        <span class="s1">&#39;GB__min_samples_split&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;GB__min_samples_split&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
        <span class="s1">&#39;GB__min_samples_leaf&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;GB__min_samples_leaf&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
    <span class="p">})</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Grid for preprocessing method refined for Logistic Regression</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Based on previous trials we refine the grid</span>
<span class="k">def</span> <span class="nf">common_param_grid_refined_logistic</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span> <span class="c1"># We force to apply scaler, since seems to work really well with logistic regression</span>

    <span class="c1"># Fix Grid</span>
    <span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;preprocessing__quant__scaler__apply&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__quant__scaler__apply&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]),</span>
        <span class="s1">&#39;preprocessing__cat__encoder__method&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__cat__encoder__method&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;one-hot&#39;</span><span class="p">]),</span>
        <span class="s1">&#39;preprocessing__cat__imputer__apply&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__cat__imputer__apply&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">True</span><span class="p">]),</span>
        <span class="s1">&#39;preprocessing__quant__imputer__apply&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__quant__imputer__apply&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">True</span><span class="p">]),</span>
        <span class="s1">&#39;features_selector__apply&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;features_selector__apply&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">True</span><span class="p">])</span>
    <span class="p">}</span>

    <span class="c1"># Conditioned Grid</span>
    <span class="k">if</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;features_selector__apply&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>

        <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;features_selector__method&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;features_selector__method&#39;</span><span class="p">,</span> 
                                                                                  <span class="p">[</span><span class="s1">&#39;Fdr_f_class&#39;</span><span class="p">])})</span>

    <span class="k">if</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;preprocessing__quant__scaler__apply&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
    
        <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__quant__scaler__method&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__quant__scaler__method&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;standard&#39;</span><span class="p">,</span> <span class="s1">&#39;min-max&#39;</span><span class="p">])})</span>

    <span class="k">if</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;preprocessing__quant__imputer__apply&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>

        <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__quant__imputer__method&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__quant__imputer__method&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;iterative_median&#39;</span><span class="p">])})</span>
        <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__cat__imputer__method&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__cat__imputer__method&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;simple_most_frequent&#39;</span><span class="p">])})</span>
        
        <span class="k">if</span>  <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;preprocessing__quant__imputer__method&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;knn&#39;</span><span class="p">:</span>
            <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__quant__imputer__n_neighbors&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;preprocessing__quant__imputer__n_neighbors&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)})</span>

        <span class="k">if</span>  <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;preprocessing__cat__imputer__method&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;knn&#39;</span><span class="p">:</span>
            <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__cat__imputer__n_neighbors&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;preprocessing__cat__imputer__n_neighbors&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)})</span>

        <span class="k">if</span> <span class="s1">&#39;iterative&#39;</span> <span class="ow">in</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;preprocessing__quant__imputer__method&#39;</span><span class="p">]:</span>
            <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__quant__imputer__n_nearest_features&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;preprocessing__quant__imputer__n_nearest_features&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">6</span><span class="p">])})</span>

        <span class="k">if</span> <span class="s1">&#39;iterative&#39;</span> <span class="ow">in</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;preprocessing__cat__imputer__method&#39;</span><span class="p">]:</span>
            <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__cat__imputer__n_nearest_features&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s1">&#39;preprocessing__cat__imputer__n_nearest_features&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">)})</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Grid for Logistic Regression</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Grid for Logistic Regression</span>
<span class="k">def</span> <span class="nf">param_grid_logistic_regression_refined</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

    <span class="n">param_grid</span> <span class="o">=</span> <span class="n">common_param_grid_refined_logistic</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>

    <span class="c1"># Not conditioned params for logistic regression</span>
    <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s1">&#39;logistic_reg__penalty&#39;</span><span class="p">:</span>  <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;logistic_reg__penalty&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;elasticnet&#39;</span><span class="p">]),</span>
       <span class="c1">#&#39;logistic_reg__penalty&#39;:  trial.suggest_categorical(&#39;logistic_reg__penalty&#39;, [&#39;l1&#39;, &#39;l2&#39;, &#39;elasticnet&#39;, None]),</span>
        <span class="s1">&#39;logistic_reg__C&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s1">&#39;logistic_reg__C&#39;</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="s1">&#39;logistic_reg__class_weight&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s1">&#39;logistic_reg__class_weight&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;balanced&#39;</span><span class="p">])</span>
    <span class="p">})</span>

    <span class="k">if</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;logistic_reg__penalty&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;elasticnet&#39;</span><span class="p">:</span>
        <span class="n">param_grid</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;logistic_reg__l1_ratio&#39;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s1">&#39;logistic_reg__l1_ratio&#39;</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">)})</span>

    <span class="k">return</span> <span class="n">param_grid</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="hyper-parameter-optimization-hpo">
<h4><strong>Hyper-parameter Optimization (HPO)</strong><a class="headerlink" href="#hyper-parameter-optimization-hpo" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>HPO of Trees</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;trees&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_trees_refined</span>

<span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">],</span>  
                               <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                               <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                               <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                               <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                               <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                               <span class="n">n_trials</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> 
                               <span class="n">random_state</span><span class="o">=</span><span class="mi">666</span><span class="p">)</span>

<span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

<span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
<span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
<span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-03 23:48:39,732] A new study created in memory with name: no-name-abd5df36-c444-40cf-af7f-c744371eab4a
[I 2024-03-03 23:48:40,817] Trial 0 finished with value: 0.5359597899943952 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 14, &#39;trees__min_samples_leaf&#39;: 6, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 0 with value: 0.5359597899943952.
[I 2024-03-03 23:48:41,888] Trial 1 finished with value: 0.5361177510668665 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 2, &#39;trees__min_samples_leaf&#39;: 19, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 1 with value: 0.5361177510668665.
[I 2024-03-03 23:48:42,942] Trial 2 finished with value: 0.5391549662156934 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 5, &#39;trees__min_samples_split&#39;: 17, &#39;trees__min_samples_leaf&#39;: 21, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 2 with value: 0.5391549662156934.
[I 2024-03-03 23:48:44,140] Trial 3 finished with value: 0.5525362689818837 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 10, &#39;trees__min_samples_split&#39;: 5, &#39;trees__min_samples_leaf&#39;: 21, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.5525362689818837.
[I 2024-03-03 23:48:45,183] Trial 4 finished with value: 0.5391549662156934 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 5, &#39;trees__min_samples_split&#39;: 10, &#39;trees__min_samples_leaf&#39;: 23, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.5525362689818837.
[I 2024-03-03 23:48:46,470] Trial 5 finished with value: 0.5685590647578107 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 15, &#39;trees__min_samples_split&#39;: 8, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.5685590647578107.
[I 2024-03-03 23:48:47,967] Trial 6 finished with value: 0.5673643852350152 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 19, &#39;trees__min_samples_leaf&#39;: 19, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.5685590647578107.
[I 2024-03-03 23:48:49,072] Trial 7 finished with value: 0.5 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 3, &#39;trees__min_samples_split&#39;: 2, &#39;trees__min_samples_leaf&#39;: 19, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.5685590647578107.
[I 2024-03-03 23:48:50,671] Trial 8 finished with value: 0.5684841420870407 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 15, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 6, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.5685590647578107.
[I 2024-03-03 23:48:52,340] Trial 9 finished with value: 0.5853690357459593 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 13, &#39;trees__min_samples_leaf&#39;: 4, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:48:53,869] Trial 10 finished with value: 0.5800851880284758 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 25, &#39;trees__min_samples_leaf&#39;: 2, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:48:55,587] Trial 11 finished with value: 0.5800851880284758 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 25, &#39;trees__min_samples_leaf&#39;: 2, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:48:56,742] Trial 12 finished with value: 0.5299723686677051 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 4, &#39;trees__min_samples_split&#39;: 24, &#39;trees__min_samples_leaf&#39;: 2, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:48:58,139] Trial 13 finished with value: 0.5779027322495223 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 20, &#39;trees__min_samples_leaf&#39;: 9, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:48:59,509] Trial 14 finished with value: 0.5819767953849753 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 15, &#39;trees__min_samples_leaf&#39;: 6, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:49:00,953] Trial 15 finished with value: 0.574235182190331 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 20, &#39;trees__min_samples_split&#39;: 15, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:49:02,572] Trial 16 finished with value: 0.5828662986438946 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: None, &#39;trees__min_samples_split&#39;: 11, &#39;trees__min_samples_leaf&#39;: 7, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:49:04,145] Trial 17 finished with value: 0.5774443427002497 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: None, &#39;trees__min_samples_split&#39;: 8, &#39;trees__min_samples_leaf&#39;: 9, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:49:05,454] Trial 18 finished with value: 0.5774443427002497 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: None, &#39;trees__min_samples_split&#39;: 11, &#39;trees__min_samples_leaf&#39;: 9, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:49:06,865] Trial 19 finished with value: 0.570624774536434 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: None, &#39;trees__min_samples_split&#39;: 7, &#39;trees__min_samples_leaf&#39;: 16, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:49:08,489] Trial 20 finished with value: 0.5788914152597955 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 20, &#39;trees__min_samples_split&#39;: 17, &#39;trees__min_samples_leaf&#39;: 4, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:49:10,088] Trial 21 finished with value: 0.5824522330380154 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 13, &#39;trees__min_samples_leaf&#39;: 7, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:49:11,206] Trial 22 finished with value: 0.5299074146601889 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 4, &#39;trees__min_samples_split&#39;: 13, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:49:12,139] Trial 23 finished with value: 0.5 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 3, &#39;trees__min_samples_split&#39;: 10, &#39;trees__min_samples_leaf&#39;: 5, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:49:13,322] Trial 24 finished with value: 0.550214972815061 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 10, &#39;trees__min_samples_split&#39;: 16, &#39;trees__min_samples_leaf&#39;: 8, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:49:14,771] Trial 25 finished with value: 0.5706092808682924 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: None, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 15, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:49:16,389] Trial 26 finished with value: 0.5827322905232976 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 19, &#39;trees__min_samples_leaf&#39;: 4, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:49:18,020] Trial 27 finished with value: 0.5787896898385034 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 22, &#39;trees__min_samples_leaf&#39;: 4, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:49:19,386] Trial 28 finished with value: 0.5720976135560845 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: None, &#39;trees__min_samples_split&#39;: 21, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:49:20,537] Trial 29 finished with value: 0.5350545339947299 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 18, &#39;trees__min_samples_leaf&#39;: 4, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:49:22,137] Trial 30 finished with value: 0.5824522330380154 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 14, &#39;trees__min_samples_leaf&#39;: 7, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:49:23,702] Trial 31 finished with value: 0.5824522330380154 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 14, &#39;trees__min_samples_leaf&#39;: 7, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 9 with value: 0.5853690357459593.
[I 2024-03-03 23:49:25,286] Trial 32 finished with value: 0.5854872911258103 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 11, &#39;trees__min_samples_leaf&#39;: 4, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 32 with value: 0.5854872911258103.
[I 2024-03-03 23:49:26,336] Trial 33 finished with value: 0.535030192433949 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 10, &#39;trees__min_samples_leaf&#39;: 4, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 32 with value: 0.5854872911258103.
[I 2024-03-03 23:49:27,602] Trial 34 finished with value: 0.539337657457852 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 5, &#39;trees__min_samples_split&#39;: 6, &#39;trees__min_samples_leaf&#39;: 3, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 32 with value: 0.5854872911258103.
[I 2024-03-03 23:49:29,024] Trial 35 finished with value: 0.546849535204599 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 10, &#39;trees__min_samples_split&#39;: 9, &#39;trees__min_samples_leaf&#39;: 5, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 32 with value: 0.5854872911258103.
[I 2024-03-03 23:49:30,636] Trial 36 finished with value: 0.582882439993547 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 4, &#39;trees__min_samples_leaf&#39;: 6, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 32 with value: 0.5854872911258103.
[I 2024-03-03 23:49:32,004] Trial 37 finished with value: 0.5684841420870407 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 15, &#39;trees__min_samples_split&#39;: 4, &#39;trees__min_samples_leaf&#39;: 6, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 32 with value: 0.5854872911258103.
[I 2024-03-03 23:49:32,987] Trial 38 finished with value: 0.5392848742307262 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 5, &#39;trees__min_samples_split&#39;: 4, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 32 with value: 0.5854872911258103.
[I 2024-03-03 23:49:33,921] Trial 39 finished with value: 0.5 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 3, &#39;trees__min_samples_split&#39;: 6, &#39;trees__min_samples_leaf&#39;: 25, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 32 with value: 0.5854872911258103.
[I 2024-03-03 23:49:35,403] Trial 40 finished with value: 0.5789470032294639 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 3, &#39;trees__min_samples_leaf&#39;: 8, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 32 with value: 0.5854872911258103.
[I 2024-03-03 23:49:37,020] Trial 41 finished with value: 0.5880511412024316 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 3, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 41 with value: 0.5880511412024316.
[I 2024-03-03 23:49:38,656] Trial 42 finished with value: 0.5880511412024316 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 3, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 41 with value: 0.5880511412024316.
[I 2024-03-03 23:49:40,122] Trial 43 finished with value: 0.5868331564092727 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 2, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 41 with value: 0.5880511412024316.
[I 2024-03-03 23:49:41,553] Trial 44 finished with value: 0.586849945440436 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 11, &#39;trees__min_samples_leaf&#39;: 2, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 41 with value: 0.5880511412024316.
[I 2024-03-03 23:49:43,039] Trial 45 finished with value: 0.5868331564092727 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 2, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 41 with value: 0.5880511412024316.
[I 2024-03-03 23:49:44,654] Trial 46 finished with value: 0.5868331564092727 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 2, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 41 with value: 0.5880511412024316.
[I 2024-03-03 23:49:46,320] Trial 47 finished with value: 0.5855643314335202 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 9, &#39;trees__min_samples_leaf&#39;: 2, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 41 with value: 0.5880511412024316.
[I 2024-03-03 23:49:47,270] Trial 48 finished with value: 0.5299723686677051 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 4, &#39;trees__min_samples_split&#39;: 16, &#39;trees__min_samples_leaf&#39;: 3, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 41 with value: 0.5880511412024316.
[I 2024-03-03 23:49:48,770] Trial 49 finished with value: 0.5880511412024316 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 3, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 41 with value: 0.5880511412024316.
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>HPO of KNN</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;knn&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_knn_refined</span>

<span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">],</span>  
                               <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                               <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                               <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                               <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                               <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                               <span class="n">n_trials</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> 
                               <span class="n">random_state</span><span class="o">=</span><span class="mi">111</span><span class="p">)</span>

<span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

<span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
<span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
<span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-03 23:49:48,803] A new study created in memory with name: no-name-927d8fbe-420c-4f0a-b854-3d2edf61768e
[I 2024-03-03 23:49:58,672] Trial 0 finished with value: 0.5323152016527661 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 10, &#39;knn__metric&#39;: &#39;cityblock&#39;}. Best is trial 0 with value: 0.5323152016527661.
[I 2024-03-03 23:50:08,520] Trial 1 finished with value: 0.5619666863719542 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 5, &#39;knn__metric&#39;: &#39;cityblock&#39;}. Best is trial 1 with value: 0.5619666863719542.
[I 2024-03-03 23:50:18,486] Trial 2 finished with value: 0.5347568257802743 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 4, &#39;knn__metric&#39;: &#39;minkowski&#39;, &#39;knn__p&#39;: 1}. Best is trial 1 with value: 0.5619666863719542.
[I 2024-03-03 23:50:56,767] Trial 3 finished with value: 0.5528834825918191 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 11, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 1 with value: 0.5619666863719542.
[I 2024-03-03 23:52:40,421] Trial 4 finished with value: 0.5309784883906524 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 2, &#39;knn__metric&#39;: &#39;minkowski&#39;, &#39;knn__p&#39;: 4}. Best is trial 1 with value: 0.5619666863719542.
[I 2024-03-03 23:53:19,491] Trial 5 finished with value: 0.5487909915093798 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 13, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 1 with value: 0.5619666863719542.
[I 2024-03-03 23:55:19,695] Trial 6 finished with value: 0.5442004051850476 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 7, &#39;knn__metric&#39;: &#39;minkowski&#39;, &#39;knn__p&#39;: 4}. Best is trial 1 with value: 0.5619666863719542.
[I 2024-03-03 23:55:30,403] Trial 7 finished with value: 0.545890977832599 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 11, &#39;knn__metric&#39;: &#39;cityblock&#39;}. Best is trial 1 with value: 0.5619666863719542.
[I 2024-03-03 23:56:05,019] Trial 8 finished with value: 0.57998683055104 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 3, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 8 with value: 0.57998683055104.
[I 2024-03-03 23:59:23,129] Trial 9 finished with value: 0.5353203931748922 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 11, &#39;knn__metric&#39;: &#39;minkowski&#39;, &#39;knn__p&#39;: 4}. Best is trial 8 with value: 0.57998683055104.
[I 2024-03-04 00:00:03,135] Trial 10 finished with value: 0.5838496368737002 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 1, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 10 with value: 0.5838496368737002.
[I 2024-03-04 00:00:43,567] Trial 11 finished with value: 0.5838496368737002 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 1, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 10 with value: 0.5838496368737002.
[I 2024-03-04 00:01:10,936] Trial 12 finished with value: 0.5838496368737002 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 1, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 10 with value: 0.5838496368737002.
[I 2024-03-04 00:01:51,418] Trial 13 finished with value: 0.5427261863221785 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 6, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 10 with value: 0.5838496368737002.
[I 2024-03-04 00:02:22,199] Trial 14 finished with value: 0.5838496368737002 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 1, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 10 with value: 0.5838496368737002.
[I 2024-03-04 00:03:07,335] Trial 15 finished with value: 0.5416940861226273 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 8, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 10 with value: 0.5838496368737002.
[I 2024-03-04 00:03:48,751] Trial 16 finished with value: 0.5447999667756281 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 15, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 10 with value: 0.5838496368737002.
[I 2024-03-04 00:04:23,387] Trial 17 finished with value: 0.57998683055104 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 3, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 10 with value: 0.5838496368737002.
[I 2024-03-04 00:05:07,119] Trial 18 finished with value: 0.5684084421984577 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 5, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 10 with value: 0.5838496368737002.
[I 2024-03-04 00:05:18,493] Trial 19 finished with value: 0.5807399996679364 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 1, &#39;knn__metric&#39;: &#39;cityblock&#39;}. Best is trial 10 with value: 0.5838496368737002.
[I 2024-03-04 00:06:00,501] Trial 20 finished with value: 0.5416940861226273 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 8, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 10 with value: 0.5838496368737002.
[I 2024-03-04 00:06:29,383] Trial 21 finished with value: 0.5838496368737002 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 1, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 10 with value: 0.5838496368737002.
[I 2024-03-04 00:07:02,517] Trial 22 finished with value: 0.57998683055104 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 3, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 10 with value: 0.5838496368737002.
[I 2024-03-04 00:07:34,740] Trial 23 finished with value: 0.5379945574928403 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 2, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 10 with value: 0.5838496368737002.
[I 2024-03-04 00:08:07,018] Trial 24 finished with value: 0.5379945574928403 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 2, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 10 with value: 0.5838496368737002.
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>HPO of Extra-trees</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;extra_trees&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_extra_trees_refined</span>

<span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">],</span>  
                               <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                               <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                               <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                               <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                               <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                               <span class="n">n_trials</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> 
                               <span class="n">random_state</span><span class="o">=</span><span class="mi">666</span><span class="p">)</span>

<span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

<span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
<span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
<span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 00:08:07,046] A new study created in memory with name: no-name-e2f887d1-cf5c-4ac5-8c5b-2dc5989d0bff
[I 2024-03-04 00:08:45,002] Trial 0 finished with value: 0.5396906100891748 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 7, &#39;extra_trees__min_samples_leaf&#39;: 16, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.9}. Best is trial 0 with value: 0.5396906100891748.
[I 2024-03-04 00:09:01,899] Trial 1 finished with value: 0.5171309394168627 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 5, &#39;extra_trees__min_samples_split&#39;: 16, &#39;extra_trees__min_samples_leaf&#39;: 9, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 0 with value: 0.5396906100891748.
[I 2024-03-04 00:10:15,199] Trial 2 finished with value: 0.5541885608361387 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 150, &#39;extra_trees__max_depth&#39;: 20, &#39;extra_trees__min_samples_split&#39;: 16, &#39;extra_trees__min_samples_leaf&#39;: 9, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.9}. Best is trial 2 with value: 0.5541885608361387.
[I 2024-03-04 00:10:25,265] Trial 3 finished with value: 0.5388704200159404 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 30, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 18, &#39;extra_trees__min_samples_leaf&#39;: 20, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.9}. Best is trial 2 with value: 0.5541885608361387.
[I 2024-03-04 00:10:55,739] Trial 4 finished with value: 0.5242758802436661 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 200, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 5, &#39;extra_trees__min_samples_leaf&#39;: 4, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.6}. Best is trial 2 with value: 0.5541885608361387.
[I 2024-03-04 00:11:25,683] Trial 5 finished with value: 0.5521056734174454 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 75, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 19, &#39;extra_trees__min_samples_leaf&#39;: 10, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 2 with value: 0.5541885608361387.
[I 2024-03-04 00:11:39,165] Trial 6 finished with value: 0.5185396861270067 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 75, &#39;extra_trees__max_depth&#39;: 5, &#39;extra_trees__min_samples_split&#39;: 17, &#39;extra_trees__min_samples_leaf&#39;: 9, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.9}. Best is trial 2 with value: 0.5541885608361387.
[I 2024-03-04 00:12:17,565] Trial 7 finished with value: 0.5580458365218975 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 75, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 8, &#39;extra_trees__min_samples_leaf&#39;: 8, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 7 with value: 0.5580458365218975.
[I 2024-03-04 00:12:21,446] Trial 8 finished with value: 0.5111878420335659 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 30, &#39;extra_trees__max_depth&#39;: 4, &#39;extra_trees__min_samples_split&#39;: 15, &#39;extra_trees__min_samples_leaf&#39;: 4, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 7 with value: 0.5580458365218975.
[I 2024-03-04 00:12:45,024] Trial 9 finished with value: 0.558784931237963 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 75, &#39;extra_trees__max_depth&#39;: 20, &#39;extra_trees__min_samples_split&#39;: 14, &#39;extra_trees__min_samples_leaf&#39;: 3, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 9 with value: 0.558784931237963.
[I 2024-03-04 00:13:10,981] Trial 10 finished with value: 0.5449562438761714 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 20, &#39;extra_trees__min_samples_split&#39;: 12, &#39;extra_trees__min_samples_leaf&#39;: 14, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 9 with value: 0.558784931237963.
[I 2024-03-04 00:13:34,765] Trial 11 finished with value: 0.5744085073946418 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 9, &#39;extra_trees__min_samples_leaf&#39;: 2, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:13:56,848] Trial 12 finished with value: 0.5693456767687185 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 20, &#39;extra_trees__min_samples_split&#39;: 11, &#39;extra_trees__min_samples_leaf&#39;: 2, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:14:01,568] Trial 13 finished with value: 0.5 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 3, &#39;extra_trees__min_samples_split&#39;: 2, &#39;extra_trees__min_samples_leaf&#39;: 2, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:14:24,464] Trial 14 finished with value: 0.5655003127907456 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 10, &#39;extra_trees__min_samples_leaf&#39;: 5, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:14:34,147] Trial 15 finished with value: 0.5287985782816372 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 11, &#39;extra_trees__min_samples_leaf&#39;: 6, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:14:37,481] Trial 16 finished with value: 0.5 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 3, &#39;extra_trees__min_samples_split&#39;: 8, &#39;extra_trees__min_samples_leaf&#39;: 2, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.6}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:14:42,669] Trial 17 finished with value: 0.5076439428228113 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 4, &#39;extra_trees__min_samples_split&#39;: 13, &#39;extra_trees__min_samples_leaf&#39;: 12, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:16:03,414] Trial 18 finished with value: 0.5610826630618178 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 200, &#39;extra_trees__max_depth&#39;: 20, &#39;extra_trees__min_samples_split&#39;: 5, &#39;extra_trees__min_samples_leaf&#39;: 6, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:17:05,030] Trial 19 finished with value: 0.5590366765932024 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 150, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 10, &#39;extra_trees__min_samples_leaf&#39;: 7, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:17:42,897] Trial 20 finished with value: 0.5523657485201153 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 6, &#39;extra_trees__min_samples_leaf&#39;: 12, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:18:05,063] Trial 21 finished with value: 0.5655003127907456 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 9, &#39;extra_trees__min_samples_leaf&#39;: 5, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:18:28,180] Trial 22 finished with value: 0.5706206744308698 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 11, &#39;extra_trees__min_samples_leaf&#39;: 2, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:18:51,180] Trial 23 finished with value: 0.5695852513435674 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 12, &#39;extra_trees__min_samples_leaf&#39;: 2, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:19:05,396] Trial 24 finished with value: 0.5569334181829905 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 13, &#39;extra_trees__min_samples_leaf&#39;: 4, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.6}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:19:27,613] Trial 25 finished with value: 0.5673639966261088 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 13, &#39;extra_trees__min_samples_leaf&#39;: 3, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:19:48,439] Trial 26 finished with value: 0.5727449515141115 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 2, &#39;extra_trees__min_samples_leaf&#39;: 2, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:20:03,196] Trial 27 finished with value: 0.5449358728846525 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 2, &#39;extra_trees__min_samples_leaf&#39;: 17, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:20:42,946] Trial 28 finished with value: 0.5547125520744384 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 4, &#39;extra_trees__min_samples_leaf&#39;: 7, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:21:03,462] Trial 29 finished with value: 0.5427925201729136 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 7, &#39;extra_trees__min_samples_leaf&#39;: 5, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:21:06,779] Trial 30 finished with value: 0.5066209495885037 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 30, &#39;extra_trees__max_depth&#39;: 4, &#39;extra_trees__min_samples_split&#39;: 9, &#39;extra_trees__min_samples_leaf&#39;: 3, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 11 with value: 0.5744085073946418.
[I 2024-03-04 00:21:32,679] Trial 31 finished with value: 0.5768102962932274 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 3, &#39;extra_trees__min_samples_leaf&#39;: 2, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 31 with value: 0.5768102962932274.
[I 2024-03-04 00:21:44,214] Trial 32 finished with value: 0.5171958934243791 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 5, &#39;extra_trees__min_samples_split&#39;: 3, &#39;extra_trees__min_samples_leaf&#39;: 3, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 31 with value: 0.5768102962932274.
[I 2024-03-04 00:22:07,579] Trial 33 finished with value: 0.5682819415513732 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 4, &#39;extra_trees__min_samples_leaf&#39;: 4, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 31 with value: 0.5768102962932274.
[I 2024-03-04 00:22:38,495] Trial 34 finished with value: 0.5435965688964955 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 150, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 3, &#39;extra_trees__min_samples_leaf&#39;: 2, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 31 with value: 0.5768102962932274.
[I 2024-03-04 00:23:10,179] Trial 35 finished with value: 0.5285060909433602 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 200, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 6, &#39;extra_trees__min_samples_leaf&#39;: 6, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.9}. Best is trial 31 with value: 0.5768102962932274.
[I 2024-03-04 00:23:21,695] Trial 36 finished with value: 0.5393822404738496 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 20, &#39;extra_trees__min_samples_leaf&#39;: 20, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.6}. Best is trial 31 with value: 0.5768102962932274.
[I 2024-03-04 00:23:26,262] Trial 37 finished with value: 0.5 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 3, &#39;extra_trees__min_samples_split&#39;: 4, &#39;extra_trees__min_samples_leaf&#39;: 4, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.9}. Best is trial 31 with value: 0.5768102962932274.
[I 2024-03-04 00:23:41,928] Trial 38 finished with value: 0.5189578518384502 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 5, &#39;extra_trees__min_samples_split&#39;: 6, &#39;extra_trees__min_samples_leaf&#39;: 18, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 31 with value: 0.5768102962932274.
[I 2024-03-04 00:24:26,547] Trial 39 finished with value: 0.5467101316154163 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 150, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 2, &#39;extra_trees__min_samples_leaf&#39;: 15, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 31 with value: 0.5768102962932274.
[I 2024-03-04 00:24:35,555] Trial 40 finished with value: 0.5522353223598739 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 30, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 16, &#39;extra_trees__min_samples_leaf&#39;: 10, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 31 with value: 0.5768102962932274.
[I 2024-03-04 00:24:58,712] Trial 41 finished with value: 0.5695852513435674 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 12, &#39;extra_trees__min_samples_leaf&#39;: 2, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 31 with value: 0.5768102962932274.
[I 2024-03-04 00:25:20,678] Trial 42 finished with value: 0.5637755143993567 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 14, &#39;extra_trees__min_samples_leaf&#39;: 3, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 31 with value: 0.5768102962932274.
[I 2024-03-04 00:25:43,896] Trial 43 finished with value: 0.5698205962765499 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 10, &#39;extra_trees__min_samples_leaf&#39;: 3, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 31 with value: 0.5768102962932274.
[I 2024-03-04 00:26:07,678] Trial 44 finished with value: 0.5718874718818928 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 8, &#39;extra_trees__min_samples_leaf&#39;: 3, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 31 with value: 0.5768102962932274.
[I 2024-03-04 00:26:26,811] Trial 45 finished with value: 0.5475467221109073 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 75, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 8, &#39;extra_trees__min_samples_leaf&#39;: 5, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 31 with value: 0.5768102962932274.
[I 2024-03-04 00:26:47,652] Trial 46 finished with value: 0.5661782945322547 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 9, &#39;extra_trees__min_samples_leaf&#39;: 4, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.9}. Best is trial 31 with value: 0.5768102962932274.
[I 2024-03-04 00:26:57,211] Trial 47 finished with value: 0.5302640787881692 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 7, &#39;extra_trees__min_samples_leaf&#39;: 2, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 31 with value: 0.5768102962932274.
[I 2024-03-04 00:27:18,211] Trial 48 finished with value: 0.5106925515342156 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 200, &#39;extra_trees__max_depth&#39;: 4, &#39;extra_trees__min_samples_split&#39;: 3, &#39;extra_trees__min_samples_leaf&#39;: 3, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 31 with value: 0.5768102962932274.
[I 2024-03-04 00:27:23,211] Trial 49 finished with value: 0.5154421352214364 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 5, &#39;extra_trees__min_samples_split&#39;: 5, &#39;extra_trees__min_samples_leaf&#39;: 8, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.6}. Best is trial 31 with value: 0.5768102962932274.
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>HPO of HGB</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;HGB&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_HGB_refined</span>

<span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">],</span>  
                               <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                               <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                               <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                               <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                               <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                               <span class="n">n_trials</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> 
                               <span class="n">random_state</span><span class="o">=</span><span class="mi">666</span><span class="p">)</span>

<span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

<span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
<span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
<span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 00:27:23,228] A new study created in memory with name: no-name-34987ece-d99b-4416-856d-5c5e9ee46cdb
[I 2024-03-04 00:27:28,120] Trial 0 finished with value: 0.5544505339272359 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 10, &#39;HGB__l2_regularization&#39;: 0.08658290339442937, &#39;HGB__max_iter&#39;: 175}. Best is trial 0 with value: 0.5544505339272359.
[I 2024-03-04 00:27:33,181] Trial 1 finished with value: 0.5544709049187547 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 40, &#39;HGB__l2_regularization&#39;: 0.030069324187383572, &#39;HGB__max_iter&#39;: 100}. Best is trial 1 with value: 0.5544709049187547.
[I 2024-03-04 00:27:37,764] Trial 2 finished with value: 0.5522746394435877 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 40, &#39;HGB__l2_regularization&#39;: 0.031545036232203526, &#39;HGB__max_iter&#39;: 70}. Best is trial 1 with value: 0.5544709049187547.
[I 2024-03-04 00:27:40,761] Trial 3 finished with value: 0.556415165966078 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.17365483111093305, &#39;HGB__max_iter&#39;: 150}. Best is trial 3 with value: 0.556415165966078.
[I 2024-03-04 00:27:44,511] Trial 4 finished with value: 0.5546128541778758 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 5, &#39;HGB__l2_regularization&#39;: 0.01989598949689477, &#39;HGB__max_iter&#39;: 100}. Best is trial 3 with value: 0.556415165966078.
[I 2024-03-04 00:27:48,794] Trial 5 finished with value: 0.5515397743693886 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 10, &#39;HGB__l2_regularization&#39;: 0.016518368077748997, &#39;HGB__max_iter&#39;: 250}. Best is trial 3 with value: 0.556415165966078.
[I 2024-03-04 00:27:51,644] Trial 6 finished with value: 0.5454907219229334 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 5, &#39;HGB__l2_regularization&#39;: 0.03716355160516042, &#39;HGB__max_iter&#39;: 50}. Best is trial 3 with value: 0.556415165966078.
[I 2024-03-04 00:27:56,368] Trial 7 finished with value: 0.553878248176281 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 50, &#39;HGB__l2_regularization&#39;: 0.2992411820785484, &#39;HGB__max_iter&#39;: 150}. Best is trial 3 with value: 0.556415165966078.
[I 2024-03-04 00:28:00,994] Trial 8 finished with value: 0.5530378166478299 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 20, &#39;HGB__l2_regularization&#39;: 0.03650786159767062, &#39;HGB__max_iter&#39;: 200}. Best is trial 3 with value: 0.556415165966078.
[I 2024-03-04 00:28:05,893] Trial 9 finished with value: 0.5531150709720388 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 30, &#39;HGB__l2_regularization&#39;: 0.3385735123729237, &#39;HGB__max_iter&#39;: 150}. Best is trial 3 with value: 0.556415165966078.
[I 2024-03-04 00:28:08,777] Trial 10 finished with value: 0.5560458129124984 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.6961133282304629, &#39;HGB__max_iter&#39;: 130}. Best is trial 3 with value: 0.556415165966078.
[I 2024-03-04 00:28:11,643] Trial 11 finished with value: 0.555867221775904 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.6489476126574635, &#39;HGB__max_iter&#39;: 130}. Best is trial 3 with value: 0.556415165966078.
[I 2024-03-04 00:28:14,512] Trial 12 finished with value: 0.5546939495350447 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.13581103989508067, &#39;HGB__max_iter&#39;: 130}. Best is trial 3 with value: 0.556415165966078.
[I 2024-03-04 00:28:17,376] Trial 13 finished with value: 0.5541540760196968 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.1896583803412946, &#39;HGB__max_iter&#39;: 130}. Best is trial 3 with value: 0.556415165966078.
[I 2024-03-04 00:28:20,594] Trial 14 finished with value: 0.5540731101988302 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 4, &#39;HGB__l2_regularization&#39;: 0.5039749827105775, &#39;HGB__max_iter&#39;: 150}. Best is trial 3 with value: 0.556415165966078.
[I 2024-03-04 00:28:24,088] Trial 15 finished with value: 0.5483328159766065 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 7, &#39;HGB__l2_regularization&#39;: 0.10425711224011987, &#39;HGB__max_iter&#39;: 50}. Best is trial 3 with value: 0.556415165966078.
[I 2024-03-04 00:28:26,361] Trial 16 finished with value: 0.543968208547409 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.22031612071394505, &#39;HGB__max_iter&#39;: 70}. Best is trial 3 with value: 0.556415165966078.
[I 2024-03-04 00:28:29,677] Trial 17 finished with value: 0.5572557270308313 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.06321234146908146, &#39;HGB__max_iter&#39;: 200}. Best is trial 17 with value: 0.5572557270308313.
[I 2024-03-04 00:28:32,827] Trial 18 finished with value: 0.5537034980726463 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 4, &#39;HGB__l2_regularization&#39;: 0.07188198299348343, &#39;HGB__max_iter&#39;: 200}. Best is trial 17 with value: 0.5572557270308313.
[I 2024-03-04 00:28:37,411] Trial 19 finished with value: 0.5533017327834596 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 20, &#39;HGB__l2_regularization&#39;: 0.05146068557375658, &#39;HGB__max_iter&#39;: 200}. Best is trial 17 with value: 0.5572557270308313.
[I 2024-03-04 00:28:41,978] Trial 20 finished with value: 0.5530013043066586 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 50, &#39;HGB__l2_regularization&#39;: 0.14448687889871192, &#39;HGB__max_iter&#39;: 250}. Best is trial 17 with value: 0.5572557270308313.
[I 2024-03-04 00:28:45,111] Trial 21 finished with value: 0.5570404940167633 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.39280397478658186, &#39;HGB__max_iter&#39;: 175}. Best is trial 17 with value: 0.5572557270308313.
[I 2024-03-04 00:28:48,227] Trial 22 finished with value: 0.5569268568876852 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.39862290378352366, &#39;HGB__max_iter&#39;: 175}. Best is trial 17 with value: 0.5572557270308313.
[I 2024-03-04 00:28:51,277] Trial 23 finished with value: 0.556683182207272 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.41045588393916305, &#39;HGB__max_iter&#39;: 175}. Best is trial 17 with value: 0.5572557270308313.
[I 2024-03-04 00:28:55,834] Trial 24 finished with value: 0.5522747689798898 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 30, &#39;HGB__l2_regularization&#39;: 0.24706485839519302, &#39;HGB__max_iter&#39;: 175}. Best is trial 17 with value: 0.5572557270308313.
[I 2024-03-04 00:28:58,960] Trial 25 finished with value: 0.5576614628892799 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.06500901205245409, &#39;HGB__max_iter&#39;: 175}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:29:03,164] Trial 26 finished with value: 0.5525709678148246 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 7, &#39;HGB__l2_regularization&#39;: 0.0614909061627138, &#39;HGB__max_iter&#39;: 175}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:29:06,011] Trial 27 finished with value: 0.5558223796873021 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.051424350339103544, &#39;HGB__max_iter&#39;: 200}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:29:09,012] Trial 28 finished with value: 0.5559402464582465 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.11226354776888522, &#39;HGB__max_iter&#39;: 175}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:29:13,677] Trial 29 finished with value: 0.5544505339272359 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 10, &#39;HGB__l2_regularization&#39;: 0.08692275917211494, &#39;HGB__max_iter&#39;: 175}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:29:16,660] Trial 30 finished with value: 0.5555546225187125 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.011545647270378778, &#39;HGB__max_iter&#39;: 175}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:29:19,677] Trial 31 finished with value: 0.5548522992164224 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.4608601934434075, &#39;HGB__max_iter&#39;: 175}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:29:24,244] Trial 32 finished with value: 0.5555178511049368 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 40, &#39;HGB__l2_regularization&#39;: 0.07780046881094337, &#39;HGB__max_iter&#39;: 175}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:29:27,160] Trial 33 finished with value: 0.5565125322092016 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.046117111900986975, &#39;HGB__max_iter&#39;: 175}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:29:29,844] Trial 34 finished with value: 0.5518560851232378 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.026261473393742272, &#39;HGB__max_iter&#39;: 100}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:29:34,460] Trial 35 finished with value: 0.5536548149510844 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 40, &#39;HGB__l2_regularization&#39;: 0.3077772628634964, &#39;HGB__max_iter&#39;: 70}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:29:37,927] Trial 36 finished with value: 0.5522459387046383 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 5, &#39;HGB__l2_regularization&#39;: 0.11167063656797029, &#39;HGB__max_iter&#39;: 200}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:29:42,494] Trial 37 finished with value: 0.5547914453144703 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 10, &#39;HGB__l2_regularization&#39;: 0.14976420843729066, &#39;HGB__max_iter&#39;: 250}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:29:47,279] Trial 38 finished with value: 0.55385774764846 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 50, &#39;HGB__l2_regularization&#39;: 0.02720886712400973, &#39;HGB__max_iter&#39;: 100}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:29:52,146] Trial 39 finished with value: 0.5535778196994798 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 20, &#39;HGB__l2_regularization&#39;: 0.06525462587140388, &#39;HGB__max_iter&#39;: 175}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:29:54,227] Trial 40 finished with value: 0.5346963154311235 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.23579807368266806, &#39;HGB__max_iter&#39;: 50}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:29:57,344] Trial 41 finished with value: 0.5557373137608711 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.37185368716059775, &#39;HGB__max_iter&#39;: 175}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:30:00,551] Trial 42 finished with value: 0.5562447750406119 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.47196691576035404, &#39;HGB__max_iter&#39;: 175}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:30:05,144] Trial 43 finished with value: 0.553123141646865 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 30, &#39;HGB__l2_regularization&#39;: 0.3907910476092215, &#39;HGB__max_iter&#39;: 175}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:30:08,394] Trial 44 finished with value: 0.5568578027746045 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.5715841859703752, &#39;HGB__max_iter&#39;: 175}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:30:12,427] Trial 45 finished with value: 0.5555629522661429 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 5, &#39;HGB__l2_regularization&#39;: 0.6345841439243604, &#39;HGB__max_iter&#39;: 200}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:30:15,643] Trial 46 finished with value: 0.5569837402203753 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.5428011993755436, &#39;HGB__max_iter&#39;: 175}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:30:18,862] Trial 47 finished with value: 0.5568578027746045 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 4, &#39;HGB__l2_regularization&#39;: 0.03847123264648896, &#39;HGB__max_iter&#39;: 150}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:30:23,095] Trial 48 finished with value: 0.5523112813210612 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 7, &#39;HGB__l2_regularization&#39;: 0.27083659065400195, &#39;HGB__max_iter&#39;: 70}. Best is trial 25 with value: 0.5576614628892799.
[I 2024-03-04 00:30:25,227] Trial 49 finished with value: 0.5348951480229348 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.5546470621367239, &#39;HGB__max_iter&#39;: 50}. Best is trial 25 with value: 0.5576614628892799.
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>HPO of XGBoost</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;XGB&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_XGB_refined</span>

<span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">],</span>  
                               <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                               <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                               <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                               <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                               <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                               <span class="n">n_trials</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
                               <span class="n">random_state</span><span class="o">=</span><span class="mi">666</span><span class="p">)</span>

<span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

<span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
<span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
<span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 00:30:25,269] A new study created in memory with name: no-name-a21aead1-6d7e-4cab-8bed-7f2cca0da004
[I 2024-03-04 00:30:29,312] Trial 0 finished with value: 0.5447000098064613 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.125662570595452, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.015435011613167845, &#39;XGB__alpha&#39;: 0.1768757232419398}. Best is trial 0 with value: 0.5447000098064613.
[I 2024-03-04 00:30:33,161] Trial 1 finished with value: 0.5784205508014231 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 15, &#39;XGB__reg_lambda&#39;: 0.0841928944943576, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.2075055026979249, &#39;XGB__alpha&#39;: 0.1657330964240997}. Best is trial 1 with value: 0.5784205508014231.
[I 2024-03-04 00:30:38,949] Trial 2 finished with value: 0.5806740882181868 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 25, &#39;XGB__reg_lambda&#39;: 0.010567193620488522, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.20818212251807922, &#39;XGB__alpha&#39;: 0.29127830826603435}. Best is trial 2 with value: 0.5806740882181868.
[I 2024-03-04 00:30:40,960] Trial 3 finished with value: 0.572843049918943 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 4, &#39;XGB__reg_lambda&#39;: 0.05445149785418209, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.43207128785503873, &#39;XGB__alpha&#39;: 0.15741649122240603}. Best is trial 2 with value: 0.5806740882181868.
[I 2024-03-04 00:30:53,410] Trial 4 finished with value: 0.5690285887970563 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 50, &#39;XGB__reg_lambda&#39;: 0.02058401384143928, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.01927721470479139, &#39;XGB__alpha&#39;: 0.13126020961968174}. Best is trial 2 with value: 0.5806740882181868.
[I 2024-03-04 00:30:58,460] Trial 5 finished with value: 0.5774025644267947 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 25, &#39;XGB__reg_lambda&#39;: 0.1308854371197164, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.3664698517986202, &#39;XGB__alpha&#39;: 0.8278386231505405}. Best is trial 2 with value: 0.5806740882181868.
[I 2024-03-04 00:31:01,425] Trial 6 finished with value: 0.5540570983854798 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.04757293093863051, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.027908718111746725, &#39;XGB__alpha&#39;: 0.1723938955702933}. Best is trial 2 with value: 0.5806740882181868.
[I 2024-03-04 00:31:08,293] Trial 7 finished with value: 0.5810479299862369 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.06759417313927013, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.22414485300192474, &#39;XGB__alpha&#39;: 0.11389864265209748}. Best is trial 7 with value: 0.5810479299862369.
[I 2024-03-04 00:31:19,936] Trial 8 finished with value: 0.5732716179586516 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 30, &#39;XGB__reg_lambda&#39;: 0.1677207983103243, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.051845567896747324, &#39;XGB__alpha&#39;: 0.5967955560313202}. Best is trial 7 with value: 0.5810479299862369.
[I 2024-03-04 00:31:22,247] Trial 9 finished with value: 0.5265000692399696 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 6, &#39;XGB__reg_lambda&#39;: 0.19219529085747006, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.017727431569917646, &#39;XGB__alpha&#39;: 0.23841253668659473}. Best is trial 7 with value: 0.5810479299862369.
[I 2024-03-04 00:31:30,578] Trial 10 finished with value: 0.5761220417597553 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.2986657454122731, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.09461290645300438, &#39;XGB__alpha&#39;: 0.10245388213605865}. Best is trial 7 with value: 0.5810479299862369.
[I 2024-03-04 00:31:32,895] Trial 11 finished with value: 0.5631304179825584 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.01105536771988065, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.1533413654144714, &#39;XGB__alpha&#39;: 0.3938677128171642}. Best is trial 7 with value: 0.5810479299862369.
[I 2024-03-04 00:31:38,976] Trial 12 finished with value: 0.5808849620540862 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.029553346950317663, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.1846739005884442, &#39;XGB__alpha&#39;: 0.3541139758027978}. Best is trial 7 with value: 0.5810479299862369.
[I 2024-03-04 00:31:45,994] Trial 13 finished with value: 0.5772102480741838 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.030497963438643277, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.10955146305020864, &#39;XGB__alpha&#39;: 0.43000011615225686}. Best is trial 7 with value: 0.5810479299862369.
[I 2024-03-04 00:31:57,096] Trial 14 finished with value: 0.5747984905123444 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.029880068914399278, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.052741418856215166, &#39;XGB__alpha&#39;: 0.24801001122403646}. Best is trial 7 with value: 0.5810479299862369.
[I 2024-03-04 00:32:02,710] Trial 15 finished with value: 0.5817784358822676 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.022107000578704013, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.26288583436108803, &#39;XGB__alpha&#39;: 0.45168632765621924}. Best is trial 15 with value: 0.5817784358822676.
[I 2024-03-04 00:32:09,026] Trial 16 finished with value: 0.5797740981508052 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.017486517664593384, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.31724428205546107, &#39;XGB__alpha&#39;: 0.5981064572821001}. Best is trial 15 with value: 0.5817784358822676.
[I 2024-03-04 00:32:16,033] Trial 17 finished with value: 0.5786366905696062 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.07306880714427501, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.28242382901879093, &#39;XGB__alpha&#39;: 0.4987616496391654}. Best is trial 15 with value: 0.5817784358822676.
[I 2024-03-04 00:32:20,894] Trial 18 finished with value: 0.5805249750383545 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 30, &#39;XGB__reg_lambda&#39;: 0.04121833004766069, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.47775622283758346, &#39;XGB__alpha&#39;: 0.8565494481345344}. Best is trial 15 with value: 0.5817784358822676.
[I 2024-03-04 00:32:23,793] Trial 19 finished with value: 0.5711176038460432 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.07267265556379564, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.10514378487533332, &#39;XGB__alpha&#39;: 0.28096933894016457}. Best is trial 15 with value: 0.5817784358822676.
[I 2024-03-04 00:32:28,826] Trial 20 finished with value: 0.5776408436386219 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 15, &#39;XGB__reg_lambda&#39;: 0.01863059391752957, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.14233548163838228, &#39;XGB__alpha&#39;: 0.10441417949004841}. Best is trial 15 with value: 0.5817784358822676.
[I 2024-03-04 00:32:34,751] Trial 21 finished with value: 0.5788757469992465 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.030850051013610623, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.21513597180415603, &#39;XGB__alpha&#39;: 0.3927974926614288}. Best is trial 15 with value: 0.5817784358822676.
[I 2024-03-04 00:32:42,360] Trial 22 finished with value: 0.5761053822648942 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.022995126569848718, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.06680297444748586, &#39;XGB__alpha&#39;: 0.3311562570272097}. Best is trial 15 with value: 0.5817784358822676.
[I 2024-03-04 00:32:47,993] Trial 23 finished with value: 0.5810800831492396 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.013606182733960014, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.2506859115939304, &#39;XGB__alpha&#39;: 0.5918890078600059}. Best is trial 15 with value: 0.5817784358822676.
[I 2024-03-04 00:32:50,044] Trial 24 finished with value: 0.5657708747018778 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 4, &#39;XGB__reg_lambda&#39;: 0.015912070946182313, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.2803220263310738, &#39;XGB__alpha&#39;: 0.6663949969206828}. Best is trial 15 with value: 0.5817784358822676.
[I 2024-03-04 00:32:51,993] Trial 25 finished with value: 0.5628262780090996 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.013113302096176976, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.26844012028480974, &#39;XGB__alpha&#39;: 0.5185266957185634}. Best is trial 15 with value: 0.5817784358822676.
[I 2024-03-04 00:33:01,294] Trial 26 finished with value: 0.57725534923539 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 50, &#39;XGB__reg_lambda&#39;: 0.013447307894481944, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.13312499195348845, &#39;XGB__alpha&#39;: 0.7748189266885327}. Best is trial 15 with value: 0.5817784358822676.
[I 2024-03-04 00:33:04,476] Trial 27 finished with value: 0.5558113747336311 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.03694431144523314, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.03441362161921288, &#39;XGB__alpha&#39;: 0.9687428052534152}. Best is trial 15 with value: 0.5817784358822676.
[I 2024-03-04 00:33:06,726] Trial 28 finished with value: 0.5061621714303247 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 6, &#39;XGB__reg_lambda&#39;: 0.023867333030537875, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.010064439088260642, &#39;XGB__alpha&#39;: 0.48951437535073283}. Best is trial 15 with value: 0.5817784358822676.
[I 2024-03-04 00:33:12,679] Trial 29 finished with value: 0.5851443916379383 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.09517051002826848, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.36785165082976573, &#39;XGB__alpha&#39;: 0.2052592437642439}. Best is trial 29 with value: 0.5851443916379383.
[I 2024-03-04 00:33:16,809] Trial 30 finished with value: 0.5866269402482379 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.0981979232738521, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.3719315225218939, &#39;XGB__alpha&#39;: 0.20836296059692608}. Best is trial 30 with value: 0.5866269402482379.
[I 2024-03-04 00:33:20,878] Trial 31 finished with value: 0.5853440014475626 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.09549117370244395, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.3651344239705777, &#39;XGB__alpha&#39;: 0.21292700573789558}. Best is trial 30 with value: 0.5866269402482379.
[I 2024-03-04 00:33:24,976] Trial 32 finished with value: 0.5908281911363783 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.09948186458269269, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.3692771684777653, &#39;XGB__alpha&#39;: 0.19872282775439062}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:33:29,056] Trial 33 finished with value: 0.5849506954421088 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.10575076356969733, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.37799204255006597, &#39;XGB__alpha&#39;: 0.20869256985050677}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:33:33,079] Trial 34 finished with value: 0.5848452585241589 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.10098165096276827, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.3804291809981099, &#39;XGB__alpha&#39;: 0.2072047706487122}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:33:37,054] Trial 35 finished with value: 0.5868025971059877 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.14402137606285328, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.49435474515952216, &#39;XGB__alpha&#39;: 0.146464465912714}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:33:41,026] Trial 36 finished with value: 0.5879750921290341 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.14159913983229377, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.4228354198405921, &#39;XGB__alpha&#39;: 0.14372805992726329}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:33:45,093] Trial 37 finished with value: 0.587184380012562 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.1475934243773565, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.49422665692683543, &#39;XGB__alpha&#39;: 0.144465919499514}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:33:49,026] Trial 38 finished with value: 0.586879980966499 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.1387961839033922, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.48696791615575746, &#39;XGB__alpha&#39;: 0.1454868442471868}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:33:53,056] Trial 39 finished with value: 0.5816309616182584 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.21716674878835798, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.1880973593774305, &#39;XGB__alpha&#39;: 0.13098506461353798}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:33:57,162] Trial 40 finished with value: 0.5847874684373536 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.12856699109481262, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.31895486900951575, &#39;XGB__alpha&#39;: 0.14888462069400396}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:34:01,227] Trial 41 finished with value: 0.5865429106122243 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.16125133516722479, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.4976210511667325, &#39;XGB__alpha&#39;: 0.13841172969247117}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:34:05,397] Trial 42 finished with value: 0.5889012372654269 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.1443014621403946, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.48845115785562276, &#39;XGB__alpha&#39;: 0.17677306702836468}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:34:11,595] Trial 43 finished with value: 0.5833709101249877 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 25, &#39;XGB__reg_lambda&#39;: 0.24874152072970096, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.4216460860340139, &#39;XGB__alpha&#39;: 0.17412205913747233}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:34:14,828] Trial 44 finished with value: 0.5868662557504826 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.1732526303702149, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.42638010408166177, &#39;XGB__alpha&#39;: 0.12644029373559584}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:34:18,994] Trial 45 finished with value: 0.585814173168319 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.116447443714145, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.3060197653470494, &#39;XGB__alpha&#39;: 0.16333437904439935}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:34:21,295] Trial 46 finished with value: 0.5756617091659505 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 4, &#39;XGB__reg_lambda&#39;: 0.1426130650795653, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.4897668716405236, &#39;XGB__alpha&#39;: 0.1136905992718603}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:34:26,876] Trial 47 finished with value: 0.5836906733028903 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 15, &#39;XGB__reg_lambda&#39;: 0.057569671874871754, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.22485155268018175, &#39;XGB__alpha&#39;: 0.12001398721231252}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:34:29,725] Trial 48 finished with value: 0.583170134488644 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.21158565703059887, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.3346143573166571, &#39;XGB__alpha&#39;: 0.18652013085579178}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:34:38,226] Trial 49 finished with value: 0.5797526908688689 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 50, &#39;XGB__reg_lambda&#39;: 0.0859015316004965, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.1802235678237336, &#39;XGB__alpha&#39;: 0.18360307556312205}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:34:41,010] Trial 50 finished with value: 0.5816696760765666 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.26705541387838655, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.4219540469650473, &#39;XGB__alpha&#39;: 0.24456818988112292}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:34:44,376] Trial 51 finished with value: 0.5847154800454283 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.17376403613033103, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.4306038417364219, &#39;XGB__alpha&#39;: 0.1280390820538684}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:34:47,728] Trial 52 finished with value: 0.5841426761492646 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.18742984227159049, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.4255981063839612, &#39;XGB__alpha&#39;: 0.15490478662359378}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:34:51,043] Trial 53 finished with value: 0.5833295204604392 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.1517812826731054, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.3291461139951752, &#39;XGB__alpha&#39;: 0.11712478937453723}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:34:58,754] Trial 54 finished with value: 0.5805486689176246 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 30, &#39;XGB__reg_lambda&#39;: 0.11910257212915296, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.2413297819249033, &#39;XGB__alpha&#39;: 0.14045611756305612}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:35:01,092] Trial 55 finished with value: 0.5805264844178755 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 6, &#39;XGB__reg_lambda&#39;: 0.2117919082476321, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.4198213891574488, &#39;XGB__alpha&#39;: 0.16016236794176753}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:35:14,609] Trial 56 finished with value: 0.5728697231331628 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 25, &#39;XGB__reg_lambda&#39;: 0.13163083370365852, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.0239579673168011, &#39;XGB__alpha&#39;: 0.10693777219230695}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:35:19,412] Trial 57 finished with value: 0.5894937644715985 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.18129076869223326, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.49864595740377665, &#39;XGB__alpha&#39;: 0.27801428554821883}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:35:24,324] Trial 58 finished with value: 0.5855789577716386 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.08199681066371958, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.291236206271479, &#39;XGB__alpha&#39;: 0.27081053804411503}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:35:29,076] Trial 59 finished with value: 0.5876309873903506 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.11261807731133557, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.49815419653208337, &#39;XGB__alpha&#39;: 0.19164085575229567}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:35:36,678] Trial 60 finished with value: 0.5773967803493024 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 15, &#39;XGB__reg_lambda&#39;: 0.05678967350368344, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.08139503348234366, &#39;XGB__alpha&#39;: 0.22777512748087103}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:35:39,061] Trial 61 finished with value: 0.570768233175076 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.10991213486254092, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.49638906261630134, &#39;XGB__alpha&#39;: 0.18705791617788525}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:35:43,797] Trial 62 finished with value: 0.5863635422578168 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.1312093524606057, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.3585873937255156, &#39;XGB__alpha&#39;: 0.17232900045859292}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:35:48,609] Trial 63 finished with value: 0.5864979389873204 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.1859744293979085, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.44111294699522885, &#39;XGB__alpha&#39;: 0.2578067823427624}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:35:51,792] Trial 64 finished with value: 0.584575687847339 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.08139878837498671, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.32874357574635993, &#39;XGB__alpha&#39;: 0.3097458706657847}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:35:55,756] Trial 65 finished with value: 0.5901135675173957 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.1507663357171009, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.38916498608528394, &#39;XGB__alpha&#39;: 0.2215604879970134}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:35:59,585] Trial 66 finished with value: 0.5831381108619433 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.15591338258859774, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.26752897131335795, &#39;XGB__alpha&#39;: 0.22481921945285185}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:36:03,495] Trial 67 finished with value: 0.5864888320220768 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.23698725971332446, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.3890307269885094, &#39;XGB__alpha&#39;: 0.19475552322644504}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:36:07,230] Trial 68 finished with value: 0.5663099709994127 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.06538998582220604, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.04681429405516888, &#39;XGB__alpha&#39;: 0.30042128449412364}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:36:14,395] Trial 69 finished with value: 0.5861468843444251 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 30, &#39;XGB__reg_lambda&#39;: 0.11518730512033391, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.34726243350122105, &#39;XGB__alpha&#39;: 0.3591505089297719}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:36:16,334] Trial 70 finished with value: 0.566894427530758 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 4, &#39;XGB__reg_lambda&#39;: 0.2808256251818527, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.2937522928715982, &#39;XGB__alpha&#39;: 0.22912571957301262}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:36:20,492] Trial 71 finished with value: 0.5861284563974386 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.13772311120410038, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.44729152452027343, &#39;XGB__alpha&#39;: 0.1675980033673271}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:36:27,275] Trial 72 finished with value: 0.5843776324733407 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 50, &#39;XGB__reg_lambda&#39;: 0.16009521359999296, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.4032450689732629, &#39;XGB__alpha&#39;: 0.15310582636015993}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:36:31,957] Trial 73 finished with value: 0.5862306999639394 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.1929733373512798, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.4954831951948535, &#39;XGB__alpha&#39;: 0.19436180307496256}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:36:35,992] Trial 74 finished with value: 0.5869567171454994 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.14570926611549467, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.4648493419319416, &#39;XGB__alpha&#39;: 0.1400844899032338}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:36:37,909] Trial 75 finished with value: 0.5638778424460545 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.12265137953447863, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.3710342976220817, &#39;XGB__alpha&#39;: 0.26683471589887003}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:36:40,553] Trial 76 finished with value: 0.5840721971368602 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 6, &#39;XGB__reg_lambda&#39;: 0.0919234778212512, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.45502056104492083, &#39;XGB__alpha&#39;: 0.18057553549987526}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:36:45,125] Trial 77 finished with value: 0.5832381973674123 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.17446332542764495, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.39131871371630067, &#39;XGB__alpha&#39;: 0.13639097555321036}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:36:52,877] Trial 78 finished with value: 0.5807510834697914 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.10399128558086343, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.2404695349757713, &#39;XGB__alpha&#39;: 0.282587458976255}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:36:56,175] Trial 79 finished with value: 0.5833815264697522 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.2026351454459434, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.30312950664397664, &#39;XGB__alpha&#39;: 0.19649802880019981}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:37:13,625] Trial 80 finished with value: 0.5676195830143079 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 25, &#39;XGB__reg_lambda&#39;: 0.23321060255794795, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.012576393384821952, &#39;XGB__alpha&#39;: 0.21950426962722822}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:37:17,758] Trial 81 finished with value: 0.5863073066066375 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.1455292999994213, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.4651400483036756, &#39;XGB__alpha&#39;: 0.1456587952612393}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:37:22,058] Trial 82 finished with value: 0.5834730241551862 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.13638163439400874, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.3457535414202096, &#39;XGB__alpha&#39;: 0.16301513070284995}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:37:26,254] Trial 83 finished with value: 0.5851177634798236 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.12590212249118005, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.4552801447381359, &#39;XGB__alpha&#39;: 0.2386501473502303}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:37:30,225] Trial 84 finished with value: 0.5888759889505308 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.1620136326616411, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.40217341548743146, &#39;XGB__alpha&#39;: 0.12442624986822932}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:37:33,044] Trial 85 finished with value: 0.5840806564205929 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.16246542756835428, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.3979265954254079, &#39;XGB__alpha&#39;: 0.12298437707936354}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:37:37,108] Trial 86 finished with value: 0.585859144793223 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.17573109999538172, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.34487620390943563, &#39;XGB__alpha&#39;: 0.11145231134302333}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:37:41,225] Trial 87 finished with value: 0.5848038688596103 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.11110718499142042, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.3122515409311502, &#39;XGB__alpha&#39;: 0.137895419126605}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:37:46,492] Trial 88 finished with value: 0.5857824086142226 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 15, &#39;XGB__reg_lambda&#39;: 0.15051173064442525, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.4042162977639466, &#39;XGB__alpha&#39;: 0.1550464079913524}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:37:51,109] Trial 89 finished with value: 0.5832488137121767 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.1235120971567459, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.26756911476666806, &#39;XGB__alpha&#39;: 0.13349382443610236}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:37:53,394] Trial 90 finished with value: 0.5622091501695428 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 4, &#39;XGB__reg_lambda&#39;: 0.04689536039749466, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.12569447761636304, &#39;XGB__alpha&#39;: 0.10075515005285385}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:37:57,608] Trial 91 finished with value: 0.5859661361467987 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.1488897034860535, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.4676131378892376, &#39;XGB__alpha&#39;: 0.1777887779518045}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:38:01,758] Trial 92 finished with value: 0.5888278239741779 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.1830555192941961, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.4567173576330661, &#39;XGB__alpha&#39;: 0.14841931834365046}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:38:06,092] Trial 93 finished with value: 0.5851048154816202 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.19764380600687068, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.38404939320372505, &#39;XGB__alpha&#39;: 0.16948280167041954}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:38:10,275] Trial 94 finished with value: 0.5865139508006706 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.22748396940917895, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.4448786195480299, &#39;XGB__alpha&#39;: 0.14351295903724884}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:38:14,908] Trial 95 finished with value: 0.5846579490312273 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.18175266816863872, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.41253580021263697, &#39;XGB__alpha&#39;: 0.12560496976494537}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:38:19,208] Trial 96 finished with value: 0.5862407981634951 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.16907147470870082, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.3533794104748943, &#39;XGB__alpha&#39;: 0.32688875265333633}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:38:25,594] Trial 97 finished with value: 0.5866473112397569 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 50, &#39;XGB__reg_lambda&#39;: 0.08988323518108872, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.49425647742046724, &#39;XGB__alpha&#39;: 0.15651510949348726}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:38:32,825] Trial 98 finished with value: 0.5842723250916931 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 30, &#39;XGB__reg_lambda&#39;: 0.2563811508690983, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.44136408737605987, &#39;XGB__alpha&#39;: 0.1087675905239308}. Best is trial 32 with value: 0.5908281911363783.
[I 2024-03-04 00:38:36,225] Trial 99 finished with value: 0.5835648653934217 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.1596163737316769, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.32176625289236555, &#39;XGB__alpha&#39;: 0.13046526794268168}. Best is trial 32 with value: 0.5908281911363783.
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>HPO of Random Forest</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;RF&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_RF_refined</span>

<span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">],</span>  
                               <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                               <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                               <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                               <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                               <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                               <span class="n">n_trials</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> 
                               <span class="n">random_state</span><span class="o">=</span><span class="mi">666</span><span class="p">)</span>

<span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

<span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
<span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
<span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 00:38:36,291] A new study created in memory with name: no-name-9ec253f0-c4ef-4898-89a1-cff7c233cd8c
[I 2024-03-04 00:38:41,290] Trial 0 finished with value: 0.5090889428015223 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 16, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 0 with value: 0.5090889428015223.
[I 2024-03-04 00:38:48,775] Trial 1 finished with value: 0.5090646012407415 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 16, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 0 with value: 0.5090889428015223.
[I 2024-03-04 00:38:52,634] Trial 2 finished with value: 0.5 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 4, &#39;RF__min_samples_split&#39;: 8, &#39;RF__min_samples_leaf&#39;: 18, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 0 with value: 0.5090889428015223.
[I 2024-03-04 00:38:57,225] Trial 3 finished with value: 0.5093203171652428 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 3, &#39;RF__min_samples_leaf&#39;: 10, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 3 with value: 0.5093203171652428.
[I 2024-03-04 00:39:02,414] Trial 4 finished with value: 0.5220915264762852 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 6, &#39;RF__min_samples_leaf&#39;: 20, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 4 with value: 0.5220915264762852.
[I 2024-03-04 00:39:09,395] Trial 5 finished with value: 0.5180807784325255 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 18, &#39;RF__min_samples_leaf&#39;: 16, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 4 with value: 0.5220915264762852.
[I 2024-03-04 00:39:12,699] Trial 6 finished with value: 0.5 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 4, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 4 with value: 0.5220915264762852.
[I 2024-03-04 00:39:26,695] Trial 7 finished with value: 0.5217991686743103 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 250, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 20, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 4 with value: 0.5220915264762852.
[I 2024-03-04 00:39:31,295] Trial 8 finished with value: 0.5 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 3, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 19, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 4 with value: 0.5220915264762852.
[I 2024-03-04 00:39:34,675] Trial 9 finished with value: 0.5 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 3, &#39;RF__min_samples_split&#39;: 8, &#39;RF__min_samples_leaf&#39;: 6, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 4 with value: 0.5220915264762852.
[I 2024-03-04 00:39:40,108] Trial 10 finished with value: 0.5265570821089619 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 2, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 10 with value: 0.5265570821089619.
[I 2024-03-04 00:39:45,594] Trial 11 finished with value: 0.5265570821089619 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 2, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 10 with value: 0.5265570821089619.
[I 2024-03-04 00:39:50,975] Trial 12 finished with value: 0.5262729245181154 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 2, &#39;RF__min_samples_leaf&#39;: 11, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 10 with value: 0.5265570821089619.
[I 2024-03-04 00:39:53,876] Trial 13 finished with value: 0.5006860524163352 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 5, &#39;RF__min_samples_split&#39;: 4, &#39;RF__min_samples_leaf&#39;: 13, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 10 with value: 0.5265570821089619.
[I 2024-03-04 00:39:58,208] Trial 14 finished with value: 0.5295409958854539 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 13, &#39;RF__min_samples_leaf&#39;: 8, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 14 with value: 0.5295409958854539.
[I 2024-03-04 00:40:02,426] Trial 15 finished with value: 0.5315868528177672 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 7, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 15 with value: 0.5315868528177672.
[I 2024-03-04 00:40:06,758] Trial 16 finished with value: 0.5315868528177672 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 13, &#39;RF__min_samples_leaf&#39;: 7, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 15 with value: 0.5315868528177672.
[I 2024-03-04 00:40:08,740] Trial 17 finished with value: 0.5006089276284285 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 5, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 6, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 15 with value: 0.5315868528177672.
[I 2024-03-04 00:40:12,000] Trial 18 finished with value: 0.5224202670831293 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 20, &#39;RF__min_samples_leaf&#39;: 5, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 15 with value: 0.5315868528177672.
[I 2024-03-04 00:40:14,941] Trial 19 finished with value: 0.5302066773102704 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 14, &#39;RF__min_samples_leaf&#39;: 9, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 15 with value: 0.5315868528177672.
[I 2024-03-04 00:40:22,258] Trial 20 finished with value: 0.5289116972655067 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 100, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 8, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 15 with value: 0.5315868528177672.
[I 2024-03-04 00:40:25,175] Trial 21 finished with value: 0.5302066773102704 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 14, &#39;RF__min_samples_leaf&#39;: 9, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 15 with value: 0.5315868528177672.
[I 2024-03-04 00:40:28,259] Trial 22 finished with value: 0.535695485249859 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 17, &#39;RF__min_samples_leaf&#39;: 4, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 22 with value: 0.535695485249859.
[I 2024-03-04 00:40:32,524] Trial 23 finished with value: 0.5338726729338358 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 17, &#39;RF__min_samples_leaf&#39;: 4, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 22 with value: 0.535695485249859.
[I 2024-03-04 00:40:35,575] Trial 24 finished with value: 0.535695485249859 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 17, &#39;RF__min_samples_leaf&#39;: 4, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 22 with value: 0.535695485249859.
[I 2024-03-04 00:40:38,474] Trial 25 finished with value: 0.535695485249859 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 17, &#39;RF__min_samples_leaf&#39;: 4, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 22 with value: 0.535695485249859.
[I 2024-03-04 00:40:41,706] Trial 26 finished with value: 0.5354236279757051 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 20, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 22 with value: 0.535695485249859.
[I 2024-03-04 00:40:44,658] Trial 27 finished with value: 0.5349604906393576 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 18, &#39;RF__min_samples_leaf&#39;: 4, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 22 with value: 0.535695485249859.
[I 2024-03-04 00:40:47,608] Trial 28 finished with value: 0.5349604906393576 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 18, &#39;RF__min_samples_leaf&#39;: 4, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 22 with value: 0.535695485249859.
[I 2024-03-04 00:40:50,605] Trial 29 finished with value: 0.5403840854988256 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 16, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:40:53,574] Trial 30 finished with value: 0.5403840854988256 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 16, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:40:56,908] Trial 31 finished with value: 0.5403840854988256 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 16, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:40:59,993] Trial 32 finished with value: 0.5380866127475751 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:41:03,093] Trial 33 finished with value: 0.5380866127475751 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:41:11,704] Trial 34 finished with value: 0.5092188508165552 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 250, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:41:13,141] Trial 35 finished with value: 0.5 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 4, &#39;RF__min_samples_split&#39;: 16, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:41:20,873] Trial 36 finished with value: 0.533373152792619 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 100, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 19, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:41:28,057] Trial 37 finished with value: 0.5091295552482579 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 16, &#39;RF__min_samples_leaf&#39;: 15, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:41:34,241] Trial 38 finished with value: 0.5217627858694412 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 14, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:41:37,124] Trial 39 finished with value: 0.5326303465798959 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 19, &#39;RF__min_samples_leaf&#39;: 5, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:41:38,724] Trial 40 finished with value: 0.5 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 3, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:41:41,800] Trial 41 finished with value: 0.5380866127475751 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:41:44,974] Trial 42 finished with value: 0.5380866127475751 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:41:49,074] Trial 43 finished with value: 0.5 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 4, &#39;RF__min_samples_split&#39;: 16, &#39;RF__min_samples_leaf&#39;: 5, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:42:07,593] Trial 44 finished with value: 0.5333732823289212 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 250, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 13, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:42:09,394] Trial 45 finished with value: 0.5011488011437762 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 5, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:42:23,641] Trial 46 finished with value: 0.5305557889086332 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 6, &#39;RF__min_samples_leaf&#39;: 6, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:42:26,707] Trial 47 finished with value: 0.5 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 3, &#39;RF__min_samples_split&#39;: 16, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:42:29,064] Trial 48 finished with value: 0.5237884013465378 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 19, &#39;RF__min_samples_leaf&#39;: 17, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
[I 2024-03-04 00:42:34,875] Trial 49 finished with value: 0.5089387933312729 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 14, &#39;RF__min_samples_leaf&#39;: 5, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 29 with value: 0.5403840854988256.
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>HPO of SVM</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;SVM&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_linear_SVM_refined</span>

<span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">],</span>  
                               <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                               <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                               <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                               <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                               <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                               <span class="n">n_trials</span><span class="o">=</span><span class="mi">75</span><span class="p">,</span> 
                               <span class="n">random_state</span><span class="o">=</span><span class="mi">666</span><span class="p">)</span>

<span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

<span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
<span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
<span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-25 19:03:39,486] A new study created in memory with name: no-name-84229dad-a4c5-4a43-b9cf-b0af48146654
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-25 19:03:46,641] Trial 0 finished with value: 0.7340715686041941 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.2051936653659478, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 0 with value: 0.7340715686041941.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
[W 2024-03-25 19:03:51,690] Trial 1 failed with parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.6119070120996282, &#39;SVM__class_weight&#39;: &#39;balanced&#39;} because of the following error: KeyboardInterrupt().
Traceback (most recent call last):
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\optuna\study\_optimize.py&quot;, line 200, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File &quot;C:\Users\fscielzo\Documents\DataScience-GitHub\Regression\ML\PyML.py&quot;, line 46, in &lt;lambda&gt;
  File &quot;C:\Users\fscielzo\Documents\DataScience-GitHub\Regression\ML\PyML.py&quot;, line 39, in objective
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\model_selection\_validation.py&quot;, line 562, in cross_val_score
    cv_results = cross_validate(
                 ^^^^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\_param_validation.py&quot;, line 211, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\model_selection\_validation.py&quot;, line 309, in cross_validate
    results = parallel(
              ^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\parallel.py&quot;, line 65, in __call__
    return super().__call__(iterable_with_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\joblib\parallel.py&quot;, line 1863, in __call__
    return output if self.return_generator else list(output)
                                                ^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\joblib\parallel.py&quot;, line 1792, in _get_sequential_output
    res = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\parallel.py&quot;, line 127, in __call__
    return self.function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\model_selection\_validation.py&quot;, line 729, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\base.py&quot;, line 1152, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\pipeline.py&quot;, line 427, in fit
    self._final_estimator.fit(Xt, y, **fit_params_last_step)
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\base.py&quot;, line 1152, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py&quot;, line 326, in fit
    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                                           ^^^^^^^^^^^^^^^
  File &quot;c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py&quot;, line 1230, in _fit_liblinear
    raw_coef_, n_iter_ = liblinear.train_wrap(
                         ^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
[W 2024-03-25 19:03:51,708] Trial 1 failed with value None.
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">133</span><span class="p">],</span> <span class="n">line</span> <span class="mi">13</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_linear_SVM_refined</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">],</span>  
<span class="g g-Whitespace">      </span><span class="mi">5</span>                                <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
<span class="g g-Whitespace">      </span><span class="mi">6</span>                                <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">10</span>                                <span class="n">n_trials</span><span class="o">=</span><span class="mi">75</span><span class="p">,</span> 
<span class="g g-Whitespace">     </span><span class="mi">11</span>                                <span class="n">random_state</span><span class="o">=</span><span class="mi">666</span><span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">13</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span> <span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
<span class="g g-Whitespace">     </span><span class="mi">16</span> <span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>

<span class="nn">File ~\Documents\DataScience-GitHub\Regression\ML\PyML.py:167,</span> in <span class="ni">fit</span><span class="nt">(self, X, Y)</span>

<span class="nn">File ~\Documents\DataScience-GitHub\Regression\ML\PyML.py:46,</span> in <span class="ni">fit</span><span class="nt">(self, X, y)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\optuna\study\study.py:451,</span> in <span class="ni">Study.optimize</span><span class="nt">(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)</span>
<span class="g g-Whitespace">    </span><span class="mi">348</span> <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">349</span>     <span class="bp">self</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">350</span>     <span class="n">func</span><span class="p">:</span> <span class="n">ObjectiveFuncType</span><span class="p">,</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">357</span>     <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">358</span> <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">359</span><span class="w">     </span><span class="sd">&quot;&quot;&quot;Optimize an objective function.</span>
<span class="g g-Whitespace">    </span><span class="mi">360</span><span class="sd"> </span>
<span class="g g-Whitespace">    </span><span class="mi">361</span><span class="sd">     Optimization is done by choosing a suitable set of hyperparameter values from a given</span>
<span class="sd">   (...)</span>
<span class="g g-Whitespace">    </span><span class="mi">449</span><span class="sd">             If nested invocation of this method occurs.</span>
<span class="g g-Whitespace">    </span><span class="mi">450</span><span class="sd">     &quot;&quot;&quot;</span>
<span class="ne">--&gt; </span><span class="mi">451</span>     <span class="n">_optimize</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">452</span>         <span class="n">study</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">453</span>         <span class="n">func</span><span class="o">=</span><span class="n">func</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">454</span>         <span class="n">n_trials</span><span class="o">=</span><span class="n">n_trials</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">455</span>         <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">456</span>         <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">457</span>         <span class="n">catch</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">catch</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">catch</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="n">catch</span><span class="p">,),</span>
<span class="g g-Whitespace">    </span><span class="mi">458</span>         <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">459</span>         <span class="n">gc_after_trial</span><span class="o">=</span><span class="n">gc_after_trial</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">460</span>         <span class="n">show_progress_bar</span><span class="o">=</span><span class="n">show_progress_bar</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">461</span>     <span class="p">)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\optuna\study\_optimize.py:66,</span> in <span class="ni">_optimize</span><span class="nt">(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)</span>
<span class="g g-Whitespace">     </span><span class="mi">64</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">65</span>     <span class="k">if</span> <span class="n">n_jobs</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
<span class="ne">---&gt; </span><span class="mi">66</span>         <span class="n">_optimize_sequential</span><span class="p">(</span>
<span class="g g-Whitespace">     </span><span class="mi">67</span>             <span class="n">study</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">68</span>             <span class="n">func</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">69</span>             <span class="n">n_trials</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">70</span>             <span class="n">timeout</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">71</span>             <span class="n">catch</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">72</span>             <span class="n">callbacks</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">73</span>             <span class="n">gc_after_trial</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">74</span>             <span class="n">reseed_sampler_rng</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">75</span>             <span class="n">time_start</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">76</span>             <span class="n">progress_bar</span><span class="o">=</span><span class="n">progress_bar</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">77</span>         <span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">78</span>     <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">79</span>         <span class="k">if</span> <span class="n">n_jobs</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\optuna\study\_optimize.py:163,</span> in <span class="ni">_optimize_sequential</span><span class="nt">(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)</span>
<span class="g g-Whitespace">    </span><span class="mi">160</span>         <span class="k">break</span>
<span class="g g-Whitespace">    </span><span class="mi">162</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">163</span>     <span class="n">frozen_trial</span> <span class="o">=</span> <span class="n">_run_trial</span><span class="p">(</span><span class="n">study</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">catch</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">164</span> <span class="k">finally</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">165</span>     <span class="c1"># The following line mitigates memory problems that can be occurred in some</span>
<span class="g g-Whitespace">    </span><span class="mi">166</span>     <span class="c1"># environments (e.g., services that use computing containers such as GitHub Actions).</span>
<span class="g g-Whitespace">    </span><span class="mi">167</span>     <span class="c1"># Please refer to the following PR for further details:</span>
<span class="g g-Whitespace">    </span><span class="mi">168</span>     <span class="c1"># https://github.com/optuna/optuna/pull/325.</span>
<span class="g g-Whitespace">    </span><span class="mi">169</span>     <span class="k">if</span> <span class="n">gc_after_trial</span><span class="p">:</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\optuna\study\_optimize.py:251,</span> in <span class="ni">_run_trial</span><span class="nt">(study, func, catch)</span>
<span class="g g-Whitespace">    </span><span class="mi">244</span>         <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;Should not reach.&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">246</span> <span class="k">if</span> <span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">247</span>     <span class="n">frozen_trial</span><span class="o">.</span><span class="n">state</span> <span class="o">==</span> <span class="n">TrialState</span><span class="o">.</span><span class="n">FAIL</span>
<span class="g g-Whitespace">    </span><span class="mi">248</span>     <span class="ow">and</span> <span class="n">func_err</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span class="g g-Whitespace">    </span><span class="mi">249</span>     <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func_err</span><span class="p">,</span> <span class="n">catch</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">250</span> <span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">251</span>     <span class="k">raise</span> <span class="n">func_err</span>
<span class="g g-Whitespace">    </span><span class="mi">252</span> <span class="k">return</span> <span class="n">frozen_trial</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\optuna\study\_optimize.py:200,</span> in <span class="ni">_run_trial</span><span class="nt">(study, func, catch)</span>
<span class="g g-Whitespace">    </span><span class="mi">198</span> <span class="k">with</span> <span class="n">get_heartbeat_thread</span><span class="p">(</span><span class="n">trial</span><span class="o">.</span><span class="n">_trial_id</span><span class="p">,</span> <span class="n">study</span><span class="o">.</span><span class="n">_storage</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">199</span>     <span class="k">try</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">200</span>         <span class="n">value_or_values</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">201</span>     <span class="k">except</span> <span class="n">exceptions</span><span class="o">.</span><span class="n">TrialPruned</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">202</span>         <span class="c1"># TODO(mamu): Handle multi-objective cases.</span>
<span class="g g-Whitespace">    </span><span class="mi">203</span>         <span class="n">state</span> <span class="o">=</span> <span class="n">TrialState</span><span class="o">.</span><span class="n">PRUNED</span>

<span class="nn">File ~\Documents\DataScience-GitHub\Regression\ML\PyML.py:46,</span> in <span class="ni">&lt;lambda&gt;</span><span class="nt">(trial)</span>

<span class="nn">File ~\Documents\DataScience-GitHub\Regression\ML\PyML.py:39,</span> in <span class="ni">objective</span><span class="nt">(self, trial, X, y)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\model_selection\_validation.py:562,</span> in <span class="ni">cross_val_score</span><span class="nt">(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)</span>
<span class="g g-Whitespace">    </span><span class="mi">559</span> <span class="c1"># To ensure multimetric format is not supported</span>
<span class="g g-Whitespace">    </span><span class="mi">560</span> <span class="n">scorer</span> <span class="o">=</span> <span class="n">check_scoring</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">562</span> <span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">563</span>     <span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">564</span>     <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">565</span>     <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">566</span>     <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">567</span>     <span class="n">scoring</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;score&quot;</span><span class="p">:</span> <span class="n">scorer</span><span class="p">},</span>
<span class="g g-Whitespace">    </span><span class="mi">568</span>     <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">569</span>     <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">570</span>     <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">571</span>     <span class="n">fit_params</span><span class="o">=</span><span class="n">fit_params</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">572</span>     <span class="n">pre_dispatch</span><span class="o">=</span><span class="n">pre_dispatch</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">573</span>     <span class="n">error_score</span><span class="o">=</span><span class="n">error_score</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">574</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">575</span> <span class="k">return</span> <span class="n">cv_results</span><span class="p">[</span><span class="s2">&quot;test_score&quot;</span><span class="p">]</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\_param_validation.py:211,</span> in <span class="ni">validate_params.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">205</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">206</span>     <span class="k">with</span> <span class="n">config_context</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">207</span>         <span class="n">skip_parameter_validation</span><span class="o">=</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">208</span>             <span class="n">prefer_skip_nested_validation</span> <span class="ow">or</span> <span class="n">global_skip_validation</span>
<span class="g g-Whitespace">    </span><span class="mi">209</span>         <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">210</span>     <span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">211</span>         <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">212</span> <span class="k">except</span> <span class="n">InvalidParameterError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">213</span>     <span class="c1"># When the function is just a wrapper around an estimator, we allow</span>
<span class="g g-Whitespace">    </span><span class="mi">214</span>     <span class="c1"># the function to delegate validation to the estimator, but we replace</span>
<span class="g g-Whitespace">    </span><span class="mi">215</span>     <span class="c1"># the name of the estimator by the name of the function in the error</span>
<span class="g g-Whitespace">    </span><span class="mi">216</span>     <span class="c1"># message to avoid confusion.</span>
<span class="g g-Whitespace">    </span><span class="mi">217</span>     <span class="n">msg</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">218</span>         <span class="sa">r</span><span class="s2">&quot;parameter of \w+ must be&quot;</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">219</span>         <span class="sa">f</span><span class="s2">&quot;parameter of </span><span class="si">{</span><span class="n">func</span><span class="o">.</span><span class="vm">__qualname__</span><span class="si">}</span><span class="s2"> must be&quot;</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">220</span>         <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">221</span>     <span class="p">)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\model_selection\_validation.py:309,</span> in <span class="ni">cross_validate</span><span class="nt">(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)</span>
<span class="g g-Whitespace">    </span><span class="mi">306</span> <span class="c1"># We clone the estimator to make sure that all the folds are</span>
<span class="g g-Whitespace">    </span><span class="mi">307</span> <span class="c1"># independent, and that it is pickle-able.</span>
<span class="g g-Whitespace">    </span><span class="mi">308</span> <span class="n">parallel</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span> <span class="n">pre_dispatch</span><span class="o">=</span><span class="n">pre_dispatch</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">309</span> <span class="n">results</span> <span class="o">=</span> <span class="n">parallel</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">310</span>     <span class="n">delayed</span><span class="p">(</span><span class="n">_fit_and_score</span><span class="p">)(</span>
<span class="g g-Whitespace">    </span><span class="mi">311</span>         <span class="n">clone</span><span class="p">(</span><span class="n">estimator</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">312</span>         <span class="n">X</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">313</span>         <span class="n">y</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">314</span>         <span class="n">scorers</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">315</span>         <span class="n">train</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">316</span>         <span class="n">test</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">317</span>         <span class="n">verbose</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">318</span>         <span class="kc">None</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">319</span>         <span class="n">fit_params</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">320</span>         <span class="n">return_train_score</span><span class="o">=</span><span class="n">return_train_score</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">321</span>         <span class="n">return_times</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">322</span>         <span class="n">return_estimator</span><span class="o">=</span><span class="n">return_estimator</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">323</span>         <span class="n">error_score</span><span class="o">=</span><span class="n">error_score</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">324</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">325</span>     <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">indices</span>
<span class="g g-Whitespace">    </span><span class="mi">326</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">328</span> <span class="n">_warn_or_raise_about_fit_failures</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">error_score</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">330</span> <span class="c1"># For callable scoring, the return type is only know after calling. If the</span>
<span class="g g-Whitespace">    </span><span class="mi">331</span> <span class="c1"># return type is a dictionary, the error scores can now be inserted with</span>
<span class="g g-Whitespace">    </span><span class="mi">332</span> <span class="c1"># the correct key.</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\parallel.py:65,</span> in <span class="ni">Parallel.__call__</span><span class="nt">(self, iterable)</span>
<span class="g g-Whitespace">     </span><span class="mi">60</span> <span class="n">config</span> <span class="o">=</span> <span class="n">get_config</span><span class="p">()</span>
<span class="g g-Whitespace">     </span><span class="mi">61</span> <span class="n">iterable_with_config</span> <span class="o">=</span> <span class="p">(</span>
<span class="g g-Whitespace">     </span><span class="mi">62</span>     <span class="p">(</span><span class="n">_with_config</span><span class="p">(</span><span class="n">delayed_func</span><span class="p">,</span> <span class="n">config</span><span class="p">),</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">63</span>     <span class="k">for</span> <span class="n">delayed_func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="n">iterable</span>
<span class="g g-Whitespace">     </span><span class="mi">64</span> <span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">65</span> <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">iterable_with_config</span><span class="p">)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\joblib\parallel.py:1863,</span> in <span class="ni">Parallel.__call__</span><span class="nt">(self, iterable)</span>
<span class="g g-Whitespace">   </span><span class="mi">1861</span>     <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_sequential_output</span><span class="p">(</span><span class="n">iterable</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1862</span>     <span class="nb">next</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1863</span>     <span class="k">return</span> <span class="n">output</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_generator</span> <span class="k">else</span> <span class="nb">list</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1865</span> <span class="c1"># Let&#39;s create an ID that uniquely identifies the current call. If the</span>
<span class="g g-Whitespace">   </span><span class="mi">1866</span> <span class="c1"># call is interrupted early and that the same instance is immediately</span>
<span class="g g-Whitespace">   </span><span class="mi">1867</span> <span class="c1"># re-used, this id will be used to prevent workers that were</span>
<span class="g g-Whitespace">   </span><span class="mi">1868</span> <span class="c1"># concurrently finalizing a task from the previous call to run the</span>
<span class="g g-Whitespace">   </span><span class="mi">1869</span> <span class="c1"># callback.</span>
<span class="g g-Whitespace">   </span><span class="mi">1870</span> <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\joblib\parallel.py:1792,</span> in <span class="ni">Parallel._get_sequential_output</span><span class="nt">(self, iterable)</span>
<span class="g g-Whitespace">   </span><span class="mi">1790</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dispatched_batches</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="g g-Whitespace">   </span><span class="mi">1791</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dispatched_tasks</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="ne">-&gt; </span><span class="mi">1792</span> <span class="n">res</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1793</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_completed_tasks</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="g g-Whitespace">   </span><span class="mi">1794</span> <span class="bp">self</span><span class="o">.</span><span class="n">print_progress</span><span class="p">()</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\parallel.py:127,</span> in <span class="ni">_FuncWrapper.__call__</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">125</span>     <span class="n">config</span> <span class="o">=</span> <span class="p">{}</span>
<span class="g g-Whitespace">    </span><span class="mi">126</span> <span class="k">with</span> <span class="n">config_context</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">127</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\model_selection\_validation.py:729,</span> in <span class="ni">_fit_and_score</span><span class="nt">(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)</span>
<span class="g g-Whitespace">    </span><span class="mi">727</span>         <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">728</span>     <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">729</span>         <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">731</span> <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">732</span>     <span class="c1"># Note fit time as time until error</span>
<span class="g g-Whitespace">    </span><span class="mi">733</span>     <span class="n">fit_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\base.py:1152,</span> in <span class="ni">_fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper</span><span class="nt">(estimator, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1145</span>     <span class="n">estimator</span><span class="o">.</span><span class="n">_validate_params</span><span class="p">()</span>
<span class="g g-Whitespace">   </span><span class="mi">1147</span> <span class="k">with</span> <span class="n">config_context</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1148</span>     <span class="n">skip_parameter_validation</span><span class="o">=</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1149</span>         <span class="n">prefer_skip_nested_validation</span> <span class="ow">or</span> <span class="n">global_skip_validation</span>
<span class="g g-Whitespace">   </span><span class="mi">1150</span>     <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1151</span> <span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1152</span>     <span class="k">return</span> <span class="n">fit_method</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\pipeline.py:427,</span> in <span class="ni">Pipeline.fit</span><span class="nt">(self, X, y, **fit_params)</span>
<span class="g g-Whitespace">    </span><span class="mi">425</span>     <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_final_estimator</span> <span class="o">!=</span> <span class="s2">&quot;passthrough&quot;</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">426</span>         <span class="n">fit_params_last_step</span> <span class="o">=</span> <span class="n">fit_params_steps</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span>
<span class="ne">--&gt; </span><span class="mi">427</span>         <span class="bp">self</span><span class="o">.</span><span class="n">_final_estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xt</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params_last_step</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">429</span> <span class="k">return</span> <span class="bp">self</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\base.py:1152,</span> in <span class="ni">_fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper</span><span class="nt">(estimator, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1145</span>     <span class="n">estimator</span><span class="o">.</span><span class="n">_validate_params</span><span class="p">()</span>
<span class="g g-Whitespace">   </span><span class="mi">1147</span> <span class="k">with</span> <span class="n">config_context</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1148</span>     <span class="n">skip_parameter_validation</span><span class="o">=</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1149</span>         <span class="n">prefer_skip_nested_validation</span> <span class="ow">or</span> <span class="n">global_skip_validation</span>
<span class="g g-Whitespace">   </span><span class="mi">1150</span>     <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1151</span> <span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1152</span>     <span class="k">return</span> <span class="n">fit_method</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:326,</span> in <span class="ni">LinearSVC.fit</span><span class="nt">(self, X, y, sample_weight)</span>
<span class="g g-Whitespace">    </span><span class="mi">320</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">322</span> <span class="n">_dual</span> <span class="o">=</span> <span class="n">_validate_dual_parameter</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">323</span>     <span class="bp">self</span><span class="o">.</span><span class="n">dual</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span><span class="p">,</span> <span class="n">X</span>
<span class="g g-Whitespace">    </span><span class="mi">324</span> <span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">326</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">n_iter_</span> <span class="o">=</span> <span class="n">_fit_liblinear</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">327</span>     <span class="n">X</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">328</span>     <span class="n">y</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">329</span>     <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">330</span>     <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">331</span>     <span class="bp">self</span><span class="o">.</span><span class="n">intercept_scaling</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">332</span>     <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">333</span>     <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">334</span>     <span class="n">_dual</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">335</span>     <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">336</span>     <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">337</span>     <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">338</span>     <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">339</span>     <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">340</span>     <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">341</span>     <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">342</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">343</span> <span class="c1"># Backward compatibility: _fit_liblinear is used both by LinearSVC/R</span>
<span class="g g-Whitespace">    </span><span class="mi">344</span> <span class="c1"># and LogisticRegression but LogisticRegression sets a structured</span>
<span class="g g-Whitespace">    </span><span class="mi">345</span> <span class="c1"># `n_iter_` attribute with information about the underlying OvR fits</span>
<span class="g g-Whitespace">    </span><span class="mi">346</span> <span class="c1"># while LinearSVC/R only reports the maximum value.</span>
<span class="g g-Whitespace">    </span><span class="mi">347</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">n_iter_</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="nn">File c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1230,</span> in <span class="ni">_fit_liblinear</span><span class="nt">(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)</span>
<span class="g g-Whitespace">   </span><span class="mi">1227</span> <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">_check_sample_weight</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1229</span> <span class="n">solver_type</span> <span class="o">=</span> <span class="n">_get_liblinear_solver_type</span><span class="p">(</span><span class="n">multi_class</span><span class="p">,</span> <span class="n">penalty</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dual</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1230</span> <span class="n">raw_coef_</span><span class="p">,</span> <span class="n">n_iter_</span> <span class="o">=</span> <span class="n">liblinear</span><span class="o">.</span><span class="n">train_wrap</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1231</span>     <span class="n">X</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1232</span>     <span class="n">y_ind</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1233</span>     <span class="n">sp</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1234</span>     <span class="n">solver_type</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1235</span>     <span class="n">tol</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1236</span>     <span class="n">bias</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1237</span>     <span class="n">C</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1238</span>     <span class="n">class_weight_</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1239</span>     <span class="n">max_iter</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1240</span>     <span class="n">rnd</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="s2">&quot;i&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1241</span>     <span class="n">epsilon</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1242</span>     <span class="n">sample_weight</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1243</span> <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1244</span> <span class="c1"># Regarding rnd.randint(..) in the above signature:</span>
<span class="g g-Whitespace">   </span><span class="mi">1245</span> <span class="c1"># seed for srand in range [0..INT_MAX); due to limitations in Numpy</span>
<span class="g g-Whitespace">   </span><span class="mi">1246</span> <span class="c1"># on 32-bit platforms, we can&#39;t get to the UINT_MAX limit that</span>
<span class="g g-Whitespace">   </span><span class="mi">1247</span> <span class="c1"># srand supports</span>
<span class="g g-Whitespace">   </span><span class="mi">1248</span> <span class="n">n_iter_max</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">n_iter_</span><span class="p">)</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>HPO of Neural Network (MultiLayer perceptron)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;NN&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_MLP_NN_refined</span>

<span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">],</span>  
                               <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                               <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                               <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                               <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                               <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                               <span class="n">n_trials</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> 
                               <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

<span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
<span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
<span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">130</span><span class="p">],</span> <span class="n">line</span> <span class="mi">2</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;NN&#39;</span>
<span class="ne">----&gt; </span><span class="mi">2</span> <span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_MLP_NN_refined</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">],</span>  
<span class="g g-Whitespace">      </span><span class="mi">5</span>                                <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
<span class="g g-Whitespace">      </span><span class="mi">6</span>                                <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">10</span>                                <span class="n">n_trials</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> 
<span class="g g-Whitespace">     </span><span class="mi">11</span>                                <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">13</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

<span class="ne">NameError</span>: name &#39;param_grid_MLP_NN_refined&#39; is not defined
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>HPO of Gradient Boosting</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;GB&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_GB_refined</span>

<span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">],</span>  
                               <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                               <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                               <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                               <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                               <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                               <span class="n">n_trials</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> 
                               <span class="n">random_state</span><span class="o">=</span><span class="mi">111</span><span class="p">)</span>

<span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

<span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
<span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
<span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 09:09:51,649] A new study created in memory with name: no-name-d6440d79-7c5c-4722-a4fb-528188a32fe8
[I 2024-03-04 09:11:18,321] Trial 0 finished with value: 0.5751837258429721 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;GB__n_estimators&#39;: 130, &#39;GB__max_depth&#39;: 20, &#39;GB__min_samples_split&#39;: 3, &#39;GB__min_samples_leaf&#39;: 14}. Best is trial 0 with value: 0.5751837258429721.
[I 2024-03-04 09:11:52,171] Trial 1 finished with value: 0.5748224434642188 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;GB__n_estimators&#39;: 50, &#39;GB__max_depth&#39;: 15, &#39;GB__min_samples_split&#39;: 10, &#39;GB__min_samples_leaf&#39;: 2}. Best is trial 0 with value: 0.5751837258429721.
[I 2024-03-04 09:12:06,754] Trial 2 finished with value: 0.5597038673975397 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;GB__n_estimators&#39;: 100, &#39;GB__max_depth&#39;: 4, &#39;GB__min_samples_split&#39;: 8, &#39;GB__min_samples_leaf&#39;: 9}. Best is trial 0 with value: 0.5751837258429721.
[I 2024-03-04 09:13:03,981] Trial 3 finished with value: 0.5732679064619939 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;GB__n_estimators&#39;: 70, &#39;GB__max_depth&#39;: 15, &#39;GB__min_samples_split&#39;: 2, &#39;GB__min_samples_leaf&#39;: 2}. Best is trial 0 with value: 0.5751837258429721.
[I 2024-03-04 09:14:42,170] Trial 4 finished with value: 0.573930135462757 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;GB__n_estimators&#39;: 150, &#39;GB__max_depth&#39;: 20, &#39;GB__min_samples_split&#39;: 10, &#39;GB__min_samples_leaf&#39;: 13}. Best is trial 0 with value: 0.5751837258429721.
[I 2024-03-04 09:15:07,920] Trial 5 finished with value: 0.566379672794004 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;GB__n_estimators&#39;: 70, &#39;GB__max_depth&#39;: 10, &#39;GB__min_samples_split&#39;: 8, &#39;GB__min_samples_leaf&#39;: 10}. Best is trial 0 with value: 0.5751837258429721.
[I 2024-03-04 09:15:19,118] Trial 6 finished with value: 0.5578891257563428 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;GB__n_estimators&#39;: 100, &#39;GB__max_depth&#39;: 3, &#39;GB__min_samples_split&#39;: 15, &#39;GB__min_samples_leaf&#39;: 13}. Best is trial 0 with value: 0.5751837258429721.
[I 2024-03-04 09:15:27,491] Trial 7 finished with value: 0.553130953249087 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;GB__n_estimators&#39;: 70, &#39;GB__max_depth&#39;: 3, &#39;GB__min_samples_split&#39;: 19, &#39;GB__min_samples_leaf&#39;: 8}. Best is trial 0 with value: 0.5751837258429721.
[I 2024-03-04 09:15:52,794] Trial 8 finished with value: 0.5693184854092901 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;GB__n_estimators&#39;: 70, &#39;GB__max_depth&#39;: 10, &#39;GB__min_samples_split&#39;: 14, &#39;GB__min_samples_leaf&#39;: 11}. Best is trial 0 with value: 0.5751837258429721.
[I 2024-03-04 09:15:59,120] Trial 9 finished with value: 0.5439967797500563 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;GB__n_estimators&#39;: 50, &#39;GB__max_depth&#39;: 3, &#39;GB__min_samples_split&#39;: 13, &#39;GB__min_samples_leaf&#39;: 6}. Best is trial 0 with value: 0.5751837258429721.
[I 2024-03-04 09:17:16,869] Trial 10 finished with value: 0.5790641772627925 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;GB__n_estimators&#39;: 130, &#39;GB__max_depth&#39;: 20, &#39;GB__min_samples_split&#39;: 2, &#39;GB__min_samples_leaf&#39;: 20}. Best is trial 10 with value: 0.5790641772627925.
[I 2024-03-04 09:18:34,852] Trial 11 finished with value: 0.5790641772627925 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;GB__n_estimators&#39;: 130, &#39;GB__max_depth&#39;: 20, &#39;GB__min_samples_split&#39;: 2, &#39;GB__min_samples_leaf&#39;: 20}. Best is trial 10 with value: 0.5790641772627925.
[I 2024-03-04 09:19:05,952] Trial 12 finished with value: 0.5633144045877388 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;GB__n_estimators&#39;: 130, &#39;GB__max_depth&#39;: 7, &#39;GB__min_samples_split&#39;: 5, &#39;GB__min_samples_leaf&#39;: 20}. Best is trial 10 with value: 0.5790641772627925.
[I 2024-03-04 09:20:41,100] Trial 13 finished with value: 0.578257323796668 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;GB__n_estimators&#39;: 130, &#39;GB__max_depth&#39;: 30, &#39;GB__min_samples_split&#39;: 5, &#39;GB__min_samples_leaf&#39;: 20}. Best is trial 10 with value: 0.5790641772627925.
[I 2024-03-04 09:22:01,702] Trial 14 finished with value: 0.5773026074576278 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;GB__n_estimators&#39;: 130, &#39;GB__max_depth&#39;: 20, &#39;GB__min_samples_split&#39;: 5, &#39;GB__min_samples_leaf&#39;: 17}. Best is trial 10 with value: 0.5790641772627925.
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>HPO of Bagging KNN</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;bagging_knn&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_bagging_knn_refined</span>

<span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">],</span>  
                               <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                               <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                               <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                               <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                               <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                               <span class="n">n_trials</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
                               <span class="n">random_state</span><span class="o">=</span><span class="mi">111</span><span class="p">)</span>

<span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

<span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
<span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
<span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 09:31:43,752] A new study created in memory with name: no-name-c6bb9ae1-e646-45d8-a9a8-42d767f7faeb
[I 2024-03-04 09:35:34,248] Trial 0 finished with value: 0.5112041129195205 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;bagging_knn__estimator__n_neighbors&#39;: 16, &#39;bagging_knn__estimator__metric&#39;: &#39;cityblock&#39;, &#39;bagging_knn__n_estimators&#39;: 50, &#39;bagging_knn__max_features&#39;: 0.7, &#39;bagging_knn__max_samples&#39;: 0.7}. Best is trial 0 with value: 0.5112041129195205.
[I 2024-03-04 09:41:34,147] Trial 1 finished with value: 0.5641687359246136 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;bagging_knn__estimator__n_neighbors&#39;: 2, &#39;bagging_knn__estimator__metric&#39;: &#39;cosine&#39;, &#39;bagging_knn__n_estimators&#39;: 20, &#39;bagging_knn__max_features&#39;: 0.9, &#39;bagging_knn__max_samples&#39;: 0.9}. Best is trial 1 with value: 0.5641687359246136.
[I 2024-03-04 09:46:48,911] Trial 2 finished with value: 0.5216130250080983 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;bagging_knn__estimator__n_neighbors&#39;: 16, &#39;bagging_knn__estimator__metric&#39;: &#39;cityblock&#39;, &#39;bagging_knn__n_estimators&#39;: 50, &#39;bagging_knn__max_features&#39;: 0.9, &#39;bagging_knn__max_samples&#39;: 0.8}. Best is trial 1 with value: 0.5641687359246136.
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>HPO of Logistic Regression</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;logistic_reg&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_logistic_regression_refined</span>

<span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">],</span>  
                               <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                               <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                               <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                               <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                               <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                               <span class="n">n_trials</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
                               <span class="n">random_state</span><span class="o">=</span><span class="mi">666</span><span class="p">)</span>

<span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

<span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
<span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
<span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 10:12:06,661] A new study created in memory with name: no-name-ccac7b4a-e7a6-438e-bcdd-6f64ccb4432f
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 10:12:23,363] Trial 0 finished with value: 0.7279459939952376 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.0011013709936245023, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.2591717724232343}. Best is trial 0 with value: 0.7279459939952376.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 10:12:40,704] Trial 1 finished with value: 0.7361896279449317 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.004332537076521914, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.5021630485318783}. Best is trial 1 with value: 0.7361896279449317.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 10:12:54,639] Trial 2 finished with value: 0.7440472493461853 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.006569922580508873, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 2 with value: 0.7440472493461853.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 10:13:12,105] Trial 3 finished with value: 0.7460332886669635 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.6964956238207622, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.34103782045412784}. Best is trial 3 with value: 0.7460332886669635.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 10:13:24,957] Trial 4 finished with value: 0.7456517648329936 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.5048084491810217, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 3 with value: 0.7460332886669635.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:13:26,787] Trial 5 finished with value: 0.740150397504108 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.0018277245304059724, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 3 with value: 0.7460332886669635.
[I 2024-03-04 10:13:34,804] Trial 6 finished with value: 0.7462323803313792 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.6483042462073663, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.26186771615480675}. Best is trial 6 with value: 0.7462323803313792.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 10:13:37,976] Trial 7 finished with value: 0.7433263685606067 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.02435643285859591, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 6 with value: 0.7462323803313792.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 10:13:41,643] Trial 8 finished with value: 0.7434521764700754 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.026429548138747785, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 6 with value: 0.7462323803313792.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 10:13:57,692] Trial 9 finished with value: 0.7457004479545554 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.04413819284996608, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 6 with value: 0.7462323803313792.
[I 2024-03-04 10:14:03,971] Trial 10 finished with value: 0.7461307844463893 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.2475281314156233, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.10245388213605865}. Best is trial 6 with value: 0.7462323803313792.
[I 2024-03-04 10:14:09,907] Trial 11 finished with value: 0.7459806349761396 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.3243031912043194, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.11071342995252571}. Best is trial 6 with value: 0.7462323803313792.
[I 2024-03-04 10:14:16,555] Trial 12 finished with value: 0.7462281506895128 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.20673561355211917, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.11355550470070468}. Best is trial 6 with value: 0.7462323803313792.
[I 2024-03-04 10:14:22,126] Trial 13 finished with value: 0.7461956089176033 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.14115260959726642, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.21998288779200076}. Best is trial 6 with value: 0.7462323803313792.
[I 2024-03-04 10:14:29,441] Trial 14 finished with value: 0.7463136052248501 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.677166148005248, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.976345749726183}. Best is trial 14 with value: 0.7463136052248501.
[I 2024-03-04 10:14:40,821] Trial 15 finished with value: 0.7463014344444597 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.9795838612644372, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.8168393810277531}. Best is trial 14 with value: 0.7463136052248501.
[I 2024-03-04 10:14:48,160] Trial 16 finished with value: 0.7463136052248501 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.736104321293743, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.9890807907249471}. Best is trial 14 with value: 0.7463136052248501.
[I 2024-03-04 10:14:56,561] Trial 17 finished with value: 0.7461350140882557 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.6816993168385257, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.8716506712736086}. Best is trial 14 with value: 0.7463136052248501.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:15:01,969] Trial 18 finished with value: 0.7457165893042078 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.1013680439775186, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 14 with value: 0.7463136052248501.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 10:15:07,438] Trial 19 finished with value: 0.7464394131343187 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 1.1788476820973817, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 19 with value: 0.7464394131343187.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 10:15:11,291] Trial 20 finished with value: 0.746118743202301 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.9738014432288739, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 19 with value: 0.7464394131343187.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 10:15:17,021] Trial 21 finished with value: 0.7464394131343187 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 1.150859537458134, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 19 with value: 0.7464394131343187.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 10:15:22,472] Trial 22 finished with value: 0.7464759254754901 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.9775219539814487, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 22 with value: 0.7464759254754901.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 10:15:25,439] Trial 23 finished with value: 0.7463134756885479 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.3939000346822377, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 22 with value: 0.7464759254754901.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 10:15:30,626] Trial 24 finished with value: 0.7465002670362709 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.9175016557427177, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 24 with value: 0.7465002670362709.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 10:15:32,692] Trial 25 finished with value: 0.7459035101882329 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.10614689151258033, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 24 with value: 0.7465002670362709.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 10:15:37,010] Trial 26 finished with value: 0.7463581882408477 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.6967827742518609, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 24 with value: 0.7465002670362709.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 10:15:39,087] Trial 27 finished with value: 0.7461186136659986 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.18482993950041246, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 24 with value: 0.7465002670362709.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 10:15:42,769] Trial 28 finished with value: 0.744864634676877 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.06315938335109125, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 24 with value: 0.7465002670362709.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 10:15:59,303] Trial 29 finished with value: 0.7459520637734925 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 1.1092749835805915, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 24 with value: 0.7465002670362709.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 10:16:02,723] Trial 30 finished with value: 0.7461957384539056 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.3590201783907888, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 24 with value: 0.7465002670362709.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 10:16:07,775] Trial 31 finished with value: 0.7464394131343187 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 1.0772120368724534, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 24 with value: 0.7465002670362709.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 10:16:12,742] Trial 32 finished with value: 0.7464759254754901 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.9890128426196607, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 24 with value: 0.7465002670362709.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 10:16:16,658] Trial 33 finished with value: 0.7463378172493288 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.48588544234606, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 24 with value: 0.7465002670362709.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 10:16:32,892] Trial 34 finished with value: 0.7398185930821172 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.011067079465579891, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 24 with value: 0.7465002670362709.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 10:16:49,794] Trial 35 finished with value: 0.745939892993102 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.8946775913869303, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 24 with value: 0.7465002670362709.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:16:55,573] Trial 36 finished with value: 0.7460253475284394 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.5267165692079634, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 24 with value: 0.7465002670362709.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 10:16:57,635] Trial 37 finished with value: 0.7248363173653821 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.0018754571965691016, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 24 with value: 0.7465002670362709.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 10:17:14,175] Trial 38 finished with value: 0.7456516352966914 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.2926810806321537, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 24 with value: 0.7465002670362709.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:17:22,888] Trial 39 finished with value: 0.7465165379222256 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.411383421621992, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 39 with value: 0.7465165379222256.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:17:30,205] Trial 40 finished with value: 0.7464109714679736 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.7687354542157299, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 39 with value: 0.7465165379222256.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:17:36,490] Trial 41 finished with value: 0.7465165379222256 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.4085636470319183, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 39 with value: 0.7465165379222256.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:17:42,962] Trial 42 finished with value: 0.7465165379222256 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.4518648562985028, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 39 with value: 0.7465165379222256.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:17:49,078] Trial 43 finished with value: 0.7465165379222256 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.501700920996048, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 39 with value: 0.7465165379222256.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:17:55,405] Trial 44 finished with value: 0.7465165379222256 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.648860578786315, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 39 with value: 0.7465165379222256.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:18:00,308] Trial 45 finished with value: 0.7459442521712706 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.472497110368529, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 39 with value: 0.7465165379222256.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 10:18:13,654] Trial 46 finished with value: 0.7460089471061826 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.9242063849284232, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 39 with value: 0.7465165379222256.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:18:19,890] Trial 47 finished with value: 0.7465165379222256 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.4354057309639225, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 39 with value: 0.7465165379222256.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:18:25,141] Trial 48 finished with value: 0.7460253475284394 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.525557792745633, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 39 with value: 0.7465165379222256.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:18:31,476] Trial 49 finished with value: 0.7465165379222256 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.4966156995806368, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 39 with value: 0.7465165379222256.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:18:33,523] Trial 50 finished with value: 0.7370194431286183 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.01023412171775196, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 39 with value: 0.7465165379222256.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:18:39,704] Trial 51 finished with value: 0.7465165379222256 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.4357127116129802, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 39 with value: 0.7465165379222256.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:18:46,287] Trial 52 finished with value: 0.7465043671418351 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.9429041494912562, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 39 with value: 0.7465165379222256.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:18:51,389] Trial 53 finished with value: 0.7461917678846436 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.6329779306647016, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 39 with value: 0.7465165379222256.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:18:57,424] Trial 54 finished with value: 0.7464637546950996 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.2841047028244443, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 39 with value: 0.7465165379222256.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:19:02,751] Trial 55 finished with value: 0.7460983722107821 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.42054326165369227, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 39 with value: 0.7465165379222256.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:19:08,024] Trial 56 finished with value: 0.7461917678846436 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.6631963967763327, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 39 with value: 0.7465165379222256.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:19:14,004] Trial 57 finished with value: 0.7465287087026159 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.384305625489586, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:19:20,339] Trial 58 finished with value: 0.7465043671418351 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.9491895227976697, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:19:25,386] Trial 59 finished with value: 0.7465002670362709 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.8070444391070428, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 10:19:38,287] Trial 60 finished with value: 0.7456029521751296 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.2538695724792383, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:19:44,154] Trial 61 finished with value: 0.7465287087026159 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.3836364636099463, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:19:49,999] Trial 62 finished with value: 0.7464759254754899 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.257983003894696, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:19:56,387] Trial 63 finished with value: 0.7465287087026159 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.5433145584801196, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:20:01,575] Trial 64 finished with value: 0.7464759254754899 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.8598218553956851, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:20:05,620] Trial 65 finished with value: 0.7456878885652584 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.03283028582487242, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:20:10,237] Trial 66 finished with value: 0.7458671273833638 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.2808422482380328, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:20:15,472] Trial 67 finished with value: 0.7461795971042532 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.6048058551563624, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:20:21,204] Trial 68 finished with value: 0.7464515839147091 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.0441615025160222, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:20:22,886] Trial 69 finished with value: 0.7375849085116637 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.0010275516905737265, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:20:27,861] Trial 70 finished with value: 0.7460618598696108 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.4507503248048579, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:20:34,258] Trial 71 finished with value: 0.7465165379222256 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.5185376385080642, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:20:40,800] Trial 72 finished with value: 0.7465165379222256 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.6835710832693884, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:20:46,571] Trial 73 finished with value: 0.7464637546950996 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.132232931076404, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
[I 2024-03-04 10:20:55,153] Trial 74 finished with value: 0.7463988006875834 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.7627511139460588, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.17083700098286245}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:21:01,483] Trial 75 finished with value: 0.7465165379222256 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.6619987156442988, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:21:06,760] Trial 76 finished with value: 0.7464880962558805 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.8908629411586926, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:21:11,171] Trial 77 finished with value: 0.7458549566029733 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.982446687377067, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 10:21:29,855] Trial 78 finished with value: 0.7459196515378853 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.217677674444156, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:21:38,426] Trial 79 finished with value: 0.7460375183088299 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.5652479225660689, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 10:21:56,687] Trial 80 finished with value: 0.7463744591268023 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.0262652638456553, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.5591974955214346}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:22:03,457] Trial 81 finished with value: 0.7465165379222256 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.4070387542619545, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:22:09,086] Trial 82 finished with value: 0.7465165379222256 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.5983750308364066, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 57 with value: 0.7465287087026159.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:22:21,339] Trial 83 finished with value: 0.7465408794830064 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.3338418110720158, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 83 with value: 0.7465408794830064.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:22:33,734] Trial 84 finished with value: 0.7464109714679736 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.7586943656191685, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 83 with value: 0.7465408794830064.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:22:37,901] Trial 85 finished with value: 0.7421351414618647 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.003288213824534991, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 83 with value: 0.7465408794830064.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:22:52,254] Trial 86 finished with value: 0.7464515839147091 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.1109668033860922, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 83 with value: 0.7465408794830064.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:23:08,119] Trial 87 finished with value: 0.7465287087026159 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.5921491152529008, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 83 with value: 0.7465408794830064.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:23:22,111] Trial 88 finished with value: 0.7464759254754899 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.9333216364455906, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 83 with value: 0.7465408794830064.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:23:38,441] Trial 89 finished with value: 0.7465287087026159 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.341911917152724, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 83 with value: 0.7465408794830064.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 10:24:13,888] Trial 90 finished with value: 0.7459196515378853 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.2166665980675329, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 83 with value: 0.7465408794830064.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:24:30,221] Trial 91 finished with value: 0.7465043671418351 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.711909251196481, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 83 with value: 0.7465408794830064.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:24:46,221] Trial 92 finished with value: 0.7465165379222256 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.415696747625559, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 83 with value: 0.7465408794830064.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:24:59,185] Trial 93 finished with value: 0.7464231422483641 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.7140469516479744, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 83 with value: 0.7465408794830064.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:25:15,961] Trial 94 finished with value: 0.7465043671418351 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.9482493173211934, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 83 with value: 0.7465408794830064.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:25:28,760] Trial 95 finished with value: 0.7464759254754899 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.9240921098552711, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 83 with value: 0.7465408794830064.
[I 2024-03-04 10:25:47,174] Trial 96 finished with value: 0.7458143441562378 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.3067877480067904, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.39265876926642496}. Best is trial 83 with value: 0.7465408794830064.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:26:01,877] Trial 97 finished with value: 0.7456556058659535 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.06754956916063948, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 83 with value: 0.7465408794830064.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:26:17,428] Trial 98 finished with value: 0.7464515839147091 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.0669177160097378, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 83 with value: 0.7465408794830064.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 10:26:24,686] Trial 99 finished with value: 0.7442222585224242 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.014022524480614312, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 83 with value: 0.7465408794830064.
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><strong>Saving the results:</strong></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Saving the results as pickle files</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">r = 2</span>

<span class="sd">with open(f&#39;results/params_round_{r}&#39;, &#39;wb&#39;) as file:</span>
<span class="sd">    pickle.dump(best_params, file)</span>

<span class="sd">with open(f&#39;results/inner_scores_round_{r}&#39;, &#39;wb&#39;) as file:</span>
<span class="sd">    pickle.dump(inner_score, file)</span>

<span class="sd">with open(f&#39;results/results_round_{r}&#39;, &#39;wb&#39;) as file:</span>
<span class="sd">    pickle.dump(inner_results, file)</span>
<span class="sd">&#39;&#39;&#39;</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Opening the results</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_params</span><span class="p">,</span> <span class="n">inner_score</span><span class="p">,</span> <span class="n">inner_results</span> <span class="o">=</span> <span class="p">{},</span> <span class="p">{},</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]:</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;results/params_round_</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
                <span class="n">best_params</span><span class="p">[</span><span class="n">r</span><span class="p">]</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;results/inner_scores_round_</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
                <span class="n">inner_score</span><span class="p">[</span><span class="n">r</span><span class="p">]</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;results/results_round_</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
                <span class="n">inner_results</span><span class="p">[</span><span class="n">r</span><span class="p">]</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>HPO practical process:</strong></p>
<ol class="arabic simple">
<li><p>We carry out preliminary test/trials with certain models</p></li>
<li><p>Based on the results of these trials we refine the common param grid and the specific grid of some of the models</p></li>
<li><p>We carry out another trial with the refined grids</p></li>
<li><p>Repeat the process</p></li>
</ol>
<p>We usually trust in the XGB results, since it is usually the best model for tabular data (or one of the best), serving as a benchmark for model performance..</p>
<p>The idea of this iterative process is to make the search space narrower (refining it), searching each time in better spaces and allowing more exhaustive search in the model hyperparameter, specially discarding preprocessing options, making  the search space smaller and more addressable computationally.</p>
<p>We have done 5 rounds.</p>
<ul class="simple">
<li><p><strong>Round 1</strong>:</p>
<ul>
<li><p>We have test how the models perform with a n initial set of grids for both the transformers and the models (estimators)</p></li>
</ul>
</li>
<li><p><strong>Round 2</strong>:</p>
<ul>
<li><p>Based on the round 1 XGB results we have refine the grids for the transformers.</p></li>
<li><p>And based on the results of each of the models we have refined the hyperparameter grids for each of them.</p></li>
</ul>
</li>
<li><p><strong>Round 3</strong>:</p>
<ul>
<li><p>We apply over and under sampling techniques which are specially designed for improving the performance in  imbalanced classification problems. We use the same grids than in the round 2.</p></li>
</ul>
</li>
<li><p><strong>Round 4</strong>:</p>
<ul>
<li><p>We apply sequential feature selection algorithms, using the pipelines of the round 3 and the grids of round 2 but modifying some parameters to deal with these feature selection algorithms better (specially the encoder).</p></li>
</ul>
</li>
<li><p><strong>Round 5</strong>:</p>
<ul>
<li><p>We try stacking algorithm with several base models and logistic regression as meta model. We use the pipelines and best parameters of round 3 for the base models, and the best params of round 2 for the meta model.</p></li>
</ul>
</li>
</ul>
<p>The following results have been used after round 1 to refined the preprocessing grids in round 2.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inner_results</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;XGB&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>preprocessing__quant__scaler__apply</th>
      <th>preprocessing__cat__encoder__method</th>
      <th>preprocessing__cat__imputer__apply</th>
      <th>preprocessing__quant__imputer__apply</th>
      <th>features_selector__apply</th>
      <th>preprocessing__quant__scaler__method</th>
      <th>preprocessing__quant__imputer__method</th>
      <th>preprocessing__cat__imputer__method</th>
      <th>preprocessing__quant__imputer__n_neighbors</th>
      <th>preprocessing__cat__imputer__n_neighbors</th>
      <th>XGB__max_depth</th>
      <th>XGB__reg_lambda</th>
      <th>XGB__n_estimators</th>
      <th>XGB__eta</th>
      <th>XGB__alpha</th>
      <th>features_selector__method</th>
      <th>preprocessing__cat__imputer__n_nearest_features</th>
      <th>preprocessing__quant__imputer__n_nearest_features</th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>63</th>
      <td>False</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>NaN</td>
      <td>iterative_median</td>
      <td>simple_most_frequent</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>10</td>
      <td>0.00</td>
      <td>130</td>
      <td>0.28</td>
      <td>0.32</td>
      <td>Fdr_f_class</td>
      <td>NaN</td>
      <td>6.0</td>
      <td>0.584433</td>
    </tr>
    <tr>
      <th>64</th>
      <td>False</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>NaN</td>
      <td>iterative_median</td>
      <td>simple_most_frequent</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>10</td>
      <td>0.00</td>
      <td>130</td>
      <td>0.30</td>
      <td>0.29</td>
      <td>Fdr_f_class</td>
      <td>NaN</td>
      <td>6.0</td>
      <td>0.583897</td>
    </tr>
    <tr>
      <th>74</th>
      <td>True</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>min-max</td>
      <td>iterative_median</td>
      <td>simple_most_frequent</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>10</td>
      <td>0.05</td>
      <td>130</td>
      <td>0.30</td>
      <td>0.27</td>
      <td>Fdr_f_class</td>
      <td>NaN</td>
      <td>7.0</td>
      <td>0.582789</td>
    </tr>
    <tr>
      <th>65</th>
      <td>False</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>NaN</td>
      <td>iterative_median</td>
      <td>simple_most_frequent</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>10</td>
      <td>0.05</td>
      <td>130</td>
      <td>0.30</td>
      <td>0.27</td>
      <td>Fdr_f_class</td>
      <td>NaN</td>
      <td>6.0</td>
      <td>0.582789</td>
    </tr>
    <tr>
      <th>66</th>
      <td>False</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>NaN</td>
      <td>iterative_median</td>
      <td>simple_most_frequent</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>10</td>
      <td>0.05</td>
      <td>130</td>
      <td>0.30</td>
      <td>0.27</td>
      <td>Fdr_f_class</td>
      <td>NaN</td>
      <td>7.0</td>
      <td>0.582789</td>
    </tr>
    <tr>
      <th>71</th>
      <td>True</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>min-max</td>
      <td>iterative_median</td>
      <td>simple_most_frequent</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>10</td>
      <td>0.10</td>
      <td>130</td>
      <td>0.30</td>
      <td>0.27</td>
      <td>Fdr_f_class</td>
      <td>NaN</td>
      <td>7.0</td>
      <td>0.582736</td>
    </tr>
    <tr>
      <th>70</th>
      <td>True</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>min-max</td>
      <td>iterative_median</td>
      <td>simple_most_frequent</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>10</td>
      <td>0.10</td>
      <td>130</td>
      <td>0.30</td>
      <td>0.27</td>
      <td>Fdr_f_class</td>
      <td>NaN</td>
      <td>7.0</td>
      <td>0.582736</td>
    </tr>
    <tr>
      <th>67</th>
      <td>True</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>min-max</td>
      <td>iterative_median</td>
      <td>simple_most_frequent</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>10</td>
      <td>0.05</td>
      <td>130</td>
      <td>0.30</td>
      <td>0.26</td>
      <td>Fdr_f_class</td>
      <td>NaN</td>
      <td>7.0</td>
      <td>0.582679</td>
    </tr>
    <tr>
      <th>55</th>
      <td>False</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>NaN</td>
      <td>iterative_median</td>
      <td>simple_most_frequent</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>10</td>
      <td>0.05</td>
      <td>130</td>
      <td>0.26</td>
      <td>0.30</td>
      <td>Fdr_f_class</td>
      <td>NaN</td>
      <td>6.0</td>
      <td>0.582082</td>
    </tr>
    <tr>
      <th>53</th>
      <td>False</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>NaN</td>
      <td>iterative_median</td>
      <td>simple_most_frequent</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>30</td>
      <td>0.20</td>
      <td>130</td>
      <td>0.28</td>
      <td>0.29</td>
      <td>Fdr_f_class</td>
      <td>NaN</td>
      <td>6.0</td>
      <td>0.581977</td>
    </tr>
    <tr>
      <th>32</th>
      <td>False</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>NaN</td>
      <td>iterative_median</td>
      <td>simple_most_frequent</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>30</td>
      <td>0.25</td>
      <td>130</td>
      <td>0.24</td>
      <td>0.24</td>
      <td>Fpr_f_class</td>
      <td>NaN</td>
      <td>7.0</td>
      <td>0.581737</td>
    </tr>
    <tr>
      <th>60</th>
      <td>False</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>NaN</td>
      <td>iterative_median</td>
      <td>simple_most_frequent</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>10</td>
      <td>0.00</td>
      <td>130</td>
      <td>0.28</td>
      <td>0.29</td>
      <td>Fdr_f_class</td>
      <td>NaN</td>
      <td>6.0</td>
      <td>0.581631</td>
    </tr>
    <tr>
      <th>61</th>
      <td>False</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>NaN</td>
      <td>iterative_median</td>
      <td>simple_most_frequent</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>10</td>
      <td>0.00</td>
      <td>130</td>
      <td>0.28</td>
      <td>0.29</td>
      <td>Fdr_f_class</td>
      <td>NaN</td>
      <td>6.0</td>
      <td>0.581631</td>
    </tr>
    <tr>
      <th>56</th>
      <td>False</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>NaN</td>
      <td>iterative_median</td>
      <td>simple_most_frequent</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>10</td>
      <td>0.00</td>
      <td>130</td>
      <td>0.26</td>
      <td>0.30</td>
      <td>Fdr_f_class</td>
      <td>NaN</td>
      <td>6.0</td>
      <td>0.581606</td>
    </tr>
    <tr>
      <th>62</th>
      <td>False</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>NaN</td>
      <td>iterative_median</td>
      <td>simple_most_frequent</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>10</td>
      <td>0.00</td>
      <td>130</td>
      <td>0.28</td>
      <td>0.33</td>
      <td>Fdr_f_class</td>
      <td>NaN</td>
      <td>6.0</td>
      <td>0.581567</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inner_results</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="s1">&#39;XGB&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>preprocessing__quant__scaler__apply</th>
      <th>preprocessing__cat__encoder__method</th>
      <th>preprocessing__cat__imputer__apply</th>
      <th>preprocessing__quant__imputer__apply</th>
      <th>features_selector__apply</th>
      <th>preprocessing__quant__scaler__method</th>
      <th>preprocessing__quant__imputer__method</th>
      <th>preprocessing__cat__imputer__method</th>
      <th>preprocessing__quant__imputer__n_neighbors</th>
      <th>preprocessing__cat__imputer__n_neighbors</th>
      <th>XGB__max_depth</th>
      <th>XGB__reg_lambda</th>
      <th>XGB__n_estimators</th>
      <th>XGB__eta</th>
      <th>XGB__alpha</th>
      <th>features_selector__method</th>
      <th>preprocessing__cat__imputer__n_nearest_features</th>
      <th>preprocessing__quant__imputer__n_nearest_features</th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>6</th>
      <td>True</td>
      <td>ordinal</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>standard</td>
      <td>simple_median</td>
      <td>iterative_most_frequent</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>10</td>
      <td>0.20</td>
      <td>70</td>
      <td>0.26</td>
      <td>0.22</td>
      <td>KBest_mutual_class</td>
      <td>5.0</td>
      <td>NaN</td>
      <td>0.571190</td>
    </tr>
    <tr>
      <th>40</th>
      <td>False</td>
      <td>ordinal</td>
      <td>True</td>
      <td>True</td>
      <td>False</td>
      <td>NaN</td>
      <td>simple_median</td>
      <td>knn</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>10</td>
      <td>0.10</td>
      <td>50</td>
      <td>0.18</td>
      <td>0.60</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.570935</td>
    </tr>
    <tr>
      <th>14</th>
      <td>False</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>NaN</td>
      <td>iterative_median</td>
      <td>simple_most_frequent</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>40</td>
      <td>0.60</td>
      <td>130</td>
      <td>0.20</td>
      <td>0.30</td>
      <td>Percentile_mutual_class</td>
      <td>NaN</td>
      <td>5.0</td>
      <td>0.566781</td>
    </tr>
    <tr>
      <th>19</th>
      <td>False</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>NaN</td>
      <td>knn</td>
      <td>knn</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>40</td>
      <td>0.05</td>
      <td>130</td>
      <td>0.16</td>
      <td>0.69</td>
      <td>Percentile_mutual_class</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.565779</td>
    </tr>
    <tr>
      <th>52</th>
      <td>False</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>NaN</td>
      <td>iterative_median</td>
      <td>simple_most_frequent</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>30</td>
      <td>0.40</td>
      <td>130</td>
      <td>0.30</td>
      <td>0.23</td>
      <td>KBest_mutual_class</td>
      <td>NaN</td>
      <td>7.0</td>
      <td>0.555518</td>
    </tr>
    <tr>
      <th>26</th>
      <td>False</td>
      <td>ordinal</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>NaN</td>
      <td>iterative_median</td>
      <td>simple_most_frequent</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>20</td>
      <td>0.10</td>
      <td>100</td>
      <td>0.22</td>
      <td>0.39</td>
      <td>Percentile_mutual_class</td>
      <td>NaN</td>
      <td>4.0</td>
      <td>0.544377</td>
    </tr>
    <tr>
      <th>1</th>
      <td>True</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>standard</td>
      <td>simple_median</td>
      <td>knn</td>
      <td>NaN</td>
      <td>4.0</td>
      <td>20</td>
      <td>0.10</td>
      <td>100</td>
      <td>0.04</td>
      <td>0.75</td>
      <td>KBest_mutual_class</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.536366</td>
    </tr>
    <tr>
      <th>38</th>
      <td>False</td>
      <td>ordinal</td>
      <td>True</td>
      <td>True</td>
      <td>False</td>
      <td>NaN</td>
      <td>simple_median</td>
      <td>knn</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>100</td>
      <td>0.20</td>
      <td>70</td>
      <td>0.02</td>
      <td>0.37</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.532262</td>
    </tr>
    <tr>
      <th>0</th>
      <td>True</td>
      <td>one-hot</td>
      <td>True</td>
      <td>True</td>
      <td>False</td>
      <td>min-max</td>
      <td>knn</td>
      <td>knn</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>40</td>
      <td>1.00</td>
      <td>70</td>
      <td>0.02</td>
      <td>0.86</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.521682</td>
    </tr>
    <tr>
      <th>2</th>
      <td>False</td>
      <td>ordinal</td>
      <td>True</td>
      <td>True</td>
      <td>False</td>
      <td>NaN</td>
      <td>simple_median</td>
      <td>knn</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>10</td>
      <td>0.15</td>
      <td>50</td>
      <td>0.00</td>
      <td>0.36</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.500000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="id5">
<h4><strong>Selecting the best model</strong><a class="headerlink" href="#id5" title="Link to this heading">#</a></h4>
<p>In this section we select the best model based on the above inner score results.</p>
<p>This code selects the best model based on the highest score from the second round of optimization, identifying both the model’s name and its score. It then combines and sorts all models by their scores for a clear comparison.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inner_score_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">inner_score</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>
<span class="n">model_names</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">inner_score</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="n">model_names</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">inner_score_values</span><span class="p">)]</span>
<span class="n">score_best_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">inner_score_values</span><span class="p">)</span>

<span class="n">combined_models_score</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">model_names</span><span class="p">,</span> <span class="n">inner_score_values</span><span class="p">))</span>
<span class="n">sorted_combined_models_score</span><span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">combined_models_score</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Sort from greater to lower</span>
<span class="n">sorted_models</span><span class="p">,</span> <span class="n">sorted_scores</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">sorted_combined_models_score</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">sorted_models</span><span class="p">),</span> <span class="n">x</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">sorted_scores</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">best_model</span><span class="p">],</span> <span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">score_best_model</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Models&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Balanced Accuracy&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">inner_score_values</span><span class="p">),</span> <span class="mi">7</span><span class="p">),</span><span class="mi">3</span><span class="p">))</span> 
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>    
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Model Selection - 3-Fold CV&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/82c24cbeca137383ddc3ba2a5d6242f4a8247b6b39fb46032f8e4bd27698df61.png" src="_images/82c24cbeca137383ddc3ba2a5d6242f4a8247b6b39fb46032f8e4bd27698df61.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_model</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;logistic_reg&#39;
</pre></div>
</div>
</div>
</div>
<p>The best model is logistic regression, but the performance of linear SVM is practically the same. The rest of the models are far away from them. This is probably due to we are facing an imbalanced classification problem and the models al re failing predicting the minority class.</p>
<p>We have to highlight that these two models are assigning a weight to each observation defined as the relative frequency of the class that they belong to. Both estimators have the parameter <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> that controls this (we have set it as ‘balanced’).</p>
<p>Is important to notice that we are using the balanced accuracy as metric and not the normal accuracy, since the first one is the suitable one for this type of problems, and the second one is completely biased by the results oin the majority class, specially in binary classification problems like this.</p>
<p>We can check which are the parameters of the best alternative (pipeline).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_params</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">best_model</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;preprocessing__quant__scaler__apply&#39;: True,
 &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;,
 &#39;preprocessing__cat__imputer__apply&#39;: True,
 &#39;preprocessing__quant__imputer__apply&#39;: True,
 &#39;features_selector__apply&#39;: True,
 &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;,
 &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;,
 &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;,
 &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;,
 &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6,
 &#39;logistic_reg__penalty&#39;: &#39;l2&#39;,
 &#39;logistic_reg__C&#39;: 1.3338418110720158,
 &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="id6">
<h3><strong>Applying outer evaluation</strong><a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<section id="estimation-of-future-performance">
<h4><strong>Estimation of future performance</strong><a class="headerlink" href="#estimation-of-future-performance" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Estimation performance of the best model (Logistic Regression)</strong></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipelines</span><span class="p">[</span><span class="n">best_model</span><span class="p">]</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">best_params</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">best_model</span><span class="p">])</span>
<span class="n">pipelines</span><span class="p">[</span><span class="n">best_model</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_test_hat_LR</span> <span class="o">=</span> <span class="n">pipelines</span><span class="p">[</span><span class="n">best_model</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">estimation_future_performance_LR</span> <span class="o">=</span> <span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_test_hat_LR</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="p">)</span>
<span class="n">estimation_future_performance_LR</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7529345548133539
</pre></div>
</div>
</div>
</div>
<p>We can ckeck which features have been selected by the best pipeline as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipelines</span><span class="p">[</span><span class="n">best_model</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-5" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,
                 ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                                  Pipeline(steps=[(&#x27;imputer&#x27;,
                                                                   imputer()),
                                                                  (&#x27;scaler&#x27;,
                                                                   scaler())]),
                                                  [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;,
                                                   &#x27;PhysHlth&#x27;]),
                                                 (&#x27;cat&#x27;,
                                                  Pipeline(steps=[(&#x27;encoder&#x27;,
                                                                   encoder()),
                                                                  (&#x27;imputer&#x27;,
                                                                   imputer())]),
                                                  [&#x27;HighBP&#x27;, &#x27;HighChol&#x27;,
                                                   &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;,
                                                   &#x27;Stroke&#x27;,
                                                   &#x27;HeartDiseaseorAttack&#x27;,
                                                   &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;,
                                                   &#x27;Veggies&#x27;,
                                                   &#x27;HvyAlcoholConsump&#x27;,
                                                   &#x27;AnyHealthcare&#x27;,
                                                   &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;,
                                                   &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;,
                                                   &#x27;Education&#x27;, &#x27;Income&#x27;])])),
                (&#x27;features_selector&#x27;, features_selector(cv=2, k=10, n_jobs=-1)),
                (&#x27;logistic_reg&#x27;,
                 LogisticRegression(max_iter=250, random_state=123,
                                    solver=&#x27;saga&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-26" type="checkbox" ><label for="sk-estimator-id-26" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,
                 ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                                  Pipeline(steps=[(&#x27;imputer&#x27;,
                                                                   imputer()),
                                                                  (&#x27;scaler&#x27;,
                                                                   scaler())]),
                                                  [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;,
                                                   &#x27;PhysHlth&#x27;]),
                                                 (&#x27;cat&#x27;,
                                                  Pipeline(steps=[(&#x27;encoder&#x27;,
                                                                   encoder()),
                                                                  (&#x27;imputer&#x27;,
                                                                   imputer())]),
                                                  [&#x27;HighBP&#x27;, &#x27;HighChol&#x27;,
                                                   &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;,
                                                   &#x27;Stroke&#x27;,
                                                   &#x27;HeartDiseaseorAttack&#x27;,
                                                   &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;,
                                                   &#x27;Veggies&#x27;,
                                                   &#x27;HvyAlcoholConsump&#x27;,
                                                   &#x27;AnyHealthcare&#x27;,
                                                   &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;,
                                                   &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;,
                                                   &#x27;Education&#x27;, &#x27;Income&#x27;])])),
                (&#x27;features_selector&#x27;, features_selector(cv=2, k=10, n_jobs=-1)),
                (&#x27;logistic_reg&#x27;,
                 LogisticRegression(max_iter=250, random_state=123,
                                    solver=&#x27;saga&#x27;))])</pre></div></div></div><div class="sk-serial"><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-27" type="checkbox" ><label for="sk-estimator-id-27" class="sk-toggleable__label sk-toggleable__label-arrow">preprocessing: ColumnTransformer</label><div class="sk-toggleable__content"><pre>ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                 Pipeline(steps=[(&#x27;imputer&#x27;, imputer()),
                                                 (&#x27;scaler&#x27;, scaler())]),
                                 [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;, &#x27;PhysHlth&#x27;]),
                                (&#x27;cat&#x27;,
                                 Pipeline(steps=[(&#x27;encoder&#x27;, encoder()),
                                                 (&#x27;imputer&#x27;, imputer())]),
                                 [&#x27;HighBP&#x27;, &#x27;HighChol&#x27;, &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;,
                                  &#x27;Stroke&#x27;, &#x27;HeartDiseaseorAttack&#x27;,
                                  &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;, &#x27;Veggies&#x27;,
                                  &#x27;HvyAlcoholConsump&#x27;, &#x27;AnyHealthcare&#x27;,
                                  &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;, &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;,
                                  &#x27;Age&#x27;, &#x27;Education&#x27;, &#x27;Income&#x27;])])</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-28" type="checkbox" ><label for="sk-estimator-id-28" class="sk-toggleable__label sk-toggleable__label-arrow">quant</label><div class="sk-toggleable__content"><pre>[&#x27;BMI&#x27;, &#x27;MentHlth&#x27;, &#x27;PhysHlth&#x27;]</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-29" type="checkbox" ><label for="sk-estimator-id-29" class="sk-toggleable__label sk-toggleable__label-arrow">imputer</label><div class="sk-toggleable__content"><pre>imputer()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-30" type="checkbox" ><label for="sk-estimator-id-30" class="sk-toggleable__label sk-toggleable__label-arrow">scaler</label><div class="sk-toggleable__content"><pre>scaler()</pre></div></div></div></div></div></div></div></div><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-31" type="checkbox" ><label for="sk-estimator-id-31" class="sk-toggleable__label sk-toggleable__label-arrow">cat</label><div class="sk-toggleable__content"><pre>[&#x27;HighBP&#x27;, &#x27;HighChol&#x27;, &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;, &#x27;Stroke&#x27;, &#x27;HeartDiseaseorAttack&#x27;, &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;, &#x27;Veggies&#x27;, &#x27;HvyAlcoholConsump&#x27;, &#x27;AnyHealthcare&#x27;, &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;, &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;, &#x27;Education&#x27;, &#x27;Income&#x27;]</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-32" type="checkbox" ><label for="sk-estimator-id-32" class="sk-toggleable__label sk-toggleable__label-arrow">encoder</label><div class="sk-toggleable__content"><pre>encoder()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-33" type="checkbox" ><label for="sk-estimator-id-33" class="sk-toggleable__label sk-toggleable__label-arrow">imputer</label><div class="sk-toggleable__content"><pre>imputer()</pre></div></div></div></div></div></div></div></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-34" type="checkbox" ><label for="sk-estimator-id-34" class="sk-toggleable__label sk-toggleable__label-arrow">features_selector</label><div class="sk-toggleable__content"><pre>features_selector(cv=2, k=10, n_jobs=-1)</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-35" type="checkbox" ><label for="sk-estimator-id-35" class="sk-toggleable__label sk-toggleable__label-arrow">LogisticRegression</label><div class="sk-toggleable__content"><pre>LogisticRegression(max_iter=250, random_state=123, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">selected_features</span> <span class="o">=</span> <span class="n">pipelines</span><span class="p">[</span><span class="n">best_model</span><span class="p">]</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">features_selector_</span><span class="o">.</span><span class="n">get_support</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">selected_features</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 11, 12, 13, 14, 15, 16, 17,
       18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36,
       37, 38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56,
       58, 59, 60, 61, 62, 63, 64, 66, 67], dtype=int64)
</pre></div>
</div>
</div>
</div>
<p>All the features have been selected, so the feature selector applied in this case has selected nothing.</p>
<ul class="simple">
<li><p><strong>Estimation performance of a bad model</strong></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;RF&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">best_params</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="s1">&#39;RF&#39;</span><span class="p">])</span>
<span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;RF&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_test_hat_RF</span> <span class="o">=</span> <span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;RF&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">estimation_future_performance_RF</span> <span class="o">=</span> <span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_test_hat_RF</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="p">)</span>
<span class="n">estimation_future_performance_RF</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5456080705486692
</pre></div>
</div>
</div>
</div>
<p>As you can see the future performance of a bad model (any different to logistic regression or svm) is quite poor..</p>
</section>
<section id="confusion-matrix">
<h4><strong>Confusion matrix</strong><a class="headerlink" href="#confusion-matrix" title="Link to this heading">#</a></h4>
<p><strong>Confusion matrix of the best model</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_test_hat_LR</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="s1">&#39;true&#39;</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="n">best_model</span><span class="p">]</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="c1"># normalize=&#39;true&#39; to normalize over the rows (true classes)</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="n">best_model</span><span class="p">]</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">,</span> <span class="n">values_format</span><span class="o">=</span><span class="s1">&#39;.3f&#39;</span><span class="p">,</span> <span class="n">text_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;fontsize&#39;</span><span class="p">:</span> <span class="mi">13</span><span class="p">})</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Confusion Matrix over true classes</span><span class="se">\n</span><span class="s1"> Logistic Regression&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">axs</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted class&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">axs</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;True class&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/32a0507034e540cee16c53a8d26d85c179ed662de30e2bb5dbf9e069ed22cbf1.png" src="_images/32a0507034e540cee16c53a8d26d85c179ed662de30e2bb5dbf9e069ed22cbf1.png" />
</div>
</div>
<p>From the confusion matrix we can compute several metrics directly:</p>
<ul class="simple">
<li><p>Balanced accuracy:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Balanced accuracy</span>
<span class="p">(</span><span class="mf">0.718</span> <span class="o">+</span> <span class="mf">0.788</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span> <span class="c1"># = 0.719*0.5 + 0.777*0.5</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.753
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_test_hat_LR</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7529345548133539
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Not balanced accuracy:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">values</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">rel_freq</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span>
<span class="n">rel_freq</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.8637023, 0.1362977])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Accuracy (not balanced)</span>
<span class="mf">0.718</span><span class="o">*</span><span class="mf">0.86</span> <span class="o">+</span> <span class="mf">0.788</span><span class="o">*</span><span class="mf">0.14</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7277999999999999
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_test_hat_LR</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7277199621570483
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Specificity: 0.719</p></li>
</ul>
<ul class="simple">
<li><p>Sensitivity (recall): 0.788</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_test_hat_LR</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7875983341045812
</pre></div>
</div>
</div>
</div>
<p>Now we compute the confusion matrix over the predicted classes to get more metrics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_test_hat_LR</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="s1">&#39;pred&#39;</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="n">best_model</span><span class="p">]</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="c1"># normalize=&#39;pred&#39; to normalize over the columns (predicted classes)</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="n">best_model</span><span class="p">]</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">,</span> <span class="n">values_format</span><span class="o">=</span><span class="s1">&#39;.3f&#39;</span><span class="p">,</span> <span class="n">text_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;fontsize&#39;</span><span class="p">:</span> <span class="mi">13</span><span class="p">})</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Confusion Matrix over pred. classes</span><span class="se">\n</span><span class="s1"> Logistic Regression&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">axs</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted class&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">axs</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;True class&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/43eb6744ccacc297b6cbee18015b3b54bec643cfacb9b64b40de3ad61c000b27.png" src="_images/43eb6744ccacc297b6cbee18015b3b54bec643cfacb9b64b40de3ad61c000b27.png" />
</div>
</div>
<ul class="simple">
<li><p>Precision: 0.306</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_test_hat_LR</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.3061151079136691
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Negative predictive value: 0.955</p></li>
</ul>
<ul class="simple">
<li><p>F1 score:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_test_hat_LR</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.4408755342572206
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Harmonic mean between sensitivity (recall) and precision</span>
<span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span> <span class="mf">0.788</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="mf">0.306</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.4408190127970749
</pre></div>
</div>
</div>
</div>
<p><strong>Confusion matrix of a bad model</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_test_hat_RF</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="s1">&#39;true&#39;</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;RF&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="c1"># normalize=&#39;true&#39; to normalize over the rows (true classes)</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;RF&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">,</span> <span class="n">values_format</span><span class="o">=</span><span class="s1">&#39;.3f&#39;</span><span class="p">,</span> <span class="n">text_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;fontsize&#39;</span><span class="p">:</span> <span class="mi">13</span><span class="p">})</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Confusion Matrix over true classes</span><span class="se">\n</span><span class="s1"> Random Forest&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">axs</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted class&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">axs</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;True class&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c926721037087fc386332577a43a49af3edc71f5120c446e0aadf4eb06ed1a5e.png" src="_images/c926721037087fc386332577a43a49af3edc71f5120c446e0aadf4eb06ed1a5e.png" />
</div>
</div>
<p>From the confusion matrix we can compute several metrics directly:</p>
<ul class="simple">
<li><p>Balanced accuracy:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Balanced accuracy</span>
<span class="p">(</span><span class="mf">0.986</span> <span class="o">+</span> <span class="mf">0.126</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span> <span class="c1"># = 0.719*0.5 + 0.777*0.5</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.556
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Not balanced accuracy:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Accuracy (not balanced)</span>
<span class="mf">0.986</span><span class="o">*</span><span class="mf">0.86</span> <span class="o">+</span> <span class="mf">0.126</span><span class="o">*</span><span class="mf">0.14</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8655999999999999
</pre></div>
</div>
</div>
</div>
<p>The difference between the balanced and not balanced accuracy lies in how they handle class imbalance. Balanced accuracy is useful when there is class imbalance in the dataset, as it provides an equitable measure of model performance across all classes. Unbalanced accuracy can be misleading in the presence of imbalanced classes, as it may be dominated by the majority class.</p>
<p>(Unbalanced) Accuracy is completely biased in imbalanced classification problems.</p>
<p>Moreover, when the problem is not imbalanced both metrics are the same, so, in any scenario, the best option is using balanced accuracy.</p>
<ul class="simple">
<li><p>Specificity: 0.989</p></li>
</ul>
<ul class="simple">
<li><p>Sensitivity (recall): 0.102</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_test_hat_RF</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.1018047200370199
</pre></div>
</div>
</div>
</div>
<p>Now we compute the confusion matrix over the predicted classes to get more metrics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_test_hat_RF</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="s1">&#39;pred&#39;</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;RF&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="c1"># normalize=&#39;pred&#39; to normalize over the columns (predicted classes)</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;RF&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">,</span> <span class="n">values_format</span><span class="o">=</span><span class="s1">&#39;.3f&#39;</span><span class="p">,</span> <span class="n">text_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;fontsize&#39;</span><span class="p">:</span> <span class="mi">13</span><span class="p">})</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Confusion Matrix over pred. classes</span><span class="se">\n</span><span class="s1"> Logistic Regression&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">axs</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted class&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">axs</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;True class&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e0b1d909b4267bfd394671055e0d42925020dfac56f7c49656a70dd26b24a2e3.png" src="_images/e0b1d909b4267bfd394671055e0d42925020dfac56f7c49656a70dd26b24a2e3.png" />
</div>
</div>
<ul class="simple">
<li><p>Precision: 0.603</p></li>
</ul>
<ul class="simple">
<li><p>Negative predictive value: 0.875</p></li>
</ul>
<ul class="simple">
<li><p>F1 score:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_test_hat_RF</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.17418844022169439
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Harmonic mean between sensitivity (recall) and precision</span>
<span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span> <span class="mf">0.102</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="mf">0.603</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.1744851063829787
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="improving-the-performance-using-imblearn">
<h3><strong>Improving the performance using <code class="docutils literal notranslate"><span class="pre">imblearn</span></code></strong><a class="headerlink" href="#improving-the-performance-using-imblearn" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">imblearn</span></code> library relying on scikit-learn and provides tools when dealing with classification with imbalanced classes.</p>
<p>The main techniques that we are going to use to improve the performance of the algorithms in imbalanced classification problems are under and over sampling. These techniques will help us to balance the class distribution and potentially improve model accuracy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;images/over_under_sampling.png&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">resize</span><span class="p">((</span><span class="mi">750</span><span class="p">,</span> <span class="mi">300</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e655bb7f01000cb5c597cb8b08083929d85996ed83d8a50c65a3e654b233e4d2.png" src="_images/e655bb7f01000cb5c597cb8b08083929d85996ed83d8a50c65a3e654b233e4d2.png" />
</div>
</div>
<p>In the below part we are going to follow the same steps as we follow in the above part, so that we are not going to repeat the comments.</p>
<section id="pipelines">
<h4><strong>Pipelines</strong><a class="headerlink" href="#pipelines" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">quant_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;imputer&#39;</span><span class="p">,</span> <span class="n">imputer</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">scaler</span><span class="p">())</span>
    <span class="p">])</span>

<span class="n">cat_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;encoder&#39;</span><span class="p">,</span> <span class="n">encoder</span><span class="p">()),</span> <span class="c1"># encoding the categorical variables is needed by some imputers</span>
    <span class="p">(</span><span class="s1">&#39;imputer&#39;</span><span class="p">,</span> <span class="n">imputer</span><span class="p">())</span>
    <span class="p">])</span>

<span class="n">quant_cat_processing</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span><span class="n">transformers</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;quant&#39;</span><span class="p">,</span> <span class="n">quant_pipeline</span><span class="p">,</span> <span class="n">quant_predictors</span><span class="p">),</span>
                                                       <span class="p">(</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">cat_pipeline</span><span class="p">,</span> <span class="n">cat_predictors</span><span class="p">)])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Defining dictionaries to save important objects:</span>
<span class="n">inner_score</span><span class="p">,</span> <span class="n">best_params</span><span class="p">,</span> <span class="n">inner_results</span><span class="p">,</span> <span class="n">imb_pipelines</span> <span class="o">=</span> <span class="p">{},</span> <span class="p">{},</span> <span class="p">{},</span> <span class="p">{}</span>

<span class="n">model_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;knn&#39;</span><span class="p">,</span> <span class="s1">&#39;trees&#39;</span><span class="p">,</span> <span class="s1">&#39;extra_trees&#39;</span><span class="p">,</span>
              <span class="s1">&#39;RF&#39;</span><span class="p">,</span> <span class="s1">&#39;HGB&#39;</span><span class="p">,</span> <span class="s1">&#39;NN&#39;</span><span class="p">,</span> <span class="s1">&#39;SVM&#39;</span><span class="p">,</span> 
              <span class="s1">&#39;XGB&#39;</span><span class="p">,</span> <span class="s1">&#39;logistic_reg&#39;</span><span class="p">]</span>

<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> 
          <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span> 
          <span class="n">ExtraTreesClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span>
          <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span> 
          <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span> 
          <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span>
          <span class="n">LinearSVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span>  
          <span class="n">XGBClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span>
          <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;saga&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span>
          <span class="p">]</span>

<span class="n">samplers_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;random_under_sampler&#39;</span><span class="p">,</span>
                 <span class="c1">#&#39;near_miss&#39;, # Bad performance</span>
                 <span class="s1">&#39;random_over_sampler&#39;</span><span class="p">,</span>
                 <span class="c1">#&#39;SMOTE&#39;, # Bad performance and too slow</span>
                 <span class="c1">#&#39;SMOTETomek&#39; # Bad performance and too slow</span>
                 <span class="p">]</span>

<span class="n">samplers</span> <span class="o">=</span> <span class="p">[</span><span class="n">RandomUnderSampler</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span> 
            <span class="c1">#NearMiss(version=2),</span>
            <span class="n">RandomOverSampler</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">),</span>
            <span class="c1">#SMOTE(random_state=123),</span>
            <span class="c1">#SMOTETomek(random_state=123)</span>
            <span class="p">]</span>

<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">model_keys</span><span class="p">,</span> <span class="n">models</span><span class="p">):</span>

    <span class="n">imb_pipelines</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">inner_score</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">best_params</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">inner_results</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span> <span class="p">{},</span> <span class="p">{},</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">sampler_key</span><span class="p">,</span> <span class="n">sampler</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">samplers_keys</span><span class="p">,</span> <span class="n">samplers</span><span class="p">):</span> 
    
        <span class="n">imb_pipelines</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">sampler_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">ImblearnPipeline</span><span class="p">([</span>
                <span class="p">(</span><span class="s1">&#39;preprocessing&#39;</span><span class="p">,</span> <span class="n">quant_cat_processing</span><span class="p">),</span>
                <span class="p">(</span><span class="n">sampler_key</span><span class="p">,</span> <span class="n">sampler</span><span class="p">),</span>
                <span class="p">(</span><span class="s1">&#39;features_selector&#39;</span><span class="p">,</span> <span class="n">features_selector</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)),</span>
                <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span> 
                <span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id7">
<h4><strong>Applying inner evaluation</strong><a class="headerlink" href="#id7" title="Link to this heading">#</a></h4>
<section id="id8">
<h5><strong>Hyper-parameter optimization (HPO)</strong><a class="headerlink" href="#id8" title="Link to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;XGB&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_XGB_refined</span>

<span class="k">for</span> <span class="n">sampler_name</span> <span class="ow">in</span> <span class="n">samplers_keys</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sampler_name</span><span class="p">)</span>

    <span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">imb_pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">],</span>  
                               <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                               <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                               <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                               <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                               <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                               <span class="n">n_trials</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> 
                               <span class="n">random_state</span><span class="o">=</span><span class="mi">777</span><span class="p">)</span>

    <span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

    <span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
    <span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
    <span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 18:26:52,054] A new study created in memory with name: no-name-f50bc19d-322c-457a-9099-e5f9253ca477
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>random_under_sampler
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 18:26:55,570] Trial 0 finished with value: 0.7203053346360856 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 15, &#39;XGB__reg_lambda&#39;: 0.013113110332485146, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.028627615380173613, &#39;XGB__alpha&#39;: 0.236188847195606}. Best is trial 0 with value: 0.7203053346360856.
[I 2024-03-04 18:26:59,018] Trial 1 finished with value: 0.7096196034697796 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 15, &#39;XGB__reg_lambda&#39;: 0.058004363720065344, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.20325404880244344, &#39;XGB__alpha&#39;: 0.13269412534436575}. Best is trial 0 with value: 0.7203053346360856.
[I 2024-03-04 18:27:07,108] Trial 2 finished with value: 0.7167350663398206 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.10147719849596659, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.014287476498437994, &#39;XGB__alpha&#39;: 0.2065959387286594}. Best is trial 0 with value: 0.7203053346360856.
[I 2024-03-04 18:27:09,902] Trial 3 finished with value: 0.7142948671116361 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 30, &#39;XGB__reg_lambda&#39;: 0.1186413180859263, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.2343034103902644, &#39;XGB__alpha&#39;: 0.6880341073140974}. Best is trial 0 with value: 0.7203053346360856.
[I 2024-03-04 18:27:12,989] Trial 4 finished with value: 0.7145175231190196 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 25, &#39;XGB__reg_lambda&#39;: 0.21832244600357867, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.14290582887635042, &#39;XGB__alpha&#39;: 0.29197459429216027}. Best is trial 0 with value: 0.7203053346360856.
[I 2024-03-04 18:27:14,616] Trial 5 finished with value: 0.7465069578678785 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06108939266268055, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.07705504645751603, &#39;XGB__alpha&#39;: 0.12764755555259621}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:18,409] Trial 6 finished with value: 0.7162428396556173 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.022606258103355165, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.12040122280963049, &#39;XGB__alpha&#39;: 0.46802067065398656}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:23,222] Trial 7 finished with value: 0.7106259372092262 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 50, &#39;XGB__reg_lambda&#39;: 0.026438263870392276, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.011485974991295793, &#39;XGB__alpha&#39;: 0.31031731402654344}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:24,980] Trial 8 finished with value: 0.7422835675360941 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 6, &#39;XGB__reg_lambda&#39;: 0.23692539130816198, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.04269948099888012, &#39;XGB__alpha&#39;: 0.4858606057497037}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:27,494] Trial 9 finished with value: 0.7097532229814701 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.018090446358556046, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.2019834306931983, &#39;XGB__alpha&#39;: 0.14550883399179676}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:29,059] Trial 10 finished with value: 0.731489808725795 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.04595298877867764, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.44415875403290833, &#39;XGB__alpha&#39;: 0.10302232975078147}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:30,904] Trial 11 finished with value: 0.7416256977134994 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 6, &#39;XGB__reg_lambda&#39;: 0.20944823532346668, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.04542580538540337, &#39;XGB__alpha&#39;: 0.9680903922173163}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:32,810] Trial 12 finished with value: 0.7341579298936466 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.11392258930783045, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.056801667166412065, &#39;XGB__alpha&#39;: 0.4857681525566683}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:34,152] Trial 13 finished with value: 0.7417757176474469 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.280139898235427, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.02678560871620918, &#39;XGB__alpha&#39;: 0.4232001038114663}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:35,863] Trial 14 finished with value: 0.7374612182391349 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 6, &#39;XGB__reg_lambda&#39;: 0.05285783973281046, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.08325146323163779, &#39;XGB__alpha&#39;: 0.6296121320943613}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:38,694] Trial 15 finished with value: 0.7324004601940463 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.07851453382106345, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.038002138386605114, &#39;XGB__alpha&#39;: 0.19389563221459913}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:40,399] Trial 16 finished with value: 0.739848375167589 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 6, &#39;XGB__reg_lambda&#39;: 0.03665536204265607, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.07757406729428373, &#39;XGB__alpha&#39;: 0.3767059342537954}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:41,896] Trial 17 finished with value: 0.7395565355108228 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 4, &#39;XGB__reg_lambda&#39;: 0.152566665736053, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.019314512130927226, &#39;XGB__alpha&#39;: 0.7671361683569408}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:43,256] Trial 18 finished with value: 0.7458856003864552 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.07271872536843724, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.1064521184259617, &#39;XGB__alpha&#39;: 0.15673326696944284}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:44,743] Trial 19 finished with value: 0.745760051549591 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.07455730389347758, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.11738756564925416, &#39;XGB__alpha&#39;: 0.14800025881456788}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:46,310] Trial 20 finished with value: 0.7353088880984546 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.031112339995931063, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.3520307346457426, &#39;XGB__alpha&#39;: 0.1111539123661638}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:47,853] Trial 21 finished with value: 0.7450249274027874 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.07425218985070411, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.11090120160817382, &#39;XGB__alpha&#39;: 0.15265081922280113}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:49,228] Trial 22 finished with value: 0.7452565608391123 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.07343113485681962, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.08796612879200268, &#39;XGB__alpha&#39;: 0.17451028700478313}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:50,739] Trial 23 finished with value: 0.7446723633803712 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.04369682069062033, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.15715456698035604, &#39;XGB__alpha&#39;: 0.1171404236198912}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:52,352] Trial 24 finished with value: 0.7457440397362407 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.1587559619940625, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.06557427220982166, &#39;XGB__alpha&#39;: 0.23470567267883335}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:56,906] Trial 25 finished with value: 0.7134628948669178 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 30, &#39;XGB__reg_lambda&#39;: 0.09066491023286319, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.11509542182347529, &#39;XGB__alpha&#39;: 0.13020982535418152}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:27:58,616] Trial 26 finished with value: 0.7322085324503419 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 4, &#39;XGB__reg_lambda&#39;: 0.05823371083172876, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.24245100298837882, &#39;XGB__alpha&#39;: 0.17112191213600966}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:28:01,994] Trial 27 finished with value: 0.7184107930007414 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.06402066864553017, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.05593336413799839, &#39;XGB__alpha&#39;: 0.2671280857728062}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:28:03,905] Trial 28 finished with value: 0.7232228695057374 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.14267957539231613, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.1526467434753663, &#39;XGB__alpha&#39;: 0.15862619683371593}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:28:09,067] Trial 29 finished with value: 0.7155295113996564 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 50, &#39;XGB__reg_lambda&#39;: 0.01561426139729306, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.027588573189845516, &#39;XGB__alpha&#39;: 0.2239260985564616}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:28:11,311] Trial 30 finished with value: 0.7229191181411853 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.012195018178299672, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.09853604666737675, &#39;XGB__alpha&#39;: 0.12799854144989956}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:28:12,998] Trial 31 finished with value: 0.7459144306617068 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.15452780086603643, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.06445425280017751, &#39;XGB__alpha&#39;: 0.2514504841467082}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:28:15,966] Trial 32 finished with value: 0.7178905132590992 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 15, &#39;XGB__reg_lambda&#39;: 0.08984559548878719, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.07223080310529427, &#39;XGB__alpha&#39;: 0.26447453971197654}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:28:17,801] Trial 33 finished with value: 0.7449159085244821 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.04138569589417173, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.05667103863399458, &#39;XGB__alpha&#39;: 0.19208974943788376}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:28:19,222] Trial 34 finished with value: 0.7436949894524784 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.11378815392561956, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.03359297296089493, &#39;XGB__alpha&#39;: 0.14094524912667628}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:28:22,636] Trial 35 finished with value: 0.7168931569485942 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 25, &#39;XGB__reg_lambda&#39;: 0.17530936484470372, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.1845844318449086, &#39;XGB__alpha&#39;: 0.3613163883959047}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:28:24,139] Trial 36 finished with value: 0.7419527543484153 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.052245538140571604, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.27899315812998143, &#39;XGB__alpha&#39;: 0.10016490179417611}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:28:28,989] Trial 37 finished with value: 0.712826043717353 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.09516412066877632, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.128249371307353, &#39;XGB__alpha&#39;: 0.11993250327546744}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:28:30,388] Trial 38 finished with value: 0.7460078263355682 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.13267074712150131, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.09173790109821314, &#39;XGB__alpha&#39;: 0.2062310832619681}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:28:33,487] Trial 39 finished with value: 0.7208329078347407 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 15, &#39;XGB__reg_lambda&#39;: 0.1303481943285343, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.048088470431800104, &#39;XGB__alpha&#39;: 0.2626183090478723}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:28:36,989] Trial 40 finished with value: 0.7170062759324637 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 30, &#39;XGB__reg_lambda&#39;: 0.18675618083341797, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.09372624846745653, &#39;XGB__alpha&#39;: 0.1977532570761699}. Best is trial 5 with value: 0.7465069578678785.
[I 2024-03-04 18:28:38,645] Trial 41 finished with value: 0.7466573664107322 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.0654247859212872, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.07108147786445947, &#39;XGB__alpha&#39;: 0.16771073476863432}. Best is trial 41 with value: 0.7466573664107322.
[I 2024-03-04 18:28:39,925] Trial 42 finished with value: 0.7460973809764698 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.1097064900614082, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.06608976220774931, &#39;XGB__alpha&#39;: 0.17148649787031733}. Best is trial 41 with value: 0.7466573664107322.
[I 2024-03-04 18:28:41,501] Trial 43 finished with value: 0.7463045433157117 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.10836659594895032, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.06603058030175157, &#39;XGB__alpha&#39;: 0.22436021062826803}. Best is trial 41 with value: 0.7466573664107322.
[I 2024-03-04 18:28:45,596] Trial 44 finished with value: 0.7180735931101645 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 25, &#39;XGB__reg_lambda&#39;: 0.10540789911596989, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.049530483205076144, &#39;XGB__alpha&#39;: 0.21677193445314555}. Best is trial 41 with value: 0.7466573664107322.
[I 2024-03-04 18:28:47,170] Trial 45 finished with value: 0.7426602140466869 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.12939684256271958, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.0372551428743379, &#39;XGB__alpha&#39;: 0.1766522628568056}. Best is trial 41 with value: 0.7466573664107322.
[I 2024-03-04 18:28:50,632] Trial 46 finished with value: 0.716162132907355 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 50, &#39;XGB__reg_lambda&#39;: 0.0636120104200884, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.07468851883579693, &#39;XGB__alpha&#39;: 0.31007124255001733}. Best is trial 41 with value: 0.7466573664107322.
[I 2024-03-04 18:28:53,789] Trial 47 finished with value: 0.7176191741301542 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.08436475680396983, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.06275570125006251, &#39;XGB__alpha&#39;: 0.20489559950915526}. Best is trial 41 with value: 0.7466573664107322.
[I 2024-03-04 18:28:58,133] Trial 48 finished with value: 0.7116406006961032 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.10507926046418797, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.02137061307395582, &#39;XGB__alpha&#39;: 0.13648862141098456}. Best is trial 41 with value: 0.7466573664107322.
[I 2024-03-04 18:28:59,659] Trial 49 finished with value: 0.7459432609369583 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.12857183551438583, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.08479472137559083, &#39;XGB__alpha&#39;: 0.1825547017466727}. Best is trial 41 with value: 0.7466573664107322.
[I 2024-03-04 18:29:01,774] Trial 50 finished with value: 0.7345237009868706 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.0328673874451707, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.04140415332213631, &#39;XGB__alpha&#39;: 0.2887779250274723}. Best is trial 41 with value: 0.7466573664107322.
[I 2024-03-04 18:29:03,247] Trial 51 finished with value: 0.7471522683011761 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.22230771795342033, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.08298571881175319, &#39;XGB__alpha&#39;: 0.17992596509979406}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:04,617] Trial 52 finished with value: 0.7445626968205552 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.2524100438500832, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.1337525400895815, &#39;XGB__alpha&#39;: 0.1645967337085378}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:05,823] Trial 53 finished with value: 0.7463532264372734 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.21326125411993957, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.07577375420440038, &#39;XGB__alpha&#39;: 0.21446696195524753}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:07,593] Trial 54 finished with value: 0.7388499825666667 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.204731145465348, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.05201198168756462, &#39;XGB__alpha&#39;: 0.23916221043344943}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:08,968] Trial 55 finished with value: 0.7459264719057952 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.010263561574846328, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.06483317687204024, &#39;XGB__alpha&#39;: 0.14749785481046096}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:10,721] Trial 56 finished with value: 0.7435691815430099 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 4, &#39;XGB__reg_lambda&#39;: 0.2692225522168256, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.07579535009388556, &#39;XGB__alpha&#39;: 0.18317136413081278}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:12,242] Trial 57 finished with value: 0.7464259920470117 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.2259333235097142, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.04446407743322264, &#39;XGB__alpha&#39;: 0.16428389649824854}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:13,823] Trial 58 finished with value: 0.7448514726621744 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.21591145645826837, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.03311296865774132, &#39;XGB__alpha&#39;: 0.10795109263260279}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:15,685] Trial 59 finished with value: 0.7412687745129148 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 6, &#39;XGB__reg_lambda&#39;: 0.22851435669046558, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.042184176051904755, &#39;XGB__alpha&#39;: 0.11874493459622305}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:17,126] Trial 60 finished with value: 0.7449155199155756 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.17835330580135794, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.10568467306329993, &#39;XGB__alpha&#39;: 0.21879673256744356}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:18,720] Trial 61 finished with value: 0.746385250063974 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.27077399448853007, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.07989264711451065, &#39;XGB__alpha&#39;: 0.1582616827037623}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:20,204] Trial 62 finished with value: 0.7454434817231373 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.27256391032402444, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.07983989719762877, &#39;XGB__alpha&#39;: 0.1297967183300398}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:21,763] Trial 63 finished with value: 0.7453991577797442 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.2421208237356868, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.05952733438985355, &#39;XGB__alpha&#39;: 0.1552644412674702}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:23,196] Trial 64 finished with value: 0.7456997157928473 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.29929902097890576, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.051745621096149365, &#39;XGB__alpha&#39;: 0.18736951780270011}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:24,796] Trial 65 finished with value: 0.7446677451295983 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.19388621410360796, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.10037933849171081, &#39;XGB__alpha&#39;: 0.1640573727276078}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:29,301] Trial 66 finished with value: 0.7125700687202473 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 30, &#39;XGB__reg_lambda&#39;: 0.16041330693387035, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.08708123987461953, &#39;XGB__alpha&#39;: 0.14588394365129592}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:35,194] Trial 67 finished with value: 0.7158091802760321 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 50, &#39;XGB__reg_lambda&#39;: 0.29819579882275676, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.011367842226104298, &#39;XGB__alpha&#39;: 0.23313384347147909}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:37,921] Trial 68 finished with value: 0.7199444408662387 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 15, &#39;XGB__reg_lambda&#39;: 0.049285220073461235, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.07173531592598059, &#39;XGB__alpha&#39;: 0.13926220427099328}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:39,551] Trial 69 finished with value: 0.7453217739192329 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06495593802788806, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.046146640109147374, &#39;XGB__alpha&#39;: 0.12700895955931504}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:41,235] Trial 70 finished with value: 0.7219846432573617 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.2460236655057369, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.17101965498232627, &#39;XGB__alpha&#39;: 0.16122989115779288}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:42,728] Trial 71 finished with value: 0.746296213568281 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.21915345311915044, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.06628928306017945, &#39;XGB__alpha&#39;: 0.17190300670406033}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:44,334] Trial 72 finished with value: 0.7450991179118495 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.2095307387548008, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.05523811395411605, &#39;XGB__alpha&#39;: 0.19604743026396834}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:45,687] Trial 73 finished with value: 0.745691515581719 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.1723835085672675, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.06925871059112418, &#39;XGB__alpha&#39;: 0.17462301323089022}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:48,612] Trial 74 finished with value: 0.7175496018718649 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.02248934628129506, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.12204158964812502, &#39;XGB__alpha&#39;: 0.5785852353935742}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:50,246] Trial 75 finished with value: 0.7468642696773697 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.260921022352217, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.06006105863188066, &#39;XGB__alpha&#39;: 0.21320818579875947}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:53,861] Trial 76 finished with value: 0.7160037832259771 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 25, &#39;XGB__reg_lambda&#39;: 0.26124054471074215, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.07937869051189814, &#39;XGB__alpha&#39;: 0.21378375449398979}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:55,450] Trial 77 finished with value: 0.7450782287751222 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.03985048669388965, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.05815352262875874, &#39;XGB__alpha&#39;: 0.35719114056081225}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:57,783] Trial 78 finished with value: 0.7327500899376179 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.05814340349935915, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.03761097068936544, &#39;XGB__alpha&#39;: 0.24640295985822938}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:29:59,442] Trial 79 finished with value: 0.7434636150887579 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 4, &#39;XGB__reg_lambda&#39;: 0.23090560072043823, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.09919560627241387, &#39;XGB__alpha&#39;: 0.15148325520415098}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:04,449] Trial 80 finished with value: 0.7178587487050031 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.18664123909829256, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.03318125997565898, &#39;XGB__alpha&#39;: 0.2832471150471694}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:05,944] Trial 81 finished with value: 0.7459064895231826 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.21016390449372724, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.08644034968014179, &#39;XGB__alpha&#39;: 0.20274702002793227}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:06,999] Trial 82 finished with value: 0.7468805405633243 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.231574384562681, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.0723189850392092, &#39;XGB__alpha&#39;: 0.2275737996697292}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:08,676] Trial 83 finished with value: 0.7461131337172159 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.28141565190167195, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.07362138210712418, &#39;XGB__alpha&#39;: 0.22743052653619852}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:09,754] Trial 84 finished with value: 0.74453809618717 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.16552754227805533, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.10934444529578799, &#39;XGB__alpha&#39;: 0.1909955358345712}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:11,489] Trial 85 finished with value: 0.7360529896741995 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 6, &#39;XGB__reg_lambda&#39;: 0.19586587773784794, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.061444804090705255, &#39;XGB__alpha&#39;: 0.2583646167125544}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:12,934] Trial 86 finished with value: 0.7442430631789546 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.14736044615441785, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.045139618473213884, &#39;XGB__alpha&#39;: 0.2066107701855552}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:14,145] Trial 87 finished with value: 0.7457401987032807 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.2346338338686771, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.05331396429662264, &#39;XGB__alpha&#39;: 0.31524121044038655}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:15,640] Trial 88 finished with value: 0.7469129527989314 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.24596376133253187, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.08109972334689387, &#39;XGB__alpha&#39;: 0.2749056177717228}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:19,071] Trial 89 finished with value: 0.7175503790896779 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 30, &#39;XGB__reg_lambda&#39;: 0.2805122910433476, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.09192701935729844, &#39;XGB__alpha&#39;: 0.2739028373627862}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:20,658] Trial 90 finished with value: 0.7458048936381929 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.25247973724931255, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.08007366460893722, &#39;XGB__alpha&#39;: 0.24261848654912077}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:22,112] Trial 91 finished with value: 0.7465762710535634 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06812791916417757, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.0715123826910869, &#39;XGB__alpha&#39;: 0.33663888548890836}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:23,180] Trial 92 finished with value: 0.7460811100905153 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.0806049634136552, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.07178257162427248, &#39;XGB__alpha&#39;: 0.35176379717249967}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:24,361] Trial 93 finished with value: 0.7460483092460016 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06670402633888997, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.08245792701937653, &#39;XGB__alpha&#39;: 0.39424773208226743}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:25,584] Trial 94 finished with value: 0.7447984303624442 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.04820823164813262, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.09376347523199734, &#39;XGB__alpha&#39;: 0.32273414043222354}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:28,674] Trial 95 finished with value: 0.7205003261949369 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 15, &#39;XGB__reg_lambda&#39;: 0.22441287270626753, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.06010689883198077, &#39;XGB__alpha&#39;: 0.30117471843592736}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:30,166] Trial 96 finished with value: 0.7451953183282537 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.2585947614841375, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.13930110389126524, &#39;XGB__alpha&#39;: 0.1631392234017591}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:34,233] Trial 97 finished with value: 0.7171769259305342 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 50, &#39;XGB__reg_lambda&#39;: 0.20336236257686355, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.04917190814606861, &#39;XGB__alpha&#39;: 0.18099074338381782}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:36,954] Trial 98 finished with value: 0.7062667252486144 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.06952399207856469, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.46864968529653545, &#39;XGB__alpha&#39;: 0.4304131415781681}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:38,383] Trial 99 finished with value: 0.7450050745564774 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.05904296771915156, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.11418874335257043, &#39;XGB__alpha&#39;: 0.12330383470205791}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:39,793] Trial 100 finished with value: 0.7453176738136689 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.09785256455853883, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.06774323515673318, &#39;XGB__alpha&#39;: 0.3319138062504177}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:41,327] Trial 101 finished with value: 0.7468276277998962 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.18384120737807427, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.1010366242909199, &#39;XGB__alpha&#39;: 0.2135193804710141}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:42,678] Trial 102 finished with value: 0.7463684610328108 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.18279700042665278, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.10210887875299245, &#39;XGB__alpha&#39;: 0.21409520342767213}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:43,758] Trial 103 finished with value: 0.7457399396306764 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.05358255539275721, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.10304320580939581, &#39;XGB__alpha&#39;: 0.11173301832103956}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:46,798] Trial 104 finished with value: 0.7135924142730442 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 25, &#39;XGB__reg_lambda&#39;: 0.18022194228925556, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.1266185993484159, &#39;XGB__alpha&#39;: 0.2500574325289477}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:48,251] Trial 105 finished with value: 0.7460403681074775 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.1390424143754474, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.09267383530398939, &#39;XGB__alpha&#39;: 0.13676589695063662}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:52,307] Trial 106 finished with value: 0.7139905976018753 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.24192020654168184, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.08551808133116028, &#39;XGB__alpha&#39;: 0.2338399782380957}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:54,362] Trial 107 finished with value: 0.7252077429997964 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.28239027044293635, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.15660454341787566, &#39;XGB__alpha&#39;: 0.18747768139491358}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:57,007] Trial 108 finished with value: 0.7198141442422994 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.1900682963754088, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.07715558121036782, &#39;XGB__alpha&#39;: 0.16893826737354442}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:30:58,821] Trial 109 finished with value: 0.7448428838421396 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.22947368074293908, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.1101960464999976, &#39;XGB__alpha&#39;: 0.2073885300764386}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:00,322] Trial 110 finished with value: 0.7442754754145618 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 4, &#39;XGB__reg_lambda&#39;: 0.2982276248659769, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.09822336410431262, &#39;XGB__alpha&#39;: 0.2780869413265054}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:01,904] Trial 111 finished with value: 0.7457117570369357 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.19808659353397015, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.07045916084256779, &#39;XGB__alpha&#39;: 0.20954863709302643}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:03,174] Trial 112 finished with value: 0.746624954175125 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.21667314066214685, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.07807577220386872, &#39;XGB__alpha&#39;: 0.22148910179527576}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:04,697] Trial 113 finished with value: 0.7458010526052329 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.17114689452271686, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.06368295949980467, &#39;XGB__alpha&#39;: 0.21832882423629701}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:06,175] Trial 114 finished with value: 0.7448475020929125 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.2610855885664864, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.0564595469357133, &#39;XGB__alpha&#39;: 0.19779164691851164}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:07,369] Trial 115 finished with value: 0.7458981597757521 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.21908125804483225, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.08833688690211937, &#39;XGB__alpha&#39;: 0.22779619338456655}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:09,070] Trial 116 finished with value: 0.7412809452933052 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 6, &#39;XGB__reg_lambda&#39;: 0.1213702412767122, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.0825625148910404, &#39;XGB__alpha&#39;: 0.2980689780542869}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:10,429] Trial 117 finished with value: 0.743994122566258 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.23821191421788726, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.11842494098763819, &#39;XGB__alpha&#39;: 0.15437533708000065}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:11,653] Trial 118 finished with value: 0.7460893103016435 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.15843707894231135, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.07617380260695789, &#39;XGB__alpha&#39;: 0.25979176649602265}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:13,135] Trial 119 finished with value: 0.7423884863088351 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.061777124601801375, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.024511992658700365, &#39;XGB__alpha&#39;: 0.14213216557627792}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:14,598] Trial 120 finished with value: 0.7459230194817418 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.07831964771411554, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.06152547550956838, &#39;XGB__alpha&#39;: 0.1796975486910855}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:15,845] Trial 121 finished with value: 0.7460565094571301 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.21694674898302732, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.07339309633079141, &#39;XGB__alpha&#39;: 0.19329101880269756}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:17,128] Trial 122 finished with value: 0.7462714833985937 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.24961306458087792, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.09835485212945733, &#39;XGB__alpha&#39;: 0.21869270427213258}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:18,672] Trial 123 finished with value: 0.7465273288593973 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.20822917130565516, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.07911372570248425, &#39;XGB__alpha&#39;: 0.24367187805395316}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:20,201] Trial 124 finished with value: 0.7454356701209154 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.1845588678671006, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.0670878410151997, &#39;XGB__alpha&#39;: 0.8861720536846119}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:21,760] Trial 125 finished with value: 0.7460725212704803 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.20397924515491883, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.08182134025263232, &#39;XGB__alpha&#39;: 0.23927043886384813}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:25,294] Trial 126 finished with value: 0.71494824821976 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 30, &#39;XGB__reg_lambda&#39;: 0.023625941675546012, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.0898430565161729, &#39;XGB__alpha&#39;: 0.2700564472739601}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:28,036] Trial 127 finished with value: 0.7163276465094439 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 15, &#39;XGB__reg_lambda&#39;: 0.2668885346551404, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.10473468201852612, &#39;XGB__alpha&#39;: 0.254616935043836}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:29,497] Trial 128 finished with value: 0.74549281252621 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.23890981300509706, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.05509211434296941, &#39;XGB__alpha&#39;: 0.22918970401244804}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:33,639] Trial 129 finished with value: 0.7151674518030902 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 50, &#39;XGB__reg_lambda&#39;: 0.2779224145916535, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.06923320718878338, &#39;XGB__alpha&#39;: 0.2000365433171578}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:35,183] Trial 130 finished with value: 0.7454478409013059 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.2252914826135965, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.04089174747668284, &#39;XGB__alpha&#39;: 0.16878779250993764}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:36,668] Trial 131 finished with value: 0.7252495663293562 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.20278184507251198, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.010003370698527561, &#39;XGB__alpha&#39;: 0.21139819365971893}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:38,226] Trial 132 finished with value: 0.7335190512193522 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.21569637402196642, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.01616381710526886, &#39;XGB__alpha&#39;: 0.24285693665919525}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:39,777] Trial 133 finished with value: 0.746170017049906 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.17644768113841722, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.07954447446758676, &#39;XGB__alpha&#39;: 0.2201392360353978}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:41,052] Trial 134 finished with value: 0.7450381344735953 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.07105790909248969, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.07582164040756946, &#39;XGB__alpha&#39;: 0.15948221686275116}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:44,100] Trial 135 finished with value: 0.7202451284156443 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.1900460955681639, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.08712998887835707, &#39;XGB__alpha&#39;: 0.3381332061803055}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:48,523] Trial 136 finished with value: 0.7149323659427118 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 25, &#39;XGB__reg_lambda&#39;: 0.01797965543611209, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.0952519795671015, &#39;XGB__alpha&#39;: 0.1864755951188902}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:50,152] Trial 137 finished with value: 0.7462509828707727 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.25236610842524554, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.07072260173648091, &#39;XGB__alpha&#39;: 0.19998935621185293}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:51,544] Trial 138 finished with value: 0.7455776193800366 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.16273043424350758, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.060579251806209426, &#39;XGB__alpha&#39;: 0.1749696511189919}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:54,130] Trial 139 finished with value: 0.7312157943906095 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.1493740516073292, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.06460763159868212, &#39;XGB__alpha&#39;: 0.15029050642951308}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:57,978] Trial 140 finished with value: 0.7185085478527714 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.08567662504548744, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.05033944324444603, &#39;XGB__alpha&#39;: 0.23933074421869374}. Best is trial 51 with value: 0.7471522683011761.
[I 2024-03-04 18:31:59,464] Trial 141 finished with value: 0.7473352186159391 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.2326544076558103, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.07890028187030075, &#39;XGB__alpha&#39;: 0.22158914781040018}. Best is trial 141 with value: 0.7473352186159391.
[I 2024-03-04 18:32:00,717] Trial 142 finished with value: 0.7468073863446795 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.23219062681031993, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.07762787263034383, &#39;XGB__alpha&#39;: 0.22731706158283557}. Best is trial 141 with value: 0.7473352186159391.
[I 2024-03-04 18:32:02,234] Trial 143 finished with value: 0.7462553420489413 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.2270525374142879, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.08276915984664969, &#39;XGB__alpha&#39;: 0.228988179979321}. Best is trial 141 with value: 0.7473352186159391.
[I 2024-03-04 18:32:03,465] Trial 144 finished with value: 0.7465111875097449 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.2699005966983671, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.07600740938581112, &#39;XGB__alpha&#39;: 0.290449014685184}. Best is trial 141 with value: 0.7473352186159391.
[I 2024-03-04 18:32:05,575] Trial 145 finished with value: 0.7313864443886798 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.26822617103176694, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.07301076687759486, &#39;XGB__alpha&#39;: 0.2987242961827786}. Best is trial 141 with value: 0.7473352186159391.
[I 2024-03-04 18:32:06,969] Trial 146 finished with value: 0.7468968114492789 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.28865950200191504, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.07870335578283526, &#39;XGB__alpha&#39;: 0.2792055265857862}. Best is trial 141 with value: 0.7473352186159391.
[I 2024-03-04 18:32:08,474] Trial 147 finished with value: 0.7445878155991489 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 4, &#39;XGB__reg_lambda&#39;: 0.2472559883962641, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.08938375828883274, &#39;XGB__alpha&#39;: 0.2895917904828608}. Best is trial 141 with value: 0.7473352186159391.
[I 2024-03-04 18:32:10,067] Trial 148 finished with value: 0.7460483092460016 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.290472003042491, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.06761346110957443, &#39;XGB__alpha&#39;: 0.2735587198886292}. Best is trial 141 with value: 0.7473352186159391.
[I 2024-03-04 18:32:11,586] Trial 149 finished with value: 0.7374355813153324 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.25900767039078687, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.38687991448132636, &#39;XGB__alpha&#39;: 0.25422562569091456}. Best is trial 141 with value: 0.7473352186159391.
[I 2024-03-04 18:32:11,654] A new study created in memory with name: no-name-9a6c8232-0491-4d35-bd3e-3965b976b784
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>random_over_sampler
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 18:32:20,464] Trial 0 finished with value: 0.6506687925488737 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 15, &#39;XGB__reg_lambda&#39;: 0.013113110332485146, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.028627615380173613, &#39;XGB__alpha&#39;: 0.236188847195606}. Best is trial 0 with value: 0.6506687925488737.
[I 2024-03-04 18:32:28,626] Trial 1 finished with value: 0.6049632205887657 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 15, &#39;XGB__reg_lambda&#39;: 0.058004363720065344, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.20325404880244344, &#39;XGB__alpha&#39;: 0.13269412534436575}. Best is trial 0 with value: 0.6506687925488737.
[I 2024-03-04 18:32:56,738] Trial 2 finished with value: 0.6223875991541257 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.10147719849596659, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.014287476498437994, &#39;XGB__alpha&#39;: 0.2065959387286594}. Best is trial 0 with value: 0.6506687925488737.
[I 2024-03-04 18:33:04,424] Trial 3 finished with value: 0.6123640068763727 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 30, &#39;XGB__reg_lambda&#39;: 0.1186413180859263, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.2343034103902644, &#39;XGB__alpha&#39;: 0.6880341073140974}. Best is trial 0 with value: 0.6506687925488737.
[I 2024-03-04 18:33:12,374] Trial 4 finished with value: 0.6109720491973919 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 25, &#39;XGB__reg_lambda&#39;: 0.21832244600357867, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.14290582887635042, &#39;XGB__alpha&#39;: 0.29197459429216027}. Best is trial 0 with value: 0.6506687925488737.
[I 2024-03-04 18:33:14,932] Trial 5 finished with value: 0.7474544652301023 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06108939266268055, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.07705504645751603, &#39;XGB__alpha&#39;: 0.12764755555259621}. Best is trial 5 with value: 0.7474544652301023.
[I 2024-03-04 18:33:27,190] Trial 6 finished with value: 0.6096359385606841 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.022606258103355165, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.12040122280963049, &#39;XGB__alpha&#39;: 0.46802067065398656}. Best is trial 5 with value: 0.7474544652301023.
[I 2024-03-04 18:33:46,906] Trial 7 finished with value: 0.6224455638333383 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 50, &#39;XGB__reg_lambda&#39;: 0.026438263870392276, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.011485974991295793, &#39;XGB__alpha&#39;: 0.31031731402654344}. Best is trial 5 with value: 0.7474544652301023.
[I 2024-03-04 18:33:50,078] Trial 8 finished with value: 0.7415982022253615 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 6, &#39;XGB__reg_lambda&#39;: 0.23692539130816198, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.04269948099888012, &#39;XGB__alpha&#39;: 0.4858606057497037}. Best is trial 5 with value: 0.7474544652301023.
[I 2024-03-04 18:33:56,905] Trial 9 finished with value: 0.6063236727862544 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.018090446358556046, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.2019834306931983, &#39;XGB__alpha&#39;: 0.14550883399179676}. Best is trial 5 with value: 0.7474544652301023.
[I 2024-03-04 18:33:59,551] Trial 10 finished with value: 0.7309208852866833 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.04595298877867764, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.44415875403290833, &#39;XGB__alpha&#39;: 0.10302232975078147}. Best is trial 5 with value: 0.7474544652301023.
[I 2024-03-04 18:34:02,673] Trial 11 finished with value: 0.740830795379253 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 6, &#39;XGB__reg_lambda&#39;: 0.20944823532346668, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.04542580538540337, &#39;XGB__alpha&#39;: 0.9680903922173163}. Best is trial 5 with value: 0.7474544652301023.
[I 2024-03-04 18:34:06,406] Trial 12 finished with value: 0.7249156780625046 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.11392258930783045, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.056801667166412065, &#39;XGB__alpha&#39;: 0.4857681525566683}. Best is trial 5 with value: 0.7474544652301023.
[I 2024-03-04 18:34:09,049] Trial 13 finished with value: 0.7409667916004876 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.280139898235427, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.02678560871620918, &#39;XGB__alpha&#39;: 0.4232001038114663}. Best is trial 5 with value: 0.7474544652301023.
[I 2024-03-04 18:34:12,159] Trial 14 finished with value: 0.7342502836450776 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 6, &#39;XGB__reg_lambda&#39;: 0.05285783973281046, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.08325146323163779, &#39;XGB__alpha&#39;: 0.6296121320943613}. Best is trial 5 with value: 0.7474544652301023.
[I 2024-03-04 18:34:18,081] Trial 15 finished with value: 0.6945851302552851 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.07851453382106345, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.038002138386605114, &#39;XGB__alpha&#39;: 0.19389563221459913}. Best is trial 5 with value: 0.7474544652301023.
[I 2024-03-04 18:34:21,304] Trial 16 finished with value: 0.7340584854376754 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 6, &#39;XGB__reg_lambda&#39;: 0.03665536204265607, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.07757406729428373, &#39;XGB__alpha&#39;: 0.3767059342537954}. Best is trial 5 with value: 0.7474544652301023.
[I 2024-03-04 18:34:24,108] Trial 17 finished with value: 0.7403947649221371 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 4, &#39;XGB__reg_lambda&#39;: 0.152566665736053, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.019314512130927226, &#39;XGB__alpha&#39;: 0.7671361683569408}. Best is trial 5 with value: 0.7474544652301023.
[I 2024-03-04 18:34:26,666] Trial 18 finished with value: 0.7477502754561306 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.07271872536843724, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.1064521184259617, &#39;XGB__alpha&#39;: 0.15673326696944284}. Best is trial 18 with value: 0.7477502754561306.
[I 2024-03-04 18:34:29,110] Trial 19 finished with value: 0.7462202095509887 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.07455730389347758, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.11738756564925416, &#39;XGB__alpha&#39;: 0.14800025881456788}. Best is trial 18 with value: 0.7477502754561306.
[I 2024-03-04 18:34:31,723] Trial 20 finished with value: 0.736146295235851 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.031112339995931063, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.3520307346457426, &#39;XGB__alpha&#39;: 0.1111539123661638}. Best is trial 18 with value: 0.7477502754561306.
[I 2024-03-04 18:34:34,371] Trial 21 finished with value: 0.7471660779973895 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.07425218985070411, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.11090120160817382, &#39;XGB__alpha&#39;: 0.15265081922280113}. Best is trial 18 with value: 0.7477502754561306.
[I 2024-03-04 18:34:36,949] Trial 22 finished with value: 0.7481239876878787 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.07343113485681962, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.08796612879200268, &#39;XGB__alpha&#39;: 0.17451028700478313}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:34:39,358] Trial 23 finished with value: 0.7472558917108953 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.0430695133498055, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.0667133959171566, &#39;XGB__alpha&#39;: 0.17962568546586194}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:34:41,991] Trial 24 finished with value: 0.7471419955092129 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06108852639832091, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.08741161049351828, &#39;XGB__alpha&#39;: 0.23398631795359465}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:34:54,312] Trial 25 finished with value: 0.605336285139003 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 30, &#39;XGB__reg_lambda&#39;: 0.15385779883742398, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.15875850300137467, &#39;XGB__alpha&#39;: 0.12449377530886731}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:34:57,483] Trial 26 finished with value: 0.7452817640979031 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 4, &#39;XGB__reg_lambda&#39;: 0.09617797313239149, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.0557784495795603, &#39;XGB__alpha&#39;: 0.17251300489862295}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:35:03,774] Trial 27 finished with value: 0.6058405530672946 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.08227508699577595, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.2666351517224417, &#39;XGB__alpha&#39;: 0.2295510410193946}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:35:07,896] Trial 28 finished with value: 0.689964677365284 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.14545112615502723, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.1526467434753663, &#39;XGB__alpha&#39;: 0.12420565043464521}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:35:26,864] Trial 29 finished with value: 0.6183362386636305 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 50, &#39;XGB__reg_lambda&#39;: 0.01561426139729306, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.0265907895544761, &#39;XGB__alpha&#39;: 0.2764962211733444}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:35:32,021] Trial 30 finished with value: 0.6616724339607599 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.012195018178299672, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.09853604666737675, &#39;XGB__alpha&#39;: 0.15839587519859}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:35:34,610] Trial 31 finished with value: 0.7462375167273606 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.038757011971023395, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.06445425280017751, &#39;XGB__alpha&#39;: 0.18286535244131608}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:35:42,851] Trial 32 finished with value: 0.6224424099059812 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 15, &#39;XGB__reg_lambda&#39;: 0.061006812985374005, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.06789774346364955, &#39;XGB__alpha&#39;: 0.1301426094113872}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:35:46,229] Trial 33 finished with value: 0.7474352600653029 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.0461002832117117, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.032075282337360754, &#39;XGB__alpha&#39;: 0.20547978386594093}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:35:49,485] Trial 34 finished with value: 0.747012864711993 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.05769129823931925, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.03359297296089493, &#39;XGB__alpha&#39;: 0.21683287329112533}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:36:00,830] Trial 35 finished with value: 0.6067572026295374 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 25, &#39;XGB__reg_lambda&#39;: 0.051455081706550915, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.1845844318449086, &#39;XGB__alpha&#39;: 0.11017425889662026}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:36:04,420] Trial 36 finished with value: 0.7480389217614477 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.03228023241681858, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.053626533715018194, &#39;XGB__alpha&#39;: 0.25877668190439546}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:36:25,184] Trial 37 finished with value: 0.6098155659876959 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.09488003302469306, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.05089809010417887, &#39;XGB__alpha&#39;: 0.27494469453649617}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:36:27,835] Trial 38 finished with value: 0.7463055345500239 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.031732458788743256, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.13723997557597642, &#39;XGB__alpha&#39;: 0.24539944603057845}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:36:35,174] Trial 39 finished with value: 0.6183262249442718 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 15, &#39;XGB__reg_lambda&#39;: 0.01033365169636146, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.10206267932355415, &#39;XGB__alpha&#39;: 0.35564891198142623}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:36:50,687] Trial 40 finished with value: 0.6077928847894442 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 30, &#39;XGB__reg_lambda&#39;: 0.022433316738625583, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.08761749768923593, &#39;XGB__alpha&#39;: 0.16784543916473763}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:36:54,152] Trial 41 finished with value: 0.7422082112003124 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06788904585461711, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.019541134269798074, &#39;XGB__alpha&#39;: 0.19755606482727717}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:36:57,631] Trial 42 finished with value: 0.7467003949911039 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.048035321133824964, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.030878478369151557, &#39;XGB__alpha&#39;: 0.25907250535691223}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:37:01,179] Trial 43 finished with value: 0.7431739324930234 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.027006157731943065, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.021651406970504838, &#39;XGB__alpha&#39;: 0.13919875134885146}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:37:18,671] Trial 44 finished with value: 0.6143824584327581 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 25, &#39;XGB__reg_lambda&#39;: 0.03588491357827425, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.04465589746848696, &#39;XGB__alpha&#39;: 0.2132245565480778}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:37:22,218] Trial 45 finished with value: 0.7470287469890412 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.044419307502126366, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.0372551428743379, &#39;XGB__alpha&#39;: 0.16193195562484158}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:37:47,586] Trial 46 finished with value: 0.6234334245696934 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 50, &#39;XGB__reg_lambda&#39;: 0.10880163169114293, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.013779425038887395, &#39;XGB__alpha&#39;: 0.31578417606332465}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:38:00,537] Trial 47 finished with value: 0.618532869138305 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.02059129875860666, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.0532526395479152, &#39;XGB__alpha&#39;: 0.19604936335232775}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:38:14,960] Trial 48 finished with value: 0.6117801980265382 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.0859783721532295, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.07346224242308669, &#39;XGB__alpha&#39;: 0.13648862141098456}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:38:17,707] Trial 49 finished with value: 0.7421798990702695 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.12857183551438583, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.04342776921345071, &#39;XGB__alpha&#39;: 0.1150866846595565}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:38:25,027] Trial 50 finished with value: 0.6275723966565576 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.028199553231854503, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.1272829222718695, &#39;XGB__alpha&#39;: 0.10156793721509386}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:38:27,890] Trial 51 finished with value: 0.7469596928759606 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.04208170833293767, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.06449582713902806, &#39;XGB__alpha&#39;: 0.18282841064148136}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:38:30,743] Trial 52 finished with value: 0.7471335362254802 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06588281181319972, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.09317724497849107, &#39;XGB__alpha&#39;: 0.1751523554524119}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:38:33,677] Trial 53 finished with value: 0.7466349228383787 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.0514736394730961, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.0767436692869214, &#39;XGB__alpha&#39;: 0.214180392959399}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:38:37,924] Trial 54 finished with value: 0.7190944180194143 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.03337645744644345, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.05913424297922879, &#39;XGB__alpha&#39;: 0.15423402011455528}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:38:40,781] Trial 55 finished with value: 0.7468496377072381 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.039239731279332474, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.11283408075502636, &#39;XGB__alpha&#39;: 0.35350711764760473}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:38:44,308] Trial 56 finished with value: 0.7445338214891984 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 4, &#39;XGB__reg_lambda&#39;: 0.06951948657448945, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.07579535009388556, &#39;XGB__alpha&#39;: 0.1418728643454859}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:38:47,106] Trial 57 finished with value: 0.7462297051251388 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.04410687750522721, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.0508340446344279, &#39;XGB__alpha&#39;: 0.18815614644421041}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:38:49,854] Trial 58 finished with value: 0.7448946307788482 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.058434100860877275, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.039313611750926424, &#39;XGB__alpha&#39;: 0.25647693134212557}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:38:53,095] Trial 59 finished with value: 0.7150871223997081 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 6, &#39;XGB__reg_lambda&#39;: 0.0864070279677493, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.17258631267178612, &#39;XGB__alpha&#39;: 0.22933600597292816}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:39:05,956] Trial 60 finished with value: 0.6099171618726859 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 30, &#39;XGB__reg_lambda&#39;: 0.04976530673521354, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.08450894380098652, &#39;XGB__alpha&#39;: 0.3014145167995276}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:39:08,656] Trial 61 finished with value: 0.7473568399143744 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06817504225300892, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.12150116595763831, &#39;XGB__alpha&#39;: 0.15749815056591468}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:39:11,402] Trial 62 finished with value: 0.7479655084701985 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06574532508677182, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.13028532869434228, &#39;XGB__alpha&#39;: 0.1232319233433959}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:39:14,350] Trial 63 finished with value: 0.7466459277920497 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06643444768126604, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.12641131279648093, &#39;XGB__alpha&#39;: 0.11991552761236997}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:39:17,199] Trial 64 finished with value: 0.7453176287575637 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.07964754804946846, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.20977895920329198, &#39;XGB__alpha&#39;: 0.14672640467815318}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:39:19,945] Trial 65 finished with value: 0.7471539072169991 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.1014090853098601, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.10542162202212031, &#39;XGB__alpha&#39;: 0.1295341035282288}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:39:29,650] Trial 66 finished with value: 0.6073785601109606 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 15, &#39;XGB__reg_lambda&#39;: 0.05727775587478458, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.13911469175388577, &#39;XGB__alpha&#39;: 0.10662040636323034}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:39:38,361] Trial 67 finished with value: 0.6065264759473278 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.0758975414668839, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.1621375203006751, &#39;XGB__alpha&#39;: 0.16446422029279373}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:39:56,707] Trial 68 finished with value: 0.6209834706947546 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 50, &#39;XGB__reg_lambda&#39;: 0.09110329963544489, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.024542285816949706, &#39;XGB__alpha&#39;: 0.11904442546830057}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:39:59,385] Trial 69 finished with value: 0.7444495327805806 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.05441155414720792, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.24198234391859597, &#39;XGB__alpha&#39;: 0.13043751800255415}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:40:09,524] Trial 70 finished with value: 0.6137929950417333 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 25, &#39;XGB__reg_lambda&#39;: 0.1156360867572587, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.11747220391783199, &#39;XGB__alpha&#39;: 0.2017140049795665}. Best is trial 22 with value: 0.7481239876878787.
[I 2024-03-04 18:40:12,276] Trial 71 finished with value: 0.7483393502382487 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06306169425710667, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.0970628853576034, &#39;XGB__alpha&#39;: 0.1714526220993759}. Best is trial 71 with value: 0.7483393502382487.
[I 2024-03-04 18:40:15,029] Trial 72 finished with value: 0.7480754341026191 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06549669167516738, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.09556496566539137, &#39;XGB__alpha&#39;: 0.1516640266332797}. Best is trial 71 with value: 0.7483393502382487.
[I 2024-03-04 18:40:17,874] Trial 73 finished with value: 0.7485666244964051 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.062444976756626386, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.09498009017491364, &#39;XGB__alpha&#39;: 0.15054242055078138}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:40:21,948] Trial 74 finished with value: 0.7055831621820862 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.06112561566055573, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.0889413223508684, &#39;XGB__alpha&#39;: 0.5785852353935742}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:40:24,803] Trial 75 finished with value: 0.7473123864346792 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.0758347643110084, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.09955415107257662, &#39;XGB__alpha&#39;: 0.1435305397087809}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:40:27,760] Trial 76 finished with value: 0.7458832687330162 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 4, &#39;XGB__reg_lambda&#39;: 0.06505299790135136, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.06050623044564784, &#39;XGB__alpha&#39;: 0.1734773818584071}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:40:41,648] Trial 77 finished with value: 0.6098113363458295 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.0537347370859495, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.0824134995623681, &#39;XGB__alpha&#39;: 0.12511890008157872}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:40:46,516] Trial 78 finished with value: 0.6820981630152861 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.07309116934926503, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.07045270886737273, &#39;XGB__alpha&#39;: 0.15382141159469354}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:40:49,792] Trial 79 finished with value: 0.7452935462693873 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.10513853105945693, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.1458598466757449, &#39;XGB__alpha&#39;: 0.13533654047834778}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:40:53,669] Trial 80 finished with value: 0.7268880372233716 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 6, &#39;XGB__reg_lambda&#39;: 0.08676633846530954, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.09596413959489242, &#39;XGB__alpha&#39;: 0.11059748725594294}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:40:57,877] Trial 81 finished with value: 0.7463736819089893 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.04704274934476805, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.1050176895323302, &#39;XGB__alpha&#39;: 0.16814833781189195}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:41:00,707] Trial 82 finished with value: 0.7484043042457652 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.060222921951578204, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.08010146319103603, &#39;XGB__alpha&#39;: 0.14894203261882047}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:41:03,502] Trial 83 finished with value: 0.7472561507834997 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.05979803578166781, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.06718755658078722, &#39;XGB__alpha&#39;: 0.15030432849767997}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:41:06,202] Trial 84 finished with value: 0.74727216259685 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.07354843236285037, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.08095887594250305, &#39;XGB__alpha&#39;: 0.13949543303842477}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:41:08,874] Trial 85 finished with value: 0.7477100516183016 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06373621672241499, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.09192989976657995, &#39;XGB__alpha&#39;: 0.12038910013206772}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:41:11,520] Trial 86 finished with value: 0.7464148125009334 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.08154620741065362, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.1321144126417267, &#39;XGB__alpha&#39;: 0.11701997263443321}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:41:14,300] Trial 87 finished with value: 0.7458874983748828 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.061832440080046805, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.10737341656900365, &#39;XGB__alpha&#39;: 0.42940393138743566}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:41:16,871] Trial 88 finished with value: 0.7476816099519565 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.096555962135461, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.09342006806844207, &#39;XGB__alpha&#39;: 0.16259021652549957}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:41:28,528] Trial 89 finished with value: 0.6104604878120871 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 30, &#39;XGB__reg_lambda&#39;: 0.05472020753834042, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.11376325720636349, &#39;XGB__alpha&#39;: 0.18001651692094514}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:41:31,222] Trial 90 finished with value: 0.7447000278289032 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.015637255306519115, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.047816066689876285, &#39;XGB__alpha&#39;: 0.1470650022386367}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:41:33,843] Trial 91 finished with value: 0.7474504946608403 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.09749897915920167, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.09195014975816326, &#39;XGB__alpha&#39;: 0.16306431088647771}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:41:36,637] Trial 92 finished with value: 0.7472599918164596 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.13391445472790153, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.07294922729821708, &#39;XGB__alpha&#39;: 0.18757034514830398}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:41:39,505] Trial 93 finished with value: 0.7476126853751781 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06956937906157983, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.09629200661774759, &#39;XGB__alpha&#39;: 0.1337184565454888}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:41:48,464] Trial 94 finished with value: 0.6264627830601931 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 15, &#39;XGB__reg_lambda&#39;: 0.08994338144636951, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.061933951534260456, &#39;XGB__alpha&#39;: 0.12429657035759333}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:41:51,250] Trial 95 finished with value: 0.7478117770395936 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.17520346653346425, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.08303810572910371, &#39;XGB__alpha&#39;: 0.15769811844276516}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:41:53,662] Trial 96 finished with value: 0.747146743296288 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.21585570546948513, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.08159763833129761, &#39;XGB__alpha&#39;: 0.10517454050621301}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:42:08,616] Trial 97 finished with value: 0.6125642643675077 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 50, &#39;XGB__reg_lambda&#39;: 0.17026527308889616, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.0570969940568207, &#39;XGB__alpha&#39;: 0.153003465302275}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:42:15,717] Trial 98 finished with value: 0.6069161999924261 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 25, &#39;XGB__reg_lambda&#39;: 0.2696944530719304, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.46864968529653545, &#39;XGB__alpha&#39;: 0.17220431819460935}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:42:25,638] Trial 99 finished with value: 0.6066799483053283 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.04206897297920566, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.14836338949175162, &#39;XGB__alpha&#39;: 0.14184656207714802}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:42:28,422] Trial 100 finished with value: 0.7471058717769482 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06390291012246378, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.07623227253814996, &#39;XGB__alpha&#39;: 0.1900307807497842}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:42:31,301] Trial 101 finished with value: 0.747312386434679 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.07996550281356034, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.08759256049978162, &#39;XGB__alpha&#39;: 0.22173809697468266}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:42:33,890] Trial 102 finished with value: 0.7471943901274324 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.16927690152106958, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.10129995616757165, &#39;XGB__alpha&#39;: 0.1628313058132647}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:42:36,468] Trial 103 finished with value: 0.7470927942424425 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.1862992377406742, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.1182837052149925, &#39;XGB__alpha&#39;: 0.13439347337762653}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:42:39,139] Trial 104 finished with value: 0.7471377658673465 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.050769020765208714, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.09038133049710115, &#39;XGB__alpha&#39;: 0.15871033815380087}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:42:42,013] Trial 105 finished with value: 0.7482906671166871 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.07090725330426241, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.06857095848193204, &#39;XGB__alpha&#39;: 0.17807000678213075}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:42:56,348] Trial 106 finished with value: 0.6090040942707986 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.0574839256340642, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.06887337438261006, &#39;XGB__alpha&#39;: 0.20577106693437763}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:43:00,658] Trial 107 finished with value: 0.7070888865264471 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.06348500179853134, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.07981325023391238, &#39;XGB__alpha&#39;: 0.1754584248811781}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:43:05,878] Trial 108 finished with value: 0.6832017334615541 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.2486802245033311, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.06409030334393868, &#39;XGB__alpha&#39;: 0.11393041792125692}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:43:08,846] Trial 109 finished with value: 0.7422998778461407 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 4, &#39;XGB__reg_lambda&#39;: 0.07014252113813949, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.12758784478905746, &#39;XGB__alpha&#39;: 0.3344600682249584}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:43:11,457] Trial 110 finished with value: 0.7468901206176713 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.03952666732085631, &#39;XGB__n_estimators&#39;: 150, &#39;XGB__eta&#39;: 0.11125309165449311, &#39;XGB__alpha&#39;: 0.12081724335855984}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:43:14,254] Trial 111 finished with value: 0.7475517019369237 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.12415521614826996, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.09666271080002609, &#39;XGB__alpha&#39;: 0.1481353086419236}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:43:16,931] Trial 112 finished with value: 0.7466112289591087 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.07272544455566761, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.05333148074067638, &#39;XGB__alpha&#39;: 0.1565589344209242}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:43:19,689] Trial 113 finished with value: 0.7470116988852737 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.08244370837169587, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.10672449274649283, &#39;XGB__alpha&#39;: 0.18152686106490032}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:43:22,522] Trial 114 finished with value: 0.7463667770608825 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.056791758991542186, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.07328936343058497, &#39;XGB__alpha&#39;: 0.16774661004862895}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:43:25,570] Trial 115 finished with value: 0.7475191601650145 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06696372436070347, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.0835925261559208, &#39;XGB__alpha&#39;: 0.14371821255160824}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:43:28,947] Trial 116 finished with value: 0.732053500024702 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 6, &#39;XGB__reg_lambda&#39;: 0.0245298908302796, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.08979351911694726, &#39;XGB__alpha&#39;: 0.19472897264427472}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:43:32,570] Trial 117 finished with value: 0.7441934282471726 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.04834453793753006, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.1336626527602728, &#39;XGB__alpha&#39;: 0.10009311289766398}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:43:35,068] Trial 118 finished with value: 0.746752530536719 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.07566533200935519, &#39;XGB__n_estimators&#39;: 130, &#39;XGB__eta&#39;: 0.10082557528255047, &#39;XGB__alpha&#39;: 0.12882492261100792}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:43:37,730] Trial 119 finished with value: 0.7463590949949629 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.05234561155985856, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.06960482517511883, &#39;XGB__alpha&#39;: 0.1374360894030954}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:43:40,586] Trial 120 finished with value: 0.7475313309454048 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.09322903194032514, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.12165736574774587, &#39;XGB__alpha&#39;: 0.27539928685253895}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:43:43,352] Trial 121 finished with value: 0.7476208855863065 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06778759560248757, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.0957194027313153, &#39;XGB__alpha&#39;: 0.13318117311686714}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:43:46,189] Trial 122 finished with value: 0.747247691499767 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06140850771684247, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.07779398357644847, &#39;XGB__alpha&#39;: 0.1501022644803074}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:43:58,013] Trial 123 finished with value: 0.6078303038847307 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 30, &#39;XGB__reg_lambda&#39;: 0.06972222262099727, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.08666974987484749, &#39;XGB__alpha&#39;: 0.1630588256660892}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:44:00,849] Trial 124 finished with value: 0.7465777804330843 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06382705286940041, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.11260938930406302, &#39;XGB__alpha&#39;: 0.8861720536846119}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:44:03,619] Trial 125 finished with value: 0.7472918859068581 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.08345136432776792, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.09989933774765332, &#39;XGB__alpha&#39;: 0.13163547692323976}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:44:11,424] Trial 126 finished with value: 0.6079759646405094 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 15, &#39;XGB__reg_lambda&#39;: 0.07787139154168267, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.1618541949117714, &#39;XGB__alpha&#39;: 0.1569976252456887}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:44:14,606] Trial 127 finished with value: 0.7458544384577648 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.05540982748835736, &#39;XGB__n_estimators&#39;: 250, &#39;XGB__eta&#39;: 0.09532417421722221, &#39;XGB__alpha&#39;: 0.1406917216750128}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:44:33,758] Trial 128 finished with value: 0.6093807407813914 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 50, &#39;XGB__reg_lambda&#39;: 0.07104756503014482, &#39;XGB__n_estimators&#39;: 300, &#39;XGB__eta&#39;: 0.05686259920847728, &#39;XGB__alpha&#39;: 0.17454138571202354}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:44:36,549] Trial 129 finished with value: 0.7460796851911914 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.10645597306960675, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.048308567111187375, &#39;XGB__alpha&#39;: 0.12394413888601218}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:44:46,524] Trial 130 finished with value: 0.6118366927503217 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 20, &#39;XGB__reg_lambda&#39;: 0.059260664653132164, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.08016170377573649, &#39;XGB__alpha&#39;: 0.1495947811309827}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:44:49,431] Trial 131 finished with value: 0.7291881513887836 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.0674319744620487, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.010003370698527561, &#39;XGB__alpha&#39;: 0.13442576979197465}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:44:52,102] Trial 132 finished with value: 0.7475923143836591 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.07741165922128965, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.09362713808228018, &#39;XGB__alpha&#39;: 0.11026703579305458}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:44:54,865] Trial 133 finished with value: 0.7476006441310897 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06604786224011712, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.0915267491350882, &#39;XGB__alpha&#39;: 0.1285414584827138}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:44:57,689] Trial 134 finished with value: 0.7469956870719233 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.07128764935968261, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.10794644928422825, &#39;XGB__alpha&#39;: 0.168379774025154}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:45:09,130] Trial 135 finished with value: 0.6098229889810114 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 25, &#39;XGB__reg_lambda&#39;: 0.05949544426311844, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.08531053484907533, &#39;XGB__alpha&#39;: 0.11877651818004169}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:45:11,756] Trial 136 finished with value: 0.747101901207686 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.09017632779426227, &#39;XGB__n_estimators&#39;: 170, &#39;XGB__eta&#39;: 0.07099916525149347, &#39;XGB__alpha&#39;: 0.5330668173473873}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:45:14,682] Trial 137 finished with value: 0.7484777175370144 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.05215946637994822, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.06426124910529632, &#39;XGB__alpha&#39;: 0.13752054182363563}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:45:30,945] Trial 138 finished with value: 0.6113950922322126 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 40, &#39;XGB__reg_lambda&#39;: 0.046422062115938856, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.06151124927446024, &#39;XGB__alpha&#39;: 0.24400423869544408}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:45:35,336] Trial 139 finished with value: 0.7076045480173162 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 8, &#39;XGB__reg_lambda&#39;: 0.053770612202956376, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.07728153113507244, &#39;XGB__alpha&#39;: 0.14589333590085157}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:45:38,264] Trial 140 finished with value: 0.7478525190226314 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.050924105929536174, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.06456876743258493, &#39;XGB__alpha&#39;: 0.15920748583515637}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:45:41,168] Trial 141 finished with value: 0.7472598622801575 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.06303895491643961, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.06674097453537194, &#39;XGB__alpha&#39;: 0.15772230143387186}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:45:44,154] Trial 142 finished with value: 0.7364997660123821 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.03550006493447954, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.32204910228315403, &#39;XGB__alpha&#39;: 0.1841241882022528}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:45:47,074] Trial 143 finished with value: 0.7479297733468403 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.04999950258314469, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.06345507136150967, &#39;XGB__alpha&#39;: 0.14121276717111236}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:45:52,470] Trial 144 finished with value: 0.6883883444722163 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 10, &#39;XGB__reg_lambda&#39;: 0.042238374421414704, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.051842107468763415, &#39;XGB__alpha&#39;: 0.1536942461144856}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:45:55,509] Trial 145 finished with value: 0.7474262826363617 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.05129633529976678, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.059210990798221626, &#39;XGB__alpha&#39;: 0.16607748439808875}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:45:58,677] Trial 146 finished with value: 0.7452830594609248 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 4, &#39;XGB__reg_lambda&#39;: 0.04986259450742146, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.04156931281108814, &#39;XGB__alpha&#39;: 0.14001806945563627}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:46:01,694] Trial 147 finished with value: 0.747377729051102 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.04469921178731086, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.06563579055264636, &#39;XGB__alpha&#39;: 0.17713860761857217}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:46:04,552] Trial 148 finished with value: 0.748274137158128 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.05591849382486093, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.0736507526160206, &#39;XGB__alpha&#39;: 0.15995591834126519}. Best is trial 73 with value: 0.7485666244964051.
[I 2024-03-04 18:46:07,346] Trial 149 finished with value: 0.7473652991981071 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;XGB__max_depth&#39;: 3, &#39;XGB__reg_lambda&#39;: 0.019711155083403112, &#39;XGB__n_estimators&#39;: 200, &#39;XGB__eta&#39;: 0.07103879538571482, &#39;XGB__alpha&#39;: 0.1456574579312441}. Best is trial 73 with value: 0.7485666244964051.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;logistic_reg&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_logistic_regression_refined</span>

<span class="k">for</span> <span class="n">sampler_name</span> <span class="ow">in</span> <span class="n">samplers_keys</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sampler_name</span><span class="p">)</span>
    
    <span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">imb_pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">],</span>  
                                <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                                <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                                <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                                <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                                <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                                <span class="n">n_trials</span><span class="o">=</span><span class="mi">75</span><span class="p">,</span> 
                                <span class="n">random_state</span><span class="o">=</span><span class="mi">777</span><span class="p">)</span>

    <span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

    <span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
    <span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
    <span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 18:46:07,412] A new study created in memory with name: no-name-5f543a38-4fba-48a1-9386-5587a52a22d1
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>random_under_sampler
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:46:12,500] Trial 0 finished with value: 0.7437523458742721 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.1482727551238745, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.5333213678634535}. Best is trial 0 with value: 0.7437523458742721.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:46:15,422] Trial 1 finished with value: 0.7445200117929849 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.8378490089028552, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.7445200117929849.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:46:20,375] Trial 2 finished with value: 0.713249300768673 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.005443676914570363, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.7445200117929849.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:46:25,435] Trial 3 finished with value: 0.7426642690961459 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.11333371543784806, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.1791096330811389}. Best is trial 1 with value: 0.7445200117929849.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:46:30,178] Trial 4 finished with value: 0.7423761409360375 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.09590818476577324, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.34089488336985335}. Best is trial 1 with value: 0.7445200117929849.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:46:35,024] Trial 5 finished with value: 0.727221790321908 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.009477300214435156, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.7445200117929849.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:46:38,552] Trial 6 finished with value: 0.7420594415732816 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.06942802043184129, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.7445200117929849.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 18:46:39,868] Trial 7 finished with value: 0.7373921190699492 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.03047016998795255, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.7445200117929849.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:46:44,414] Trial 8 finished with value: 0.7411180618413516 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.05533721092015302, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.7445200117929849.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:46:49,282] Trial 9 finished with value: 0.7430744936690653 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.5747437662789773, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.2656574440259526}. Best is trial 1 with value: 0.7445200117929849.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:46:50,270] Trial 10 finished with value: 0.7300556391083478 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.0011669335568244021, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.7445200117929849.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:46:52,175] Trial 11 finished with value: 0.7436150148659241 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.733974568541504, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.7445200117929849.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:46:55,917] Trial 12 finished with value: 0.7445362826789396 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.5480757235192864, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.9903788995959669}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:46:58,175] Trial 13 finished with value: 0.7441423289919747 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.3957442674622171, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7445362826789396.
[I 2024-03-04 18:47:00,162] Trial 14 finished with value: 0.7441018460815414 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.281105302823347, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.9892909742121437}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:47:03,160] Trial 15 finished with value: 0.7444428870050781 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.9219960776440266, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:47:04,947] Trial 16 finished with value: 0.7429372921970193 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.6517830884435258, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7445362826789396.
[I 2024-03-04 18:47:07,465] Trial 17 finished with value: 0.7440325328958567 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.20201029272896606, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.105398970976356}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:47:09,173] Trial 18 finished with value: 0.7414663962219015 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.022757884528509707, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7445362826789396.
[I 2024-03-04 18:47:11,174] Trial 19 finished with value: 0.7441343878534509 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.959409104898953, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.9384347835531075}. Best is trial 12 with value: 0.7445362826789396.
[I 2024-03-04 18:47:13,413] Trial 20 finished with value: 0.7443979153801741 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.18877185818053327, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.5636627225352875}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:47:16,288] Trial 21 finished with value: 0.7444428870050781 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.9497832981763734, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:47:18,862] Trial 22 finished with value: 0.7444428870050781 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.9575473830316739, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:47:21,040] Trial 23 finished with value: 0.7441546293086674 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.8174131960884983, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:47:23,387] Trial 24 finished with value: 0.7441544997723654 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.40049505296220533, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:47:25,713] Trial 25 finished with value: 0.7441830709750125 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.1141746988828303, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:47:27,504] Trial 26 finished with value: 0.7435906733051431 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.1561329228657553, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:47:29,892] Trial 27 finished with value: 0.743846000620738 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.47963203902434215, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 18:47:31,604] Trial 28 finished with value: 0.7444670990295569 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.22035016612515268, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 18:47:33,010] Trial 29 finished with value: 0.7444264865828213 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.18480067798964775, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 18:47:34,766] Trial 30 finished with value: 0.7442358542021384 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.32782644939092775, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 18:47:37,791] Trial 31 finished with value: 0.7443942038835164 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 1.2940628159597931, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 18:47:40,700] Trial 32 finished with value: 0.744308878884481 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.7835366473764166, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:47:44,387] Trial 33 finished with value: 0.7444713286714233 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.3172059897931483, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.5986069679380058}. Best is trial 12 with value: 0.7445362826789396.
[I 2024-03-04 18:47:46,636] Trial 34 finished with value: 0.7436265379648036 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.1094121712169851, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.6250317764526203}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:47:51,655] Trial 35 finished with value: 0.7434073343814736 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.6505566607394719, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.7111112048401491}. Best is trial 12 with value: 0.7445362826789396.
[I 2024-03-04 18:47:54,389] Trial 36 finished with value: 0.7438544599044707 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.2862736969570752, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.40634892297836434}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:47:59,526] Trial 37 finished with value: 0.7314866998545431 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.012714443286233522, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.77679829270347}. Best is trial 12 with value: 0.7445362826789396.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 18:48:01,018] Trial 38 finished with value: 0.7445726654838087 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.23747984802683375, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:48:06,283] Trial 39 finished with value: 0.7220667298488429 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.0037480519847342612, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.45608736943983796}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 18:48:07,827] Trial 40 finished with value: 0.7428837317520803 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.08231239960850328, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 18:48:09,245] Trial 41 finished with value: 0.7388127774878793 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.0354807786121657, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 18:48:10,829] Trial 42 finished with value: 0.7441462995612369 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.1430122964971951, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 18:48:12,336] Trial 43 finished with value: 0.7416457645763089 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.055664143400055294, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 18:48:13,819] Trial 44 finished with value: 0.744394074347214 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.24174887524238722, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:48:18,978] Trial 45 finished with value: 0.7430744936690653 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.4861312562553433, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.2519052036344608}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 18:48:22,268] Trial 46 finished with value: 0.7443292498759999 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 1.3245268039481004, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:48:25,315] Trial 47 finished with value: 0.7442682664377456 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.8192639811255041, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.7231254104113634}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 18:48:27,139] Trial 48 finished with value: 0.7444022745583426 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.33574255192426744, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
[I 2024-03-04 18:48:29,404] Trial 49 finished with value: 0.7431928785852184 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.13686956563565053, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.883892542731162}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:48:34,710] Trial 50 finished with value: 0.7438457415481338 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 1.49399811462772, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:48:37,632] Trial 51 finished with value: 0.7445321825733755 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.7100851589311752, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:48:39,717] Trial 52 finished with value: 0.7437973174991761 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.547540446240417, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:48:41,966] Trial 53 finished with value: 0.7440166506188085 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.8642651696864025, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:48:46,622] Trial 54 finished with value: 0.7444348163302519 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.5637110342031113, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.46367419360977563}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:48:48,783] Trial 55 finished with value: 0.7437690053691334 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.6696429877205716, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:48:52,390] Trial 56 finished with value: 0.7443698623227354 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.8532516197294029, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.15290782811238773}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:48:55,340] Trial 57 finished with value: 0.7442967081040908 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.0652815333340786, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 18:48:56,592] Trial 58 finished with value: 0.6851973809967452 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.0028211157431861297, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:48:58,135] Trial 59 finished with value: 0.7415963042369341 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.01991475332159819, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
[I 2024-03-04 18:49:00,868] Trial 60 finished with value: 0.7442072829994912 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.42143698003553137, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.349566640040371}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:49:03,680] Trial 61 finished with value: 0.7444793993462494 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.6119374207782855, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:49:06,274] Trial 62 finished with value: 0.7442601957629194 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.0187354985166652, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:49:09,315] Trial 63 finished with value: 0.7445200117929849 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.7264849515978493, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:49:12,226] Trial 64 finished with value: 0.7444672285658589 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.5702678780327664, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:49:14,643] Trial 65 finished with value: 0.7444428870050782 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.5134670070374618, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:49:17,658] Trial 66 finished with value: 0.7445200117929849 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.8987146832822483, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:49:20,679] Trial 67 finished with value: 0.7444428870050781 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.9694675710613245, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:49:23,191] Trial 68 finished with value: 0.7441830709750126 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.9782726321923915, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:49:24,850] Trial 69 finished with value: 0.7428884795391553 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.7746250796621366, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:49:28,708] Trial 70 finished with value: 0.7437360749883175 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.975248901421465, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:49:31,715] Trial 71 finished with value: 0.7443008082096547 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.2516922705767435, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:49:34,667] Trial 72 finished with value: 0.7444550577854686 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.483822883759924, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:49:37,548] Trial 73 finished with value: 0.7441830709750125 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.147850508316914, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:49:39,660] Trial 74 finished with value: 0.7437649052635691 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.6356596967455209, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 38 with value: 0.7445726654838087.
[I 2024-03-04 18:49:39,675] A new study created in memory with name: no-name-babc5e37-5a8b-4a58-834f-b877d98dd4ad
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>random_over_sampler
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:50:11,498] Trial 0 finished with value: 0.7460290590250972 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.1482727551238745, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.5333213678634535}. Best is trial 0 with value: 0.7460290590250972.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:50:22,897] Trial 1 finished with value: 0.746106183813004 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.8378490089028552, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.746106183813004.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:50:47,525] Trial 2 finished with value: 0.7387671581814644 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.005443676914570363, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.746106183813004.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:51:18,778] Trial 3 finished with value: 0.7456880181015606 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.11333371543784806, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.1791096330811389}. Best is trial 1 with value: 0.746106183813004.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:51:49,115] Trial 4 finished with value: 0.7459072216848907 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.09590818476577324, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.34089488336985335}. Best is trial 1 with value: 0.746106183813004.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:52:18,916] Trial 5 finished with value: 0.7430369450374766 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.009477300214435156, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.746106183813004.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:52:42,792] Trial 6 finished with value: 0.7457853843446841 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.06942802043184129, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.746106183813004.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 18:52:46,131] Trial 7 finished with value: 0.7456270346633062 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.03047016998795255, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.746106183813004.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:53:16,472] Trial 8 finished with value: 0.7455865517528729 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.05533721092015302, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.746106183813004.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:53:47,777] Trial 9 finished with value: 0.7459275926764094 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.5747437662789773, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.2656574440259526}. Best is trial 1 with value: 0.746106183813004.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:53:50,230] Trial 10 finished with value: 0.7398538100602668 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.0011669335568244021, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.746106183813004.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:53:57,397] Trial 11 finished with value: 0.7459357928875378 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.733974568541504, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.746106183813004.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:54:17,068] Trial 12 finished with value: 0.7461346254793492 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.5480757235192864, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.9903788995959669}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:54:25,780] Trial 13 finished with value: 0.7460127881391424 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.3957442674622171, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
[I 2024-03-04 18:54:35,592] Trial 14 finished with value: 0.7460696714718327 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.281105302823347, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.9892909742121437}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:54:46,211] Trial 15 finished with value: 0.746106183813004 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.9219960776440266, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:54:54,897] Trial 16 finished with value: 0.7458464973192406 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.6517830884435258, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
[I 2024-03-04 18:55:06,231] Trial 17 finished with value: 0.7457611723202054 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.20201029272896606, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.105398970976356}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:55:12,836] Trial 18 finished with value: 0.7453059761223818 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.022757884528509707, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
[I 2024-03-04 18:55:21,444] Trial 19 finished with value: 0.7461346254793492 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.959409104898953, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.9384347835531075}. Best is trial 12 with value: 0.7461346254793492.
[I 2024-03-04 18:55:27,075] Trial 20 finished with value: 0.7460696714718327 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.891478952785584, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.99612705497188}. Best is trial 12 with value: 0.7461346254793492.
[I 2024-03-04 18:55:41,855] Trial 21 finished with value: 0.7460453299110518 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.9497832981763734, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.5954175523828745}. Best is trial 12 with value: 0.7461346254793492.
[I 2024-03-04 18:55:50,103] Trial 22 finished with value: 0.7455542690535678 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.18060833062394352, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.6315846336146118}. Best is trial 12 with value: 0.7461346254793492.
[I 2024-03-04 18:56:07,389] Trial 23 finished with value: 0.7460818422522232 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.8285983486832054, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.777003672395233}. Best is trial 12 with value: 0.7461346254793492.
[I 2024-03-04 18:56:17,027] Trial 24 finished with value: 0.7457328601901625 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.4389699815278586, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.41095993098309214}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:56:26,155] Trial 25 finished with value: 0.7461183545933944 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.1561329228657553, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
[I 2024-03-04 18:56:37,384] Trial 26 finished with value: 0.7458099849780693 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.2915453989458347, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.7625701016688908}. Best is trial 12 with value: 0.7461346254793492.
[I 2024-03-04 18:57:09,137] Trial 27 finished with value: 0.7460696714718327 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.0528476237025242, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.4537665831081773}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 18:57:23,271] Trial 28 finished with value: 0.7460453299110519 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.49711180681188694, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:57:31,972] Trial 29 finished with value: 0.745692247743427 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.1124562721617874, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
[I 2024-03-04 18:57:42,301] Trial 30 finished with value: 0.7458220262221577 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.171889597129476, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.22190203089246804}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:57:51,286] Trial 31 finished with value: 0.7461183545933944 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.2165343319352249, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:58:00,608] Trial 32 finished with value: 0.7461183545933944 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.2014507914395156, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:58:10,227] Trial 33 finished with value: 0.7461183545933944 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.2742982811238799, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:58:18,186] Trial 34 finished with value: 0.7460940130326135 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.6848013418929889, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:58:20,747] Trial 35 finished with value: 0.7407507363125015 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.0015855407242247822, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 18:58:29,065] Trial 36 finished with value: 0.7458140850836336 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.33268855406276193, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:59:00,907] Trial 37 finished with value: 0.7460290590250972 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 1.390340508339221, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
[I 2024-03-04 18:59:09,721] Trial 38 finished with value: 0.7455947519640013 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.10809973363990492, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.7098675914288838}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 18:59:44,268] Trial 39 finished with value: 0.7436088421795249 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.005986151126363592, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:00:19,701] Trial 40 finished with value: 0.7460453299110519 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.773783599171598, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 19:00:41,350] Trial 41 finished with value: 0.7461183545933944 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.3049586623831781, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 19:00:58,950] Trial 42 finished with value: 0.7460534005858781 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.5561810958660532, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 19:01:20,933] Trial 43 finished with value: 0.7461183545933944 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.482935509451738, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:02:17,003] Trial 44 finished with value: 0.7460412298054876 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 0.9500518817880748, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 19:02:27,566] Trial 45 finished with value: 0.746106183813004 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.950271375068764, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 12 with value: 0.7461346254793492.
[I 2024-03-04 19:02:43,241] Trial 46 finished with value: 0.7461711378205204 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.6179794889668365, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.14211360809764553}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:03:30,795] Trial 47 finished with value: 0.7453710596662004 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.020637468309890526, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.13505027389647467}. Best is trial 46 with value: 0.7461711378205204.
[I 2024-03-04 19:03:42,900] Trial 48 finished with value: 0.7457855138809862 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.24149989011576062, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.16731358524521037}. Best is trial 46 with value: 0.7461711378205204.
[I 2024-03-04 19:03:56,199] Trial 49 finished with value: 0.746106183813004 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.5943001821534761, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.10177125981312035}. Best is trial 46 with value: 0.7461711378205204.
[I 2024-03-04 19:04:00,366] Trial 50 finished with value: 0.7453109829820613 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.06753186128430802, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.32402229675642336}. Best is trial 46 with value: 0.7461711378205204.
[I 2024-03-04 19:04:11,532] Trial 51 finished with value: 0.745943863562364 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 0.4170185847145029, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.855237771152844}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l2)
  warnings.warn(
[I 2024-03-04 19:04:20,053] Trial 52 finished with value: 0.7460818422522232 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l2&#39;, &#39;logistic_reg__C&#39;: 1.0855013207134163, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:04:37,750] Trial 53 finished with value: 0.7461346254793492 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 1.5015752063297045, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:04:52,470] Trial 54 finished with value: 0.7460575006914422 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.7133585784095542, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:05:10,065] Trial 55 finished with value: 0.7461346254793492 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 1.529389712425142, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:05:29,154] Trial 56 finished with value: 0.7461346254793492 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 1.620475693688364, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:05:49,141] Trial 57 finished with value: 0.7461346254793492 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 1.5817527870804258, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:06:22,843] Trial 58 finished with value: 0.7460168882447067 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 1.9029901773621525, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 19:06:30,440] Trial 59 finished with value: 0.7460047174643162 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 1.55804447542043, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:07:11,887] Trial 60 finished with value: 0.7460818422522232 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.8643871784666168, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:08:01,130] Trial 61 finished with value: 0.7461467962597395 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 1.771801789007659, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:08:42,697] Trial 62 finished with value: 0.7460818422522232 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.8922612484866287, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:09:40,705] Trial 63 finished with value: 0.7461467962597395 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 1.9365033950020547, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:10:37,890] Trial 64 finished with value: 0.7461589670401301 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 1.9787174557061853, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 19:10:49,395] Trial 65 finished with value: 0.746020988350271 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.5048345823357404, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 19:10:57,553] Trial 66 finished with value: 0.7457082595567771 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.040178565910151176, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:11:41,882] Trial 67 finished with value: 0.7460818422522232 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.9772861823859342, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
[I 2024-03-04 19:11:55,558] Trial 68 finished with value: 0.7460940130326135 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;min-max&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 0.7173331667656059, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is &#39;elasticnet&#39;. Got (penalty=l1)
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:12:55,545] Trial 69 finished with value: 0.7461467962597395 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;l1&#39;, &#39;logistic_reg__C&#39;: 1.9336600232790366, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:14:23,571] Trial 70 finished with value: 0.7461711378205204 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.9854636696067853, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.4853690338134428}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:15:51,289] Trial 71 finished with value: 0.7461711378205204 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.8787947813654489, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.45929032826053606}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:17:18,513] Trial 72 finished with value: 0.7461589670401301 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.7016348435411985, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.48857265415218604}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:18:46,187] Trial 73 finished with value: 0.7461589670401301 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.759457003534312, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.4580112473018471}. Best is trial 46 with value: 0.7461711378205204.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  warnings.warn(
[I 2024-03-04 19:20:05,514] Trial 74 finished with value: 0.7461711378205204 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: True, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__scaler__method&#39;: &#39;standard&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;logistic_reg__penalty&#39;: &#39;elasticnet&#39;, &#39;logistic_reg__C&#39;: 1.9308423182691392, &#39;logistic_reg__class_weight&#39;: &#39;balanced&#39;, &#39;logistic_reg__l1_ratio&#39;: 0.4394234211589425}. Best is trial 46 with value: 0.7461711378205204.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;SVM&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_linear_SVM_refined</span>

<span class="k">for</span> <span class="n">sampler_name</span> <span class="ow">in</span> <span class="n">samplers_keys</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sampler_name</span><span class="p">)</span>
    
    <span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">imb_pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">],</span>  
                                <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                                <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                                <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                                <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                                <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                                <span class="n">n_trials</span><span class="o">=</span><span class="mi">75</span><span class="p">,</span> 
                                <span class="n">random_state</span><span class="o">=</span><span class="mi">777</span><span class="p">)</span>

    <span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

    <span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
    <span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
    <span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 19:20:05,541] A new study created in memory with name: no-name-945e4733-dfc9-4347-aee5-f742f9fa69ce
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>random_under_sampler
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
[I 2024-03-04 19:20:07,770] Trial 0 finished with value: 0.7413138306180157 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.0031911525679043278, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 0 with value: 0.7413138306180157.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:20:10,644] Trial 1 finished with value: 0.7424953030700037 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.009956076799679095, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.7424953030700037.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
[I 2024-03-04 19:20:12,256] Trial 2 finished with value: 0.7410581146935146 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.001602448444193617, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.7424953030700037.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:20:15,099] Trial 3 finished with value: 0.7438148837481524 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.03296194128713206, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 3 with value: 0.7438148837481524.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:20:17,825] Trial 4 finished with value: 0.7202521515360267 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.5717372336568046, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 3 with value: 0.7438148837481524.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:20:20,618] Trial 5 finished with value: 0.6952287961407464 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 1.1482727551238745, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 3 with value: 0.7438148837481524.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:20:23,415] Trial 6 finished with value: 0.7372285090883007 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.2510801871856053, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 3 with value: 0.7438148837481524.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:20:26,286] Trial 7 finished with value: 0.7335762386807518 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.3442144359020172, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 3 with value: 0.7438148837481524.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:20:29,078] Trial 8 finished with value: 0.7415332932739503 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.0077384503305894595, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 3 with value: 0.7438148837481524.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:20:32,016] Trial 9 finished with value: 0.7437551956729198 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.13364724517239146, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 3 with value: 0.7438148837481524.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:20:34,755] Trial 10 finished with value: 0.7442046077932508 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.04030003510341321, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 10 with value: 0.7442046077932508.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:20:38,059] Trial 11 finished with value: 0.7443994698158001 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.044393802171852625, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 11 with value: 0.7443994698158001.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:20:41,496] Trial 12 finished with value: 0.7442249787847697 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.04443906205432107, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 11 with value: 0.7443994698158001.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:20:44,855] Trial 13 finished with value: 0.7343373320401331 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.09456277164013807, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 11 with value: 0.7443994698158001.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:20:48,186] Trial 14 finished with value: 0.7438471664474574 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.01770068591265021, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 11 with value: 0.7443994698158001.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:20:51,917] Trial 15 finished with value: 0.7461033396463694 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.1198998432068778, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:20:55,355] Trial 16 finished with value: 0.7450349411221463 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.10242836962668075, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:20:58,861] Trial 17 finished with value: 0.7436629207696729 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.1376253528638055, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:21:02,259] Trial 18 finished with value: 0.6861649664858046 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.984978833648924, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:21:05,572] Trial 19 finished with value: 0.607960195003724 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 1.7953701492103953, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:21:08,673] Trial 20 finished with value: 0.7443151811071821 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.08476107812765651, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:21:12,004] Trial 21 finished with value: 0.7437132878631627 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.02240407744582017, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:21:15,576] Trial 22 finished with value: 0.7399792800489471 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.23182431859902392, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:21:18,787] Trial 23 finished with value: 0.7445846222476998 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.10260863357045782, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:21:21,986] Trial 24 finished with value: 0.7442562251936572 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.08466653126835351, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:21:25,121] Trial 25 finished with value: 0.7331920340085283 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.47780622142345947, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:21:28,316] Trial 26 finished with value: 0.7447338649638343 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.17205008583092163, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:21:31,527] Trial 27 finished with value: 0.7437039274573278 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.24081478350869684, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:21:35,065] Trial 28 finished with value: 0.7450026584228411 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.1625783044534282, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:21:38,546] Trial 29 finished with value: 0.7440626585341298 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.07014851309811607, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
[I 2024-03-04 19:21:41,896] Trial 30 finished with value: 0.7410582442298167 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.004873911737962179, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:21:45,420] Trial 31 finished with value: 0.743500730055335 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.14957881793286995, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:21:48,986] Trial 32 finished with value: 0.7366256696660741 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.5145366701030761, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:21:52,500] Trial 33 finished with value: 0.7451110296196357 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.18945558202361007, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:21:56,186] Trial 34 finished with value: 0.7395895897959277 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.06228918090729531, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:21:59,750] Trial 35 finished with value: 0.7437741417651148 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.01961837128334852, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:22:03,302] Trial 36 finished with value: 0.7346413593733291 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.3343880163081356, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:22:06,733] Trial 37 finished with value: 0.7447446559010059 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.1954617733894047, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
[I 2024-03-04 19:22:08,687] Trial 38 finished with value: 0.7402504840095769 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.0011938623850752024, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:22:12,134] Trial 39 finished with value: 0.6265613263940624 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.7094008687993406, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:22:15,700] Trial 40 finished with value: 0.7329328712919868 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.33378442813688924, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:22:19,189] Trial 41 finished with value: 0.7436046125376587 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.19150277349293246, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:22:22,860] Trial 42 finished with value: 0.7437621005210265 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.02864731790992644, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:22:26,250] Trial 43 finished with value: 0.744590231732785 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.05656329097584649, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:22:29,648] Trial 44 finished with value: 0.7444358526206692 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.11092976919728076, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:22:33,026] Trial 45 finished with value: 0.7291964924002402 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.3776249112954173, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:22:36,466] Trial 46 finished with value: 0.7445887674093692 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.19628859394982195, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:22:39,870] Trial 47 finished with value: 0.7423722604789855 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.2885994441805401, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:22:43,325] Trial 48 finished with value: 0.7354679812055666 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.8803134757505947, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:22:46,516] Trial 49 finished with value: 0.7430596476824345 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.012760839404144244, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:22:49,867] Trial 50 finished with value: 0.744233049459596 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.03525708121591175, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:22:53,236] Trial 51 finished with value: 0.7437397470608832 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.15080978406758522, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:22:56,190] Trial 52 finished with value: 0.743318556958385 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.17244303213437323, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:22:59,527] Trial 53 finished with value: 0.7440474239385925 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.10532250788120909, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:23:02,297] Trial 54 finished with value: 0.7441846254106386 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.058693532288267444, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:23:05,286] Trial 55 finished with value: 0.7419365679426578 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.1256065474891801, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:23:08,459] Trial 56 finished with value: 0.6869810733495143 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.4481152921600205, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:23:11,466] Trial 57 finished with value: 0.7420349310521067 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.221553824133915, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:23:14,900] Trial 58 finished with value: 0.744144790181716 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.08033712537893956, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:23:18,377] Trial 59 finished with value: 0.7437092778698086 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.2680052342330272, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:23:21,635] Trial 60 finished with value: 0.7443314519931367 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.13773715135792236, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:23:25,041] Trial 61 finished with value: 0.7445009361644878 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.0601056375022254, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:23:28,388] Trial 62 finished with value: 0.7442698208733716 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.05059930666018437, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:23:31,832] Trial 63 finished with value: 0.7442244606395612 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.09561131626995793, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:23:35,283] Trial 64 finished with value: 0.7446522514614567 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.07254614602439395, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:23:38,921] Trial 65 finished with value: 0.7427397099121246 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.17879061774200064, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:23:42,326] Trial 66 finished with value: 0.7440868705586086 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.07576396545647628, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
[I 2024-03-04 19:23:44,589] Trial 67 finished with value: 0.7419023872549255 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.002276804717652834, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:23:47,950] Trial 68 finished with value: 0.7448054253227611 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.15155815874632933, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:23:51,347] Trial 69 finished with value: 0.744933086164552 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.12061975053638546, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:23:54,645] Trial 70 finished with value: 0.7447050346885827 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.13224125596129582, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:23:58,116] Trial 71 finished with value: 0.739209761197912 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.26424763739317125, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:24:01,552] Trial 72 finished with value: 0.7393464051006576 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.21216184346604539, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:24:05,003] Trial 73 finished with value: 0.7443104783762123 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.16335434191347867, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:24:08,703] Trial 74 finished with value: 0.7446211345888712 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.11555696631547883, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 15 with value: 0.7461033396463694.
[I 2024-03-04 19:24:08,735] A new study created in memory with name: no-name-1d9031f3-49da-4562-8176-c041b26ef626
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>random_over_sampler
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
[I 2024-03-04 19:24:33,511] Trial 0 finished with value: 0.7453199604110026 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.0031911525679043278, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 0 with value: 0.7453199604110026.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:25:12,779] Trial 1 finished with value: 0.7453771028162972 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.009956076799679095, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.7453771028162972.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
[I 2024-03-04 19:25:25,368] Trial 2 finished with value: 0.7442846668600024 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.001602448444193617, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.7453771028162972.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:26:04,468] Trial 3 finished with value: 0.7451498285581409 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.03296194128713206, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.7453771028162972.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:26:46,921] Trial 4 finished with value: 0.7221526518412706 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.5717372336568046, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.7453771028162972.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:27:28,901] Trial 5 finished with value: 0.7302445875171024 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 1.1482727551238745, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.7453771028162972.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:28:12,427] Trial 6 finished with value: 0.7441914908346533 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.2510801871856053, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.7453771028162972.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:28:53,998] Trial 7 finished with value: 0.709951914772953 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.3442144359020172, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.7453771028162972.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:29:31,271] Trial 8 finished with value: 0.7449143540888561 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.0077384503305894595, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 1 with value: 0.7453771028162972.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:30:12,596] Trial 9 finished with value: 0.7468956906786645 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.13364724517239146, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:30:52,905] Trial 10 finished with value: 0.7438150977646517 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.07589169288389386, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:31:30,686] Trial 11 finished with value: 0.7451863408993122 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.01254586154891101, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:32:11,696] Trial 12 finished with value: 0.7458480517548666 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.07274608298216849, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:32:54,357] Trial 13 finished with value: 0.7453197013383983 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.09456277164013807, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:33:35,572] Trial 14 finished with value: 0.7426279707714739 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.1418953282902281, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:34:19,614] Trial 15 finished with value: 0.7450848745506246 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.032160003123633346, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:34:59,465] Trial 16 finished with value: 0.745441538678605 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.04181317557297847, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:35:42,094] Trial 17 finished with value: 0.6873756578219504 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 1.4314604057893596, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:36:24,478] Trial 18 finished with value: 0.7231377923152803 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.19449231776975623, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:37:04,590] Trial 19 finished with value: 0.7450605329898435 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.018431035967409855, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:37:48,371] Trial 20 finished with value: 0.7459458066068967 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.08231591020850587, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:38:43,049] Trial 21 finished with value: 0.7451818521848415 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.08359618645146624, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:39:43,663] Trial 22 finished with value: 0.7456123182129776 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.07161932827333894, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:40:49,107] Trial 23 finished with value: 0.7343532706373126 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.521500265862932, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:41:54,269] Trial 24 finished with value: 0.7454437407957416 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.13190163766650445, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:42:59,994] Trial 25 finished with value: 0.7451376577777505 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.02280374194629999, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:44:06,619] Trial 26 finished with value: 0.745592206294063 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.05513351433253543, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:45:11,505] Trial 27 finished with value: 0.7242957567444314 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.5187835462728899, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:46:15,993] Trial 28 finished with value: 0.724672391990998 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.26752810135914884, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
[I 2024-03-04 19:46:57,848] Trial 29 finished with value: 0.7452957483865239 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.004197776480451818, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:47:45,065] Trial 30 finished with value: 0.7459705367765839 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.13284846498477917, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:48:38,187] Trial 31 finished with value: 0.7436744438685524 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.15536061843085908, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:49:30,098] Trial 32 finished with value: 0.7459099870033411 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.11127920566755584, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 9 with value: 0.7468956906786645.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:50:20,432] Trial 33 finished with value: 0.7469484739057904 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.1155137267169086, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:51:11,974] Trial 34 finished with value: 0.6453561145561385 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.8758617621834202, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:52:03,149] Trial 35 finished with value: 0.7331881929755685 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.31703763485341235, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:52:53,931] Trial 36 finished with value: 0.745527511359151 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.04990228463881838, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:53:43,428] Trial 37 finished with value: 0.7454042096955286 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.1954617733894047, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
[I 2024-03-04 19:53:57,001] Trial 38 finished with value: 0.7446701612632344 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.0011938623850752024, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:54:48,965] Trial 39 finished with value: 0.7452756364676095 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.0355919757211874, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:55:36,958] Trial 40 finished with value: 0.6743442213438836 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.6811196298964369, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:56:27,808] Trial 41 finished with value: 0.7447246284622886 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.11423273949071658, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:57:19,972] Trial 42 finished with value: 0.7450618283528653 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.10836481631103069, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:58:14,111] Trial 43 finished with value: 0.7255283453476564 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.38092574380550165, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 19:59:12,577] Trial 44 finished with value: 0.7325366197436619 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.22454777177843532, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:00:13,322] Trial 45 finished with value: 0.7451740405826195 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.05825074892888417, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:01:18,019] Trial 46 finished with value: 0.7453121488087809 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.02583734926909646, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:02:20,520] Trial 47 finished with value: 0.7451145721558993 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.15682242056463302, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:03:26,188] Trial 48 finished with value: 0.7222904615707356 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.3989246850998496, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:04:29,267] Trial 49 finished with value: 0.7452756364676095 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.012995591247780654, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:05:34,006] Trial 50 finished with value: 0.7449663600981692 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.09984667575899188, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:06:38,816] Trial 51 finished with value: 0.7459173649405514 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.07657184624116133, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:07:44,057] Trial 52 finished with value: 0.7463514185610562 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.16623825754856536, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:08:46,301] Trial 53 finished with value: 0.7455879766521966 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.075011979651317, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:09:52,135] Trial 54 finished with value: 0.7457042045073182 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.17469965608582308, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:10:57,259] Trial 55 finished with value: 0.741844467631818 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.25846270471267796, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:12:00,580] Trial 56 finished with value: 0.7458320399415163 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.06569580526119753, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:13:04,088] Trial 57 finished with value: 0.7454743395231184 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.041277869900255636, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:13:56,122] Trial 58 finished with value: 0.7450843564054158 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.08228093694724678, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:14:29,255] Trial 59 finished with value: 0.7465224515360201 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.13940697745843947, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:15:00,618] Trial 60 finished with value: 0.7408093036171198 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.21625329402533652, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:15:31,109] Trial 61 finished with value: 0.7459657045093119 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.13350802493017971, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:16:13,169] Trial 62 finished with value: 0.7427405716101347 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.1338845799636253, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:16:52,182] Trial 63 finished with value: 0.7285098148304469 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.28701665463508597, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:17:25,112] Trial 64 finished with value: 0.7444541510313534 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.1564793273191976, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:17:55,126] Trial 65 finished with value: 0.7441346469260551 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.1263256482318461, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:18:39,954] Trial 66 finished with value: 0.7447610112671578 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.09678177593917016, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:19:45,100] Trial 67 finished with value: 0.7163502139860863 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.40882120269933747, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:20:49,529] Trial 68 finished with value: 0.7440067664357519 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.19378565742309914, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:21:56,464] Trial 69 finished with value: 0.7457344146257885 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.050396308569176713, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:22:56,271] Trial 70 finished with value: 0.7443894560964414 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.13674173612544915, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:23:44,119] Trial 71 finished with value: 0.7452869004938848 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.08610837041342441, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:24:32,910] Trial 72 finished with value: 0.7458831842528192 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.0934525214398405, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:25:22,031] Trial 73 finished with value: 0.7455963063996273 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.06677748281728367, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
[I 2024-03-04 20:26:09,344] Trial 74 finished with value: 0.7451619993385313 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;SVM__C&#39;: 0.03216267985668896, &#39;SVM__class_weight&#39;: &#39;balanced&#39;}. Best is trial 33 with value: 0.7469484739057904.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;RF&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_RF_refined</span>

<span class="k">for</span> <span class="n">sampler_name</span> <span class="ow">in</span> <span class="n">samplers_keys</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sampler_name</span><span class="p">)</span>
    
    <span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">imb_pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">],</span>  
                                <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                                <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                                <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                                <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                                <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                                <span class="n">n_trials</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
                                <span class="n">random_state</span><span class="o">=</span><span class="mi">777</span><span class="p">)</span>

    <span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

    <span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
    <span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
    <span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 20:26:09,400] A new study created in memory with name: no-name-c5d36070-8130-4c6f-b04f-5556b3793729
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>random_under_sampler
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 20:26:17,248] Trial 0 finished with value: 0.7363141010672868 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 13, &#39;RF__min_samples_leaf&#39;: 14, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 0 with value: 0.7363141010672868.
[I 2024-03-04 20:26:23,319] Trial 1 finished with value: 0.727890237065175 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 5, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 0 with value: 0.7363141010672868.
[I 2024-03-04 20:26:25,959] Trial 2 finished with value: 0.7320721532522138 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 8, &#39;RF__min_samples_leaf&#39;: 10, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 0 with value: 0.7363141010672868.
[I 2024-03-04 20:26:28,669] Trial 3 finished with value: 0.7307888258426319 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 3, &#39;RF__min_samples_leaf&#39;: 18, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 0 with value: 0.7363141010672868.
[I 2024-03-04 20:26:33,104] Trial 4 finished with value: 0.7343195179825788 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 100, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 19, &#39;RF__min_samples_leaf&#39;: 20, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 0 with value: 0.7363141010672868.
[I 2024-03-04 20:26:36,777] Trial 5 finished with value: 0.7375490044279113 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 8, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7375490044279113.
[I 2024-03-04 20:26:42,584] Trial 6 finished with value: 0.7247774515640675 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 4, &#39;RF__min_samples_split&#39;: 18, &#39;RF__min_samples_leaf&#39;: 4, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7375490044279113.
[I 2024-03-04 20:26:46,727] Trial 7 finished with value: 0.7308241723570837 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 100, &#39;RF__max_depth&#39;: 5, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 5 with value: 0.7375490044279113.
[I 2024-03-04 20:26:49,755] Trial 8 finished with value: 0.7282771563677309 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 5, &#39;RF__min_samples_split&#39;: 3, &#39;RF__min_samples_leaf&#39;: 11, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7375490044279113.
[I 2024-03-04 20:26:52,885] Trial 9 finished with value: 0.7368520765942073 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 20, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7375490044279113.
[I 2024-03-04 20:26:58,863] Trial 10 finished with value: 0.739756145320447 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 8, &#39;RF__min_samples_leaf&#39;: 7, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:27:04,692] Trial 11 finished with value: 0.7372968704637654 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 7, &#39;RF__min_samples_leaf&#39;: 8, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:27:12,015] Trial 12 finished with value: 0.7381522775151494 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 7, &#39;RF__min_samples_leaf&#39;: 6, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:27:19,483] Trial 13 finished with value: 0.7381522775151494 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 6, &#39;RF__min_samples_leaf&#39;: 6, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:27:26,827] Trial 14 finished with value: 0.7389318551416485 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 7, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:27:32,097] Trial 15 finished with value: 0.7380025166538066 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 8, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:27:37,933] Trial 16 finished with value: 0.720375729168293 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 250, &#39;RF__max_depth&#39;: 3, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 6, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:27:41,753] Trial 17 finished with value: 0.7227829980156616 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 3, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 15, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:27:48,490] Trial 18 finished with value: 0.737758582900789 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 5, &#39;RF__min_samples_leaf&#39;: 9, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:27:51,907] Trial 19 finished with value: 0.725029326455609 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 4, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 5, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:27:59,169] Trial 20 finished with value: 0.737284699683375 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 8, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:28:07,238] Trial 21 finished with value: 0.7381522775151494 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 8, &#39;RF__min_samples_leaf&#39;: 6, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:28:15,039] Trial 22 finished with value: 0.7389318551416485 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 6, &#39;RF__min_samples_leaf&#39;: 7, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:28:23,941] Trial 23 finished with value: 0.736277200117209 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 5, &#39;RF__min_samples_leaf&#39;: 4, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:28:37,202] Trial 24 finished with value: 0.7373124936682093 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 250, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 2, &#39;RF__min_samples_leaf&#39;: 8, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:28:44,249] Trial 25 finished with value: 0.7382502914397838 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 10, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:28:50,415] Trial 26 finished with value: 0.739756145320447 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 7, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:28:56,623] Trial 27 finished with value: 0.7395246414204243 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:29:02,939] Trial 28 finished with value: 0.7395246414204243 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:29:09,338] Trial 29 finished with value: 0.7375159445107933 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 13, &#39;RF__min_samples_leaf&#39;: 14, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:29:19,868] Trial 30 finished with value: 0.739049851448895 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 250, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:29:26,170] Trial 31 finished with value: 0.7395246414204243 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:29:35,498] Trial 32 finished with value: 0.7387008693868343 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:29:41,874] Trial 33 finished with value: 0.7382537438638371 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 4, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:29:51,253] Trial 34 finished with value: 0.7382987154887412 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 8, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 10 with value: 0.739756145320447.
[I 2024-03-04 20:29:56,807] Trial 35 finished with value: 0.7398011169453511 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 100, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 5, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:30:03,003] Trial 36 finished with value: 0.7376298407124757 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 100, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 13, &#39;RF__min_samples_leaf&#39;: 5, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:30:08,545] Trial 37 finished with value: 0.7358819961233275 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 100, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 5, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:30:14,233] Trial 38 finished with value: 0.73671241393242 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 100, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 14, &#39;RF__min_samples_leaf&#39;: 10, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:30:17,398] Trial 39 finished with value: 0.7237880363920864 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 4, &#39;RF__min_samples_split&#39;: 7, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:30:20,248] Trial 40 finished with value: 0.7248152592682605 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 5, &#39;RF__min_samples_split&#39;: 4, &#39;RF__min_samples_leaf&#39;: 4, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:30:26,289] Trial 41 finished with value: 0.7382625917564765 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 100, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:30:36,431] Trial 42 finished with value: 0.7397155328737114 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:30:45,936] Trial 43 finished with value: 0.7381612549440909 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 5, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:30:50,864] Trial 44 finished with value: 0.7382664327894363 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 7, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:30:58,500] Trial 45 finished with value: 0.7332149112458936 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 17, &#39;RF__min_samples_leaf&#39;: 19, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:31:02,517] Trial 46 finished with value: 0.7380313469290581 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 14, &#39;RF__min_samples_leaf&#39;: 4, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:31:11,825] Trial 47 finished with value: 0.7372764994722466 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 8, &#39;RF__min_samples_leaf&#39;: 9, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:31:14,514] Trial 48 finished with value: 0.71936516578698 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 3, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:31:21,072] Trial 49 finished with value: 0.728851987788624 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 5, &#39;RF__min_samples_split&#39;: 7, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:31:26,958] Trial 50 finished with value: 0.7371669624487326 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 100, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 17, &#39;RF__min_samples_leaf&#39;: 16, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:31:33,797] Trial 51 finished with value: 0.7395246414204243 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:31:40,378] Trial 52 finished with value: 0.7383358755114234 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:31:48,429] Trial 53 finished with value: 0.7396996505966634 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:31:56,374] Trial 54 finished with value: 0.737930269189277 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 6, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:32:02,035] Trial 55 finished with value: 0.723729469087468 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 4, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 7, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:32:07,700] Trial 56 finished with value: 0.738911872759036 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 13, &#39;RF__min_samples_leaf&#39;: 4, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:32:11,951] Trial 57 finished with value: 0.739358739209429 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 5, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:32:14,410] Trial 58 finished with value: 0.7370325657192293 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 6, &#39;RF__min_samples_leaf&#39;: 9, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:32:17,081] Trial 59 finished with value: 0.720176378431273 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 3, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 11, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:32:19,565] Trial 60 finished with value: 0.7351872253506553 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 100, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 8, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:32:23,051] Trial 61 finished with value: 0.7380353174983201 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 7, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 35 with value: 0.7398011169453511.
[I 2024-03-04 20:32:26,640] Trial 62 finished with value: 0.7402345172523317 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 62 with value: 0.7402345172523317.
[I 2024-03-04 20:32:29,834] Trial 63 finished with value: 0.7404300269563918 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:32:35,447] Trial 64 finished with value: 0.7386445041993529 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 250, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 14, &#39;RF__min_samples_leaf&#39;: 4, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:32:38,497] Trial 65 finished with value: 0.7374475380792237 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 6, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:32:42,541] Trial 66 finished with value: 0.738891372231215 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 13, &#39;RF__min_samples_leaf&#39;: 5, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:32:44,501] Trial 67 finished with value: 0.7364777166809483 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:32:47,812] Trial 68 finished with value: 0.7374343310084157 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 4, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:32:50,623] Trial 69 finished with value: 0.7292333820862918 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 5, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:32:52,021] Trial 70 finished with value: 0.7196251113533477 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 4, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 7, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:32:55,699] Trial 71 finished with value: 0.7395246414204243 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:32:59,087] Trial 72 finished with value: 0.7381732961881792 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 8, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:33:02,509] Trial 73 finished with value: 0.7402345172523317 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:33:06,151] Trial 74 finished with value: 0.7380235353268363 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:33:09,549] Trial 75 finished with value: 0.7382537438638371 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 4, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:33:13,050] Trial 76 finished with value: 0.7380316060016624 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:33:14,823] Trial 77 finished with value: 0.7236630057004309 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 100, &#39;RF__max_depth&#39;: 3, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 5, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:33:20,291] Trial 78 finished with value: 0.7376461115984304 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 250, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 6, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:33:25,003] Trial 79 finished with value: 0.7370776668804354 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 13, &#39;RF__min_samples_leaf&#39;: 8, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:33:27,113] Trial 80 finished with value: 0.7340758433021656 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:33:30,256] Trial 81 finished with value: 0.7381732961881792 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 8, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:33:33,765] Trial 82 finished with value: 0.7402345172523317 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:33:37,041] Trial 83 finished with value: 0.7398056056598218 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:33:40,510] Trial 84 finished with value: 0.7380235353268363 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:33:43,685] Trial 85 finished with value: 0.7383358755114234 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:33:46,902] Trial 86 finished with value: 0.7395570536560315 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:33:49,326] Trial 87 finished with value: 0.730199232915305 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 5, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 4, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:33:52,473] Trial 88 finished with value: 0.7398056056598218 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:33:56,728] Trial 89 finished with value: 0.7387856762406612 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 7, &#39;RF__min_samples_leaf&#39;: 4, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:33:59,960] Trial 90 finished with value: 0.7402345172523317 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:34:03,431] Trial 91 finished with value: 0.7402345172523317 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:34:06,959] Trial 92 finished with value: 0.7398056056598218 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:34:10,147] Trial 93 finished with value: 0.7398056056598218 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:34:13,608] Trial 94 finished with value: 0.7398056056598218 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:34:17,078] Trial 95 finished with value: 0.7404300269563918 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:34:21,314] Trial 96 finished with value: 0.7396828615655 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:34:24,448] Trial 97 finished with value: 0.7380235353268363 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:34:26,726] Trial 98 finished with value: 0.7252277704385137 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 4, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:34:29,876] Trial 99 finished with value: 0.7395569241197294 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 63 with value: 0.7404300269563918.
[I 2024-03-04 20:34:29,978] A new study created in memory with name: no-name-ccb0ad14-c627-46dd-b895-8163fee998f6
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>random_over_sampler
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 20:34:47,691] Trial 0 finished with value: 0.7348894213919763 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 13, &#39;RF__min_samples_leaf&#39;: 14, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 0 with value: 0.7348894213919763.
[I 2024-03-04 20:35:07,694] Trial 1 finished with value: 0.7275476417060124 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 5, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 0 with value: 0.7348894213919763.
[I 2024-03-04 20:35:13,665] Trial 2 finished with value: 0.7320764673742773 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 8, &#39;RF__min_samples_leaf&#39;: 10, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 0 with value: 0.7348894213919763.
[I 2024-03-04 20:35:19,429] Trial 3 finished with value: 0.7333845249535466 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 3, &#39;RF__min_samples_leaf&#39;: 18, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 0 with value: 0.7348894213919763.
[I 2024-03-04 20:35:32,775] Trial 4 finished with value: 0.7348115700743748 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 100, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 19, &#39;RF__min_samples_leaf&#39;: 20, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 0 with value: 0.7348894213919763.
[I 2024-03-04 20:35:42,038] Trial 5 finished with value: 0.7407159079432585 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 8, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:35:58,561] Trial 6 finished with value: 0.7255105031300365 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 4, &#39;RF__min_samples_split&#39;: 18, &#39;RF__min_samples_leaf&#39;: 4, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:36:09,492] Trial 7 finished with value: 0.7301883124418311 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 100, &#39;RF__max_depth&#39;: 5, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:36:16,104] Trial 8 finished with value: 0.7281227321995098 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 5, &#39;RF__min_samples_split&#39;: 3, &#39;RF__min_samples_leaf&#39;: 11, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:36:23,238] Trial 9 finished with value: 0.7377391580874774 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 20, &#39;RF__min_samples_leaf&#39;: 3, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:36:43,755] Trial 10 finished with value: 0.7375724786586689 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 8, &#39;RF__min_samples_leaf&#39;: 7, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:36:57,445] Trial 11 finished with value: 0.7370492646381823 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 7, &#39;RF__min_samples_leaf&#39;: 7, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:37:10,854] Trial 12 finished with value: 0.7355731872109778 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 15, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:37:28,156] Trial 13 finished with value: 0.7219425214311054 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 250, &#39;RF__max_depth&#39;: 3, &#39;RF__min_samples_split&#39;: 6, &#39;RF__min_samples_leaf&#39;: 7, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:37:33,479] Trial 14 finished with value: 0.7384302173634919 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 9, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:37:39,823] Trial 15 finished with value: 0.7384302173634919 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 9, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:37:45,670] Trial 16 finished with value: 0.7382562444776702 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 14, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:37:53,152] Trial 17 finished with value: 0.7369091683113834 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 5, &#39;RF__min_samples_leaf&#39;: 16, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:37:59,515] Trial 18 finished with value: 0.7225267188898465 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 3, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:38:38,941] Trial 19 finished with value: 0.7186690827552468 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 250, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 5, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:38:44,607] Trial 20 finished with value: 0.7265245189354026 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 4, &#39;RF__min_samples_split&#39;: 5, &#39;RF__min_samples_leaf&#39;: 9, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:38:49,446] Trial 21 finished with value: 0.7384302173634919 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 9, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:38:54,592] Trial 22 finished with value: 0.7384302173634919 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 9, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:38:59,539] Trial 23 finished with value: 0.7391617595499399 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 13, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:39:06,398] Trial 24 finished with value: 0.7390313333896984 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 8, &#39;RF__min_samples_leaf&#39;: 13, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:39:13,264] Trial 25 finished with value: 0.7378510267644303 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 8, &#39;RF__min_samples_leaf&#39;: 17, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:39:19,985] Trial 26 finished with value: 0.7386091971089931 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 5, &#39;RF__min_samples_leaf&#39;: 14, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:39:26,755] Trial 27 finished with value: 0.7397819512046437 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 2, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:39:36,644] Trial 28 finished with value: 0.7322844407234109 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 2, &#39;RF__min_samples_leaf&#39;: 11, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:39:42,360] Trial 29 finished with value: 0.7225267188898465 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 3, &#39;RF__min_samples_split&#39;: 4, &#39;RF__min_samples_leaf&#39;: 13, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:39:54,235] Trial 30 finished with value: 0.7383496795756237 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 100, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 13, &#39;RF__min_samples_leaf&#39;: 16, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:40:01,177] Trial 31 finished with value: 0.7390313333896984 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 7, &#39;RF__min_samples_leaf&#39;: 13, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:40:07,842] Trial 32 finished with value: 0.7390313333896984 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 13, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:40:14,782] Trial 33 finished with value: 0.7392103131351995 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 11, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:40:18,789] Trial 34 finished with value: 0.7277536382185348 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 4, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 11, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:40:30,993] Trial 35 finished with value: 0.7347227926512861 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 14, &#39;RF__min_samples_leaf&#39;: 15, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:40:35,808] Trial 36 finished with value: 0.7320064107628582 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 17, &#39;RF__min_samples_leaf&#39;: 10, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:40:52,070] Trial 37 finished with value: 0.7381960382572289 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 19, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:41:03,423] Trial 38 finished with value: 0.7279050830518058 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 250, &#39;RF__max_depth&#39;: 5, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:41:12,627] Trial 39 finished with value: 0.7340159806345256 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 2, &#39;RF__min_samples_leaf&#39;: 10, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:41:22,817] Trial 40 finished with value: 0.7388330189430959 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 13, &#39;RF__min_samples_leaf&#39;: 15, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:41:29,693] Trial 41 finished with value: 0.7397819512046437 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:41:36,526] Trial 42 finished with value: 0.7397819512046437 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:41:43,342] Trial 43 finished with value: 0.7392103131351995 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 7, &#39;RF__min_samples_leaf&#39;: 11, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:41:50,193] Trial 44 finished with value: 0.7397819512046437 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:41:54,553] Trial 45 finished with value: 0.7292224616128179 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 5, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:42:01,545] Trial 46 finished with value: 0.7381853768563594 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 10, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:42:06,354] Trial 47 finished with value: 0.7278311516153481 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 100, &#39;RF__max_depth&#39;: 4, &#39;RF__min_samples_split&#39;: 6, &#39;RF__min_samples_leaf&#39;: 14, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:42:09,842] Trial 48 finished with value: 0.7391826486866675 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 3, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:42:20,653] Trial 49 finished with value: 0.7310384873530104 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 17, &#39;RF__min_samples_leaf&#39;: 8, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:42:27,650] Trial 50 finished with value: 0.7388392761096919 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 6, &#39;RF__min_samples_leaf&#39;: 6, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:42:34,633] Trial 51 finished with value: 0.7392103131351995 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 11, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:42:41,493] Trial 52 finished with value: 0.7397819512046437 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 14, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:42:48,415] Trial 53 finished with value: 0.7397819512046437 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 14, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:42:51,737] Trial 54 finished with value: 0.7250133991224557 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 3, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 14, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:43:02,191] Trial 55 finished with value: 0.7377075230696833 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 16, &#39;RF__min_samples_leaf&#39;: 10, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:43:17,086] Trial 56 finished with value: 0.7351400516086145 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 250, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 13, &#39;RF__min_samples_leaf&#39;: 2, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:43:23,901] Trial 57 finished with value: 0.7382999207395526 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 8, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:43:33,376] Trial 58 finished with value: 0.7279208357925517 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 5, &#39;RF__min_samples_split&#39;: 4, &#39;RF__min_samples_leaf&#39;: 15, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:43:38,487] Trial 59 finished with value: 0.7389165304339008 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 8, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:43:51,192] Trial 60 finished with value: 0.7345476032506267 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 100, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 20, &#39;RF__min_samples_leaf&#39;: 13, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:43:58,250] Trial 61 finished with value: 0.7397819512046437 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 14, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:44:05,171] Trial 62 finished with value: 0.7397819512046437 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:44:12,030] Trial 63 finished with value: 0.7396402610181271 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 7, &#39;RF__min_samples_leaf&#39;: 14, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:44:18,914] Trial 64 finished with value: 0.737633980242132 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 14, &#39;RF__min_samples_leaf&#39;: 10, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:44:25,972] Trial 65 finished with value: 0.7390313333896984 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 13, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:44:38,673] Trial 66 finished with value: 0.7393034497364567 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 16, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:44:45,631] Trial 67 finished with value: 0.7339050975598718 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 19, &#39;RF__min_samples_leaf&#39;: 11, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:44:47,906] Trial 68 finished with value: 0.72561822664532 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 4, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 11, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:44:51,515] Trial 69 finished with value: 0.7250133991224557 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 3, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 14, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:44:58,560] Trial 70 finished with value: 0.7390313333896984 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 13, &#39;RF__min_samples_leaf&#39;: 13, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:45:05,461] Trial 71 finished with value: 0.7397819512046437 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:45:12,398] Trial 72 finished with value: 0.7390313333896984 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 14, &#39;RF__min_samples_leaf&#39;: 13, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:45:19,307] Trial 73 finished with value: 0.7392103131351995 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 14, &#39;RF__min_samples_leaf&#39;: 11, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:45:26,252] Trial 74 finished with value: 0.7397819512046437 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 8, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:45:33,383] Trial 75 finished with value: 0.737633980242132 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 16, &#39;RF__min_samples_leaf&#39;: 10, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:45:38,323] Trial 76 finished with value: 0.7391617595499399 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 13, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:45:48,673] Trial 77 finished with value: 0.7388982320232166 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 120, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 16, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:46:18,933] Trial 78 finished with value: 0.7334627198239495 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 250, &#39;RF__max_depth&#39;: 30, &#39;RF__min_samples_split&#39;: 13, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:46:24,344] Trial 79 finished with value: 0.7358458273349577 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 12, &#39;RF__min_samples_leaf&#39;: 14, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:46:40,683] Trial 80 finished with value: 0.7387019451013437 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 200, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 4, &#39;RF__min_samples_leaf&#39;: 11, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:46:47,754] Trial 81 finished with value: 0.7397819512046437 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:46:54,508] Trial 82 finished with value: 0.7390313333896984 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 17, &#39;RF__min_samples_leaf&#39;: 13, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:47:01,098] Trial 83 finished with value: 0.7397819512046437 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 14, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:47:08,046] Trial 84 finished with value: 0.7392103131351995 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 16, &#39;RF__min_samples_leaf&#39;: 11, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:47:12,336] Trial 85 finished with value: 0.7289707162575786 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 5, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 9, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:47:21,199] Trial 86 finished with value: 0.7369444754017432 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 100, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 10, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:47:33,702] Trial 87 finished with value: 0.7394494991011419 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 150, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 14, &#39;RF__min_samples_leaf&#39;: 14, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:47:36,617] Trial 88 finished with value: 0.7287466353508713 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 4, &#39;RF__min_samples_split&#39;: 9, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:47:46,683] Trial 89 finished with value: 0.7364033459474658 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 20, &#39;RF__min_samples_split&#39;: 18, &#39;RF__min_samples_leaf&#39;: 13, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:47:48,895] Trial 90 finished with value: 0.7200257108158148 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 30, &#39;RF__max_depth&#39;: 3, &#39;RF__min_samples_split&#39;: 10, &#39;RF__min_samples_leaf&#39;: 15, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:47:55,901] Trial 91 finished with value: 0.7397819512046437 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:48:02,623] Trial 92 finished with value: 0.7392103131351995 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 15, &#39;RF__min_samples_leaf&#39;: 11, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:48:09,766] Trial 93 finished with value: 0.7397819512046437 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 13, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:48:16,451] Trial 94 finished with value: 0.7397819512046437 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 16, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:48:23,385] Trial 95 finished with value: 0.7390313333896984 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 14, &#39;RF__min_samples_leaf&#39;: 13, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:48:30,337] Trial 96 finished with value: 0.7392103131351995 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 8, &#39;RF__min_samples_leaf&#39;: 11, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:48:37,175] Trial 97 finished with value: 0.7381853768563594 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 75, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 3, &#39;RF__min_samples_leaf&#39;: 10, &#39;RF__criterion&#39;: &#39;gini&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:48:42,558] Trial 98 finished with value: 0.7391617595499399 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 50, &#39;RF__max_depth&#39;: 10, &#39;RF__min_samples_split&#39;: 11, &#39;RF__min_samples_leaf&#39;: 13, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
[I 2024-03-04 20:48:58,217] Trial 99 finished with value: 0.7349255958123594 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;RF__n_estimators&#39;: 250, &#39;RF__max_depth&#39;: 7, &#39;RF__min_samples_split&#39;: 6, &#39;RF__min_samples_leaf&#39;: 12, &#39;RF__criterion&#39;: &#39;entropy&#39;}. Best is trial 5 with value: 0.7407159079432585.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;NN&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_MLP_NN_refined</span>

<span class="k">for</span> <span class="n">sampler_name</span> <span class="ow">in</span> <span class="n">samplers_keys</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sampler_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="s1">&#39;under&#39;</span> <span class="ow">in</span> <span class="n">sampler_name</span><span class="p">:</span>
        <span class="n">n_trials</span> <span class="o">=</span> <span class="mi">25</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">n_trials</span> <span class="o">=</span> <span class="mi">5</span>
    
    <span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">imb_pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">],</span>  
                                <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                                <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                                <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                                <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                                <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                                <span class="n">n_trials</span><span class="o">=</span><span class="n">n_trials</span><span class="p">,</span> 
                                <span class="n">random_state</span><span class="o">=</span><span class="mi">777</span><span class="p">)</span>

    <span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

    <span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
    <span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
    <span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 20:48:58,247] A new study created in memory with name: no-name-c3074479-7e2c-4499-b689-3546588cf286
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>random_under_sampler
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
[I 2024-03-04 20:49:43,505] Trial 0 finished with value: 0.7404481507746686 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.0003191152567904327, &#39;NN__alpha&#39;: 0.04024511932699885}. Best is trial 0 with value: 0.7404481507746686.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
[I 2024-03-04 20:50:25,257] Trial 1 finished with value: 0.7442160013558284 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.00016024484441936167, &#39;NN__alpha&#39;: 0.08312289936751484}. Best is trial 1 with value: 0.7442160013558284.
[I 2024-03-04 20:50:34,464] Trial 2 finished with value: 0.738355993062351 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.0571737233656805, &#39;NN__alpha&#39;: 0.7144866121251745}. Best is trial 1 with value: 0.7442160013558284.
[I 2024-03-04 20:50:41,523] Trial 3 finished with value: 0.7340373935484749 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.025108018718560526, &#39;NN__alpha&#39;: 0.3443439395748663}. Best is trial 1 with value: 0.7442160013558284.
[I 2024-03-04 20:51:08,688] Trial 4 finished with value: 0.7387912857257461 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.0007738450330589464, &#39;NN__alpha&#39;: 0.19411477109985115}. Best is trial 1 with value: 0.7442160013558284.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
[I 2024-03-04 20:52:02,213] Trial 5 finished with value: 0.7427059178332988 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.0002033423889804198, &#39;NN__alpha&#39;: 0.014433503810997733}. Best is trial 1 with value: 0.7442160013558284.
[I 2024-03-04 20:52:15,423] Trial 6 finished with value: 0.7362645506157018 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.008837566322202454, &#39;NN__alpha&#39;: 0.04860501443480164}. Best is trial 1 with value: 0.7442160013558284.
[I 2024-03-04 20:52:22,147] Trial 7 finished with value: 0.6529374406568856 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.18378490089028549, &#39;NN__alpha&#39;: 0.17903849250388507}. Best is trial 1 with value: 0.7442160013558284.
[I 2024-03-04 20:52:31,619] Trial 8 finished with value: 0.7367268713101444 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.01780630684287131, &#39;NN__alpha&#39;: 0.12720776652457155}. Best is trial 1 with value: 0.7442160013558284.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
[I 2024-03-04 20:53:16,914] Trial 9 finished with value: 0.7369374015932424 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.0007718183784439558, &#39;NN__alpha&#39;: 0.055785171539589326}. Best is trial 1 with value: 0.7442160013558284.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
[I 2024-03-04 20:54:09,913] Trial 10 finished with value: 0.7173223276137085 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.002091927724888434, &#39;NN__alpha&#39;: 0.010638032338683654}. Best is trial 1 with value: 0.7442160013558284.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
[I 2024-03-04 20:55:01,247] Trial 11 finished with value: 0.7447357629522617 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.00011491484571729588, &#39;NN__alpha&#39;: 0.01011890867202575}. Best is trial 11 with value: 0.7447357629522617.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
[I 2024-03-04 20:55:54,002] Trial 12 finished with value: 0.7457866797077056 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.00010553420540612239, &#39;NN__alpha&#39;: 0.024100068972779845}. Best is trial 12 with value: 0.7457866797077056.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
[I 2024-03-04 20:56:47,528] Trial 13 finished with value: 0.7438135883851308 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.0001218622094291373, &#39;NN__alpha&#39;: 0.024081007234748156}. Best is trial 12 with value: 0.7457866797077056.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
[I 2024-03-04 20:57:33,746] Trial 14 finished with value: 0.7217063148001127 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.0023065374574477173, &#39;NN__alpha&#39;: 0.020339693232073255}. Best is trial 12 with value: 0.7457866797077056.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
[I 2024-03-04 20:58:27,327] Trial 15 finished with value: 0.736061967103141 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.0006029339973030943, &#39;NN__alpha&#39;: 0.02589191410006346}. Best is trial 12 with value: 0.7457866797077056.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
[I 2024-03-04 20:59:20,165] Trial 16 finished with value: 0.745673431187534 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.00010377867006049513, &#39;NN__alpha&#39;: 0.010075983195013944}. Best is trial 12 with value: 0.7457866797077056.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
[I 2024-03-04 21:00:05,950] Trial 17 finished with value: 0.7400966230426699 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.0003849553367178909, &#39;NN__alpha&#39;: 0.01778479222536891}. Best is trial 12 with value: 0.7457866797077056.
[I 2024-03-04 21:00:39,661] Trial 18 finished with value: 0.7264249956312474 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.001675374785088123, &#39;NN__alpha&#39;: 0.033415966969781725}. Best is trial 12 with value: 0.7457866797077056.
[I 2024-03-04 21:00:52,759] Trial 19 finished with value: 0.7301852880507763 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.005510287838216882, &#39;NN__alpha&#39;: 0.08165653186194613}. Best is trial 12 with value: 0.7457866797077056.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
[I 2024-03-04 21:01:38,593] Trial 20 finished with value: 0.740810210371235 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.00028234568118854316, &#39;NN__alpha&#39;: 0.017682237669193347}. Best is trial 12 with value: 0.7457866797077056.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
[I 2024-03-04 21:02:31,737] Trial 21 finished with value: 0.74452475958006 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.00012682690411244376, &#39;NN__alpha&#39;: 0.01084910185791779}. Best is trial 12 with value: 0.7457866797077056.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
[I 2024-03-04 21:03:23,415] Trial 22 finished with value: 0.7448696415365564 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.0001103245950280853, &#39;NN__alpha&#39;: 0.010946399468028412}. Best is trial 12 with value: 0.7457866797077056.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
[I 2024-03-04 21:04:15,652] Trial 23 finished with value: 0.7383671219203111 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.0004708025258475236, &#39;NN__alpha&#39;: 0.01457401170153283}. Best is trial 12 with value: 0.7457866797077056.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
[I 2024-03-04 21:05:06,113] Trial 24 finished with value: 0.7410255729216052 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.0002427482139882037, &#39;NN__alpha&#39;: 0.028254238548896915}. Best is trial 12 with value: 0.7457866797077056.
[I 2024-03-04 21:05:06,121] A new study created in memory with name: no-name-f73afbaa-c795-4730-a863-289a240b5c4a
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>random_over_sampler
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
[I 2024-03-04 21:20:49,426] Trial 0 finished with value: 0.7194649819558183 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.0003191152567904327, &#39;NN__alpha&#39;: 0.04024511932699885}. Best is trial 0 with value: 0.7194649819558183.
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
[I 2024-03-04 21:35:14,645] Trial 1 finished with value: 0.7383539993297003 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.00016024484441936167, &#39;NN__alpha&#39;: 0.08312289936751484}. Best is trial 1 with value: 0.7383539993297003.
[I 2024-03-04 21:35:39,662] Trial 2 finished with value: 0.7201807319774284 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.0571737233656805, &#39;NN__alpha&#39;: 0.7144866121251745}. Best is trial 1 with value: 0.7383539993297003.
[I 2024-03-04 21:36:13,336] Trial 3 finished with value: 0.7412610023347849 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.025108018718560526, &#39;NN__alpha&#39;: 0.3443439395748663}. Best is trial 3 with value: 0.7412610023347849.
[I 2024-03-04 21:40:28,199] Trial 4 finished with value: 0.7386791129200837 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;NN__learning_rate_init&#39;: 0.0007738450330589464, &#39;NN__alpha&#39;: 0.19411477109985115}. Best is trial 3 with value: 0.7412610023347849.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;HGB&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_HGB_refined</span>

<span class="k">for</span> <span class="n">sampler_name</span> <span class="ow">in</span> <span class="n">samplers_keys</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sampler_name</span><span class="p">)</span>
    
    <span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">imb_pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">],</span>  
                                <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                                <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                                <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                                <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                                <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                                <span class="n">n_trials</span><span class="o">=</span><span class="mi">75</span><span class="p">,</span> 
                                <span class="n">random_state</span><span class="o">=</span><span class="mi">777</span><span class="p">)</span>

    <span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

    <span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
    <span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
    <span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 21:40:28,212] A new study created in memory with name: no-name-4080d0c3-5a5a-49d2-9b58-461b2ce61a28
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>random_under_sampler
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 21:40:36,589] Trial 0 finished with value: 0.7391738458501335 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 20, &#39;HGB__l2_regularization&#39;: 0.15427557882405912, &#39;HGB__max_iter&#39;: 150}. Best is trial 0 with value: 0.7391738458501335.
[I 2024-03-04 21:40:41,504] Trial 1 finished with value: 0.7433180838692813 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 40, &#39;HGB__l2_regularization&#39;: 0.029310907697768503, &#39;HGB__max_iter&#39;: 70}. Best is trial 1 with value: 0.7433180838692813.
[I 2024-03-04 21:40:44,786] Trial 2 finished with value: 0.7445958862739751 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 4, &#39;HGB__l2_regularization&#39;: 0.022572415699062254, &#39;HGB__max_iter&#39;: 130}. Best is trial 2 with value: 0.7445958862739751.
[I 2024-03-04 21:40:51,935] Trial 3 finished with value: 0.7379763615847953 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 7, &#39;HGB__l2_regularization&#39;: 0.010718613164503999, &#39;HGB__max_iter&#39;: 150}. Best is trial 2 with value: 0.7445958862739751.
[I 2024-03-04 21:40:55,700] Trial 4 finished with value: 0.7441897618066201 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 4, &#39;HGB__l2_regularization&#39;: 0.03753439324050785, &#39;HGB__max_iter&#39;: 175}. Best is trial 2 with value: 0.7445958862739751.
[I 2024-03-04 21:40:58,224] Trial 5 finished with value: 0.7428347050777173 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 5, &#39;HGB__l2_regularization&#39;: 0.07221164939169361, &#39;HGB__max_iter&#39;: 50}. Best is trial 2 with value: 0.7445958862739751.
[I 2024-03-04 21:41:04,281] Trial 6 finished with value: 0.7430943915714806 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 30, &#39;HGB__l2_regularization&#39;: 0.05437810500824152, &#39;HGB__max_iter&#39;: 100}. Best is trial 2 with value: 0.7445958862739751.
[I 2024-03-04 21:41:07,314] Trial 7 finished with value: 0.7427454095094199 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 7, &#39;HGB__l2_regularization&#39;: 0.043387432531335555, &#39;HGB__max_iter&#39;: 50}. Best is trial 2 with value: 0.7445958862739751.
[I 2024-03-04 21:41:17,994] Trial 8 finished with value: 0.7369456862845679 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 30, &#39;HGB__l2_regularization&#39;: 0.31365289849676836, &#39;HGB__max_iter&#39;: 200}. Best is trial 2 with value: 0.7445958862739751.
[I 2024-03-04 21:41:20,151] Trial 9 finished with value: 0.7468355689384202 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.5212564505939867, &#39;HGB__max_iter&#39;: 100}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:41:23,346] Trial 10 finished with value: 0.7454229811953162 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.6985769079851571, &#39;HGB__max_iter&#39;: 250}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:41:26,844] Trial 11 finished with value: 0.7466039355020954 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.5370109787722283, &#39;HGB__max_iter&#39;: 250}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:41:28,868] Trial 12 finished with value: 0.7463523196831582 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.6463584382807392, &#39;HGB__max_iter&#39;: 100}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:41:32,027] Trial 13 finished with value: 0.7438441026323105 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.21916540488506014, &#39;HGB__max_iter&#39;: 250}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:41:44,818] Trial 14 finished with value: 0.7347740214427861 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 50, &#39;HGB__l2_regularization&#39;: 0.35993589965276246, &#39;HGB__max_iter&#39;: 250}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:41:50,796] Trial 15 finished with value: 0.7412514673365429 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 10, &#39;HGB__l2_regularization&#39;: 0.14501882059892188, &#39;HGB__max_iter&#39;: 100}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:41:52,747] Trial 16 finished with value: 0.7441367195068898 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.3946252360857483, &#39;HGB__max_iter&#39;: 70}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:41:54,956] Trial 17 finished with value: 0.7460805919453066 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.12128240716065883, &#39;HGB__max_iter&#39;: 130}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:42:05,528] Trial 18 finished with value: 0.7364749964186029 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 10, &#39;HGB__l2_regularization&#39;: 0.5148660892484567, &#39;HGB__max_iter&#39;: 200}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:42:15,020] Trial 19 finished with value: 0.7384599994489639 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 40, &#39;HGB__l2_regularization&#39;: 0.23096569598418967, &#39;HGB__max_iter&#39;: 175}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:42:28,000] Trial 20 finished with value: 0.7339013072150301 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 50, &#39;HGB__l2_regularization&#39;: 0.210208732278453, &#39;HGB__max_iter&#39;: 250}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:42:30,086] Trial 21 finished with value: 0.7462022997492109 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.6420923007628879, &#39;HGB__max_iter&#39;: 100}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:42:32,173] Trial 22 finished with value: 0.7466527481599594 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.4498530074499355, &#39;HGB__max_iter&#39;: 100}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:42:34,192] Trial 23 finished with value: 0.7454433521868352 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.3874723544581305, &#39;HGB__max_iter&#39;: 100}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:42:38,060] Trial 24 finished with value: 0.7438041378670858 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 5, &#39;HGB__l2_regularization&#39;: 0.47599163035777925, &#39;HGB__max_iter&#39;: 100}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:42:44,126] Trial 25 finished with value: 0.7423157206990968 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 20, &#39;HGB__l2_regularization&#39;: 0.25790944645021824, &#39;HGB__max_iter&#39;: 100}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:42:47,182] Trial 26 finished with value: 0.7456462848842106 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.10641509977363774, &#39;HGB__max_iter&#39;: 250}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:42:49,226] Trial 27 finished with value: 0.7460925036530927 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.4824014255883123, &#39;HGB__max_iter&#39;: 100}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:42:51,810] Trial 28 finished with value: 0.7465145103974958 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.3050400980117319, &#39;HGB__max_iter&#39;: 150}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:43:02,611] Trial 29 finished with value: 0.7362023619266359 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 20, &#39;HGB__l2_regularization&#39;: 0.16850140772454603, &#39;HGB__max_iter&#39;: 200}. Best is trial 9 with value: 0.7468355689384202.
[I 2024-03-04 21:43:04,974] Trial 30 finished with value: 0.7468841225236797 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.5361641405341301, &#39;HGB__max_iter&#39;: 130}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:43:07,360] Trial 31 finished with value: 0.7460966037586569 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.4620898586172699, &#39;HGB__max_iter&#39;: 130}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:43:12,086] Trial 32 finished with value: 0.7431799756431201 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 40, &#39;HGB__l2_regularization&#39;: 0.5299810921016326, &#39;HGB__max_iter&#39;: 70}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:43:14,520] Trial 33 finished with value: 0.7462304823429516 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.34919557752948066, &#39;HGB__max_iter&#39;: 130}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:43:17,691] Trial 34 finished with value: 0.744920526775255 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 4, &#39;HGB__l2_regularization&#39;: 0.019062298053434228, &#39;HGB__max_iter&#39;: 130}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:43:25,201] Trial 35 finished with value: 0.7409516865412525 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 7, &#39;HGB__l2_regularization&#39;: 0.2799183024240289, &#39;HGB__max_iter&#39;: 150}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:43:27,960] Trial 36 finished with value: 0.7451793065149032 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.6947159884892635, &#39;HGB__max_iter&#39;: 175}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:43:30,713] Trial 37 finished with value: 0.7439427642384557 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 5, &#39;HGB__l2_regularization&#39;: 0.1768768906021466, &#39;HGB__max_iter&#39;: 50}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:43:38,223] Trial 38 finished with value: 0.7412764565788345 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 30, &#39;HGB__l2_regularization&#39;: 0.08649732490387033, &#39;HGB__max_iter&#39;: 130}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:43:40,998] Trial 39 finished with value: 0.7445514327942799 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 4, &#39;HGB__l2_regularization&#39;: 0.010754152874489754, &#39;HGB__max_iter&#39;: 100}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:43:53,945] Trial 40 finished with value: 0.7354034495990356 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 50, &#39;HGB__l2_regularization&#39;: 0.5605558368879305, &#39;HGB__max_iter&#39;: 250}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:43:56,612] Trial 41 finished with value: 0.7463724316020727 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.30085889247056036, &#39;HGB__max_iter&#39;: 150}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:43:59,267] Trial 42 finished with value: 0.7448345540947089 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.4196353524125423, &#39;HGB__max_iter&#39;: 150}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:44:01,956] Trial 43 finished with value: 0.7467785560694279 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.5661923011224965, &#39;HGB__max_iter&#39;: 150}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:44:09,286] Trial 44 finished with value: 0.7422792083579255 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 7, &#39;HGB__l2_regularization&#39;: 0.5712758424858043, &#39;HGB__max_iter&#39;: 150}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:44:10,879] Trial 45 finished with value: 0.7432971947325538 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.35765571258292356, &#39;HGB__max_iter&#39;: 50}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:44:16,459] Trial 46 finished with value: 0.7424898231212206 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 10, &#39;HGB__l2_regularization&#39;: 0.6084916030741414, &#39;HGB__max_iter&#39;: 100}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:44:21,091] Trial 47 finished with value: 0.7428836472718833 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 30, &#39;HGB__l2_regularization&#39;: 0.42016274088441696, &#39;HGB__max_iter&#39;: 70}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:44:24,153] Trial 48 finished with value: 0.7459465838247096 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.4535940533666422, &#39;HGB__max_iter&#39;: 250}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:44:33,590] Trial 49 finished with value: 0.7376116267819887 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 20, &#39;HGB__l2_regularization&#39;: 0.027247438112349278, &#39;HGB__max_iter&#39;: 175}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:44:36,217] Trial 50 finished with value: 0.7454597526090921 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.19876057055246488, &#39;HGB__max_iter&#39;: 130}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:44:38,805] Trial 51 finished with value: 0.7465594820224001 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.2963412609169494, &#39;HGB__max_iter&#39;: 150}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:44:41,294] Trial 52 finished with value: 0.746210240887735 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.6843801011718973, &#39;HGB__max_iter&#39;: 150}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:44:43,687] Trial 53 finished with value: 0.7459219831913244 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.344373084205998, &#39;HGB__max_iter&#39;: 150}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:44:52,487] Trial 54 finished with value: 0.7400379712578545 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 40, &#39;HGB__l2_regularization&#39;: 0.5840442192437783, &#39;HGB__max_iter&#39;: 150}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:44:55,570] Trial 55 finished with value: 0.7461737285465637 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.26210878694344036, &#39;HGB__max_iter&#39;: 200}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:44:57,755] Trial 56 finished with value: 0.7468355689384202 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.5108542385414844, &#39;HGB__max_iter&#39;: 100}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:45:01,030] Trial 57 finished with value: 0.7435527811207531 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 5, &#39;HGB__l2_regularization&#39;: 0.5246896418221934, &#39;HGB__max_iter&#39;: 100}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:45:07,294] Trial 58 finished with value: 0.7421127880017213 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 10, &#39;HGB__l2_regularization&#39;: 0.046058742030121766, &#39;HGB__max_iter&#39;: 100}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:45:13,725] Trial 59 finished with value: 0.7410654532066329 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 50, &#39;HGB__l2_regularization&#39;: 0.40480386202717844, &#39;HGB__max_iter&#39;: 100}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:45:17,254] Trial 60 finished with value: 0.7444611009355652 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.23798661361836895, &#39;HGB__max_iter&#39;: 250}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:45:19,343] Trial 61 finished with value: 0.7461046744334832 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.49257877733851924, &#39;HGB__max_iter&#39;: 100}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:45:21,210] Trial 62 finished with value: 0.7458209054515432 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.6033052596845075, &#39;HGB__max_iter&#39;: 100}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:45:23,594] Trial 63 finished with value: 0.7449762442812258 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.3263070254879347, &#39;HGB__max_iter&#39;: 150}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:45:25,541] Trial 64 finished with value: 0.7453299741303615 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.4321563300538845, &#39;HGB__max_iter&#39;: 70}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:45:28,624] Trial 65 finished with value: 0.7447582065246151 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 4, &#39;HGB__l2_regularization&#39;: 0.06350061145692687, &#39;HGB__max_iter&#39;: 130}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:45:31,738] Trial 66 finished with value: 0.7447898415424093 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.3818346934520486, &#39;HGB__max_iter&#39;: 250}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:45:35,348] Trial 67 finished with value: 0.7433503665685862 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 7, &#39;HGB__l2_regularization&#39;: 0.5327765950549586, &#39;HGB__max_iter&#39;: 50}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:45:37,544] Trial 68 finished with value: 0.7460114082959238 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.47628870204948653, &#39;HGB__max_iter&#39;: 100}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:45:48,821] Trial 69 finished with value: 0.7385322469134934 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 40, &#39;HGB__l2_regularization&#39;: 0.6359481764185363, &#39;HGB__max_iter&#39;: 200}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:45:51,617] Trial 70 finished with value: 0.7446641631692428 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.13120298227333324, &#39;HGB__max_iter&#39;: 175}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:45:54,155] Trial 71 finished with value: 0.7468108387687328 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.302237883292896, &#39;HGB__max_iter&#39;: 150}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:45:56,626] Trial 72 finished with value: 0.7456542260227348 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.32250871951772614, &#39;HGB__max_iter&#39;: 150}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:45:59,186] Trial 73 finished with value: 0.746806997735773 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.2797095356530202, &#39;HGB__max_iter&#39;: 150}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:46:07,705] Trial 74 finished with value: 0.7402332218893101 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 30, &#39;HGB__l2_regularization&#39;: 0.3768356005309193, &#39;HGB__max_iter&#39;: 150}. Best is trial 30 with value: 0.7468841225236797.
[I 2024-03-04 21:46:07,725] A new study created in memory with name: no-name-e59db94e-fe87-4203-8b22-1ce059ef9e03
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>random_over_sampler
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 21:46:19,189] Trial 0 finished with value: 0.7387466125975383 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 20, &#39;HGB__l2_regularization&#39;: 0.15427557882405912, &#39;HGB__max_iter&#39;: 150}. Best is trial 0 with value: 0.7387466125975383.
[I 2024-03-04 21:46:25,939] Trial 1 finished with value: 0.7437882105339327 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 40, &#39;HGB__l2_regularization&#39;: 0.029310907697768503, &#39;HGB__max_iter&#39;: 70}. Best is trial 1 with value: 0.7437882105339327.
[I 2024-03-04 21:46:31,062] Trial 2 finished with value: 0.7447539318266436 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 4, &#39;HGB__l2_regularization&#39;: 0.022572415699062254, &#39;HGB__max_iter&#39;: 130}. Best is trial 2 with value: 0.7447539318266436.
[I 2024-03-04 21:46:41,299] Trial 3 finished with value: 0.7379631939380795 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 7, &#39;HGB__l2_regularization&#39;: 0.010718613164503999, &#39;HGB__max_iter&#39;: 150}. Best is trial 2 with value: 0.7447539318266436.
[I 2024-03-04 21:46:47,413] Trial 4 finished with value: 0.7443838015552512 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 4, &#39;HGB__l2_regularization&#39;: 0.03753439324050785, &#39;HGB__max_iter&#39;: 175}. Best is trial 2 with value: 0.7447539318266436.
[I 2024-03-04 21:46:51,322] Trial 5 finished with value: 0.7435016368094501 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 5, &#39;HGB__l2_regularization&#39;: 0.07221164939169361, &#39;HGB__max_iter&#39;: 50}. Best is trial 2 with value: 0.7447539318266436.
[I 2024-03-04 21:46:59,967] Trial 6 finished with value: 0.7423492086492133 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 30, &#39;HGB__l2_regularization&#39;: 0.05437810500824152, &#39;HGB__max_iter&#39;: 100}. Best is trial 2 with value: 0.7447539318266436.
[I 2024-03-04 21:47:04,799] Trial 7 finished with value: 0.7452105529237909 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 7, &#39;HGB__l2_regularization&#39;: 0.043387432531335555, &#39;HGB__max_iter&#39;: 50}. Best is trial 7 with value: 0.7452105529237909.
[I 2024-03-04 21:47:19,541] Trial 8 finished with value: 0.736953751327381 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 30, &#39;HGB__l2_regularization&#39;: 0.31365289849676836, &#39;HGB__max_iter&#39;: 200}. Best is trial 7 with value: 0.7452105529237909.
[I 2024-03-04 21:47:23,163] Trial 9 finished with value: 0.7458029505936604 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.5212564505939867, &#39;HGB__max_iter&#39;: 100}. Best is trial 9 with value: 0.7458029505936604.
[I 2024-03-04 21:47:29,490] Trial 10 finished with value: 0.746248133072125 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.6985769079851571, &#39;HGB__max_iter&#39;: 250}. Best is trial 10 with value: 0.746248133072125.
[I 2024-03-04 21:47:35,856] Trial 11 finished with value: 0.7469990099596746 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.5370109787722283, &#39;HGB__max_iter&#39;: 250}. Best is trial 11 with value: 0.7469990099596746.
[I 2024-03-04 21:47:42,090] Trial 12 finished with value: 0.7468206778956844 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.6713158102115786, &#39;HGB__max_iter&#39;: 250}. Best is trial 11 with value: 0.7469990099596746.
[I 2024-03-04 21:47:48,556] Trial 13 finished with value: 0.7454078310799762 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.21704097856730487, &#39;HGB__max_iter&#39;: 250}. Best is trial 11 with value: 0.7469990099596746.
[I 2024-03-04 21:48:06,287] Trial 14 finished with value: 0.7319265262885141 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 50, &#39;HGB__l2_regularization&#39;: 0.3897037849003003, &#39;HGB__max_iter&#39;: 250}. Best is trial 11 with value: 0.7469990099596746.
[I 2024-03-04 21:48:23,681] Trial 15 finished with value: 0.7310904539382314 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 10, &#39;HGB__l2_regularization&#39;: 0.14406781089275775, &#39;HGB__max_iter&#39;: 250}. Best is trial 11 with value: 0.7469990099596746.
[I 2024-03-04 21:48:29,486] Trial 16 finished with value: 0.745923233498241 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.2859583731029552, &#39;HGB__max_iter&#39;: 250}. Best is trial 11 with value: 0.7469990099596746.
[I 2024-03-04 21:48:33,495] Trial 17 finished with value: 0.7475400493017418 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.6384276095343249, &#39;HGB__max_iter&#39;: 130}. Best is trial 17 with value: 0.7475400493017418.
[I 2024-03-04 21:48:43,172] Trial 18 finished with value: 0.7398275254549534 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 10, &#39;HGB__l2_regularization&#39;: 0.10699588228121988, &#39;HGB__max_iter&#39;: 130}. Best is trial 17 with value: 0.7475400493017418.
[I 2024-03-04 21:48:53,378] Trial 19 finished with value: 0.7399901047781977 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 40, &#39;HGB__l2_regularization&#39;: 0.3860248128525662, &#39;HGB__max_iter&#39;: 130}. Best is trial 17 with value: 0.7475400493017418.
[I 2024-03-04 21:49:06,732] Trial 20 finished with value: 0.7377056645053478 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 50, &#39;HGB__l2_regularization&#39;: 0.20936706595915344, &#39;HGB__max_iter&#39;: 175}. Best is trial 17 with value: 0.7475400493017418.
[I 2024-03-04 21:49:12,142] Trial 21 finished with value: 0.747153777680697 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.6414580086908492, &#39;HGB__max_iter&#39;: 200}. Best is trial 17 with value: 0.7475400493017418.
[I 2024-03-04 21:49:17,294] Trial 22 finished with value: 0.746549209230437 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.4753604969373906, &#39;HGB__max_iter&#39;: 200}. Best is trial 17 with value: 0.7475400493017418.
[I 2024-03-04 21:49:22,509] Trial 23 finished with value: 0.747599996449579 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.5417185975972347, &#39;HGB__max_iter&#39;: 200}. Best is trial 23 with value: 0.747599996449579.
[I 2024-03-04 21:49:32,559] Trial 24 finished with value: 0.7381253846524172 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 5, &#39;HGB__l2_regularization&#39;: 0.2540242503933193, &#39;HGB__max_iter&#39;: 200}. Best is trial 23 with value: 0.747599996449579.
[I 2024-03-04 21:49:47,891] Trial 25 finished with value: 0.7343808393416212 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 20, &#39;HGB__l2_regularization&#39;: 0.3779960891072497, &#39;HGB__max_iter&#39;: 200}. Best is trial 23 with value: 0.747599996449579.
[I 2024-03-04 21:49:53,401] Trial 26 finished with value: 0.7471983606966944 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.16598806453821555, &#39;HGB__max_iter&#39;: 200}. Best is trial 23 with value: 0.747599996449579.
[I 2024-03-04 21:49:56,535] Trial 27 finished with value: 0.7445781904886969 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.15183260055621733, &#39;HGB__max_iter&#39;: 70}. Best is trial 23 with value: 0.747599996449579.
[I 2024-03-04 21:50:00,800] Trial 28 finished with value: 0.7467931429834543 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.09290513581351681, &#39;HGB__max_iter&#39;: 130}. Best is trial 23 with value: 0.747599996449579.
[I 2024-03-04 21:50:12,224] Trial 29 finished with value: 0.7390466524654329 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 20, &#39;HGB__l2_regularization&#39;: 0.16850140772454603, &#39;HGB__max_iter&#39;: 150}. Best is trial 23 with value: 0.747599996449579.
[I 2024-03-04 21:50:17,770] Trial 30 finished with value: 0.746585462499004 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.11960566293534498, &#39;HGB__max_iter&#39;: 200}. Best is trial 23 with value: 0.747599996449579.
[I 2024-03-04 21:50:23,234] Trial 31 finished with value: 0.7474662474015864 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.6969008852393584, &#39;HGB__max_iter&#39;: 200}. Best is trial 23 with value: 0.747599996449579.
[I 2024-03-04 21:50:38,297] Trial 32 finished with value: 0.7339907266876163 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 40, &#39;HGB__l2_regularization&#39;: 0.4624543183092279, &#39;HGB__max_iter&#39;: 200}. Best is trial 23 with value: 0.747599996449579.
[I 2024-03-04 21:50:43,860] Trial 33 finished with value: 0.7463256464689385 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.2937459246947761, &#39;HGB__max_iter&#39;: 200}. Best is trial 23 with value: 0.747599996449579.
[I 2024-03-04 21:50:47,588] Trial 34 finished with value: 0.7442403429166092 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 4, &#39;HGB__l2_regularization&#39;: 0.019742942067129596, &#39;HGB__max_iter&#39;: 70}. Best is trial 23 with value: 0.747599996449579.
[I 2024-03-04 21:50:56,837] Trial 35 finished with value: 0.7391047016248425 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 7, &#39;HGB__l2_regularization&#39;: 0.555157997921771, &#39;HGB__max_iter&#39;: 130}. Best is trial 23 with value: 0.747599996449579.
[I 2024-03-04 21:51:02,354] Trial 36 finished with value: 0.7468127367571601 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.0725070758568942, &#39;HGB__max_iter&#39;: 200}. Best is trial 23 with value: 0.747599996449579.
[I 2024-03-04 21:51:10,747] Trial 37 finished with value: 0.7404815542445881 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 5, &#39;HGB__l2_regularization&#39;: 0.36744929929200654, &#39;HGB__max_iter&#39;: 150}. Best is trial 23 with value: 0.747599996449579.
[I 2024-03-04 21:51:15,990] Trial 38 finished with value: 0.7453526317192142 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 30, &#39;HGB__l2_regularization&#39;: 0.22347613427062907, &#39;HGB__max_iter&#39;: 50}. Best is trial 23 with value: 0.747599996449579.
[I 2024-03-04 21:51:22,343] Trial 39 finished with value: 0.743287913174903 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 4, &#39;HGB__l2_regularization&#39;: 0.010754152874489754, &#39;HGB__max_iter&#39;: 175}. Best is trial 23 with value: 0.747599996449579.
[I 2024-03-04 21:51:31,001] Trial 40 finished with value: 0.7424875759479788 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 50, &#39;HGB__l2_regularization&#39;: 0.47177406162385715, &#39;HGB__max_iter&#39;: 100}. Best is trial 23 with value: 0.747599996449579.
[I 2024-03-04 21:51:36,488] Trial 41 finished with value: 0.7470441111208808 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.6719358469022341, &#39;HGB__max_iter&#39;: 200}. Best is trial 23 with value: 0.747599996449579.
[I 2024-03-04 21:51:42,152] Trial 42 finished with value: 0.7477545050979969 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.6312569959371437, &#39;HGB__max_iter&#39;: 200}. Best is trial 42 with value: 0.7477545050979969.
[I 2024-03-04 21:51:47,676] Trial 43 finished with value: 0.747104965022833 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.5885389774557144, &#39;HGB__max_iter&#39;: 200}. Best is trial 42 with value: 0.7477545050979969.
[I 2024-03-04 21:52:00,504] Trial 44 finished with value: 0.7332483090837996 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 7, &#39;HGB__l2_regularization&#39;: 0.4296188996113299, &#39;HGB__max_iter&#39;: 200}. Best is trial 42 with value: 0.7477545050979969.
[I 2024-03-04 21:52:04,837] Trial 45 finished with value: 0.7478848017219363 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.32512916133128145, &#39;HGB__max_iter&#39;: 130}. Best is trial 45 with value: 0.7478848017219363.
[I 2024-03-04 21:52:08,694] Trial 46 finished with value: 0.7469391923481398 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.5443844080074601, &#39;HGB__max_iter&#39;: 130}. Best is trial 45 with value: 0.7478848017219363.
[I 2024-03-04 21:52:18,856] Trial 47 finished with value: 0.7409555669983044 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 30, &#39;HGB__l2_regularization&#39;: 0.3110964697267529, &#39;HGB__max_iter&#39;: 130}. Best is trial 45 with value: 0.7478848017219363.
[I 2024-03-04 21:52:22,852] Trial 48 finished with value: 0.7480676225003972 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.3468965477307378, &#39;HGB__max_iter&#39;: 130}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:52:32,491] Trial 49 finished with value: 0.7406596272359739 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 10, &#39;HGB__l2_regularization&#39;: 0.31552895405618697, &#39;HGB__max_iter&#39;: 130}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:52:42,994] Trial 50 finished with value: 0.738958393187553 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 20, &#39;HGB__l2_regularization&#39;: 0.34311539454573603, &#39;HGB__max_iter&#39;: 130}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:52:46,788] Trial 51 finished with value: 0.747000046250092 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.6898622827341221, &#39;HGB__max_iter&#39;: 130}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:52:49,459] Trial 52 finished with value: 0.7414529751345945 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.45329867677785346, &#39;HGB__max_iter&#39;: 50}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:52:53,111] Trial 53 finished with value: 0.7456771426841918 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.5753978762621416, &#39;HGB__max_iter&#39;: 100}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:53:03,776] Trial 54 finished with value: 0.7413739917823522 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 40, &#39;HGB__l2_regularization&#39;: 0.4145777921556692, &#39;HGB__max_iter&#39;: 130}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:53:07,963] Trial 55 finished with value: 0.7463546062804921 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.26375379929892984, &#39;HGB__max_iter&#39;: 130}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:53:12,919] Trial 56 finished with value: 0.7475640022536162 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.5138208134260706, &#39;HGB__max_iter&#39;: 175}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:53:21,323] Trial 57 finished with value: 0.7393891182882933 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 5, &#39;HGB__l2_regularization&#39;: 0.5296567139216852, &#39;HGB__max_iter&#39;: 175}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:53:26,115] Trial 58 finished with value: 0.7462651811758927 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.047799892450202885, &#39;HGB__max_iter&#39;: 175}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:53:38,371] Trial 59 finished with value: 0.7350024558956488 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 10, &#39;HGB__l2_regularization&#39;: 0.22736388388692502, &#39;HGB__max_iter&#39;: 175}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:53:44,485] Trial 60 finished with value: 0.7435044415519926 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 50, &#39;HGB__l2_regularization&#39;: 0.3443281645791764, &#39;HGB__max_iter&#39;: 70}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:53:48,913] Trial 61 finished with value: 0.747279974199072 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.591589257708524, &#39;HGB__max_iter&#39;: 150}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:53:53,560] Trial 62 finished with value: 0.7469593042670543 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.47967690228476884, &#39;HGB__max_iter&#39;: 175}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:53:57,741] Trial 63 finished with value: 0.7473654287344095 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.6188292411787217, &#39;HGB__max_iter&#39;: 130}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:54:02,748] Trial 64 finished with value: 0.746780194985251 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.39555279134962146, &#39;HGB__max_iter&#39;: 200}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:54:06,832] Trial 65 finished with value: 0.7468905092265778 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.5085776333441622, &#39;HGB__max_iter&#39;: 130}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:54:09,724] Trial 66 finished with value: 0.7417004908479674 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.6238158854396091, &#39;HGB__max_iter&#39;: 50}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:54:22,083] Trial 67 finished with value: 0.7355621822573069 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 7, &#39;HGB__l2_regularization&#39;: 0.6961047844535548, &#39;HGB__max_iter&#39;: 200}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:54:26,320] Trial 68 finished with value: 0.7451931162111167 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 4, &#39;HGB__l2_regularization&#39;: 0.033134419922180885, &#39;HGB__max_iter&#39;: 100}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:54:31,848] Trial 69 finished with value: 0.7476285676522262 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.5052978131108403, &#39;HGB__max_iter&#39;: 200}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:54:36,051] Trial 70 finished with value: 0.7473047043687592 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.43364903557606077, &#39;HGB__max_iter&#39;: 130}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:54:41,511] Trial 71 finished with value: 0.7476285676522262 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.5051105319362161, &#39;HGB__max_iter&#39;: 200}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:54:46,595] Trial 72 finished with value: 0.7472145020463469 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.5327230757433074, &#39;HGB__max_iter&#39;: 200}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:54:51,831] Trial 73 finished with value: 0.7470889532094827 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 3, &#39;HGB__l2_regularization&#39;: 0.19420243725621048, &#39;HGB__max_iter&#39;: 200}. Best is trial 48 with value: 0.7480676225003972.
[I 2024-03-04 21:55:08,855] Trial 74 finished with value: 0.7300918022647046 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;HGB__max_depth&#39;: 40, &#39;HGB__l2_regularization&#39;: 0.3475185433091332, &#39;HGB__max_iter&#39;: 250}. Best is trial 48 with value: 0.7480676225003972.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;trees&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_trees_refined</span>

<span class="k">for</span> <span class="n">sampler_name</span> <span class="ow">in</span> <span class="n">samplers_keys</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sampler_name</span><span class="p">)</span>
    
    <span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">imb_pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">],</span>  
                                <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                                <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                                <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                                <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                                <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                                <span class="n">n_trials</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
                                <span class="n">random_state</span><span class="o">=</span><span class="mi">777</span><span class="p">)</span>

    <span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

    <span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
    <span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
    <span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 21:55:08,886] A new study created in memory with name: no-name-0e9302ef-4af9-41e2-bf1c-99653595c7f8
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>random_under_sampler
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 21:55:09,965] Trial 0 finished with value: 0.7130865919091264 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 10, &#39;trees__min_samples_split&#39;: 17, &#39;trees__min_samples_leaf&#39;: 4, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 0 with value: 0.7130865919091264.
[I 2024-03-04 21:55:11,032] Trial 1 finished with value: 0.7204582043686805 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 5, &#39;trees__min_samples_split&#39;: 7, &#39;trees__min_samples_leaf&#39;: 6, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 1 with value: 0.7204582043686805.
[I 2024-03-04 21:55:12,047] Trial 2 finished with value: 0.7216169741757105 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 5, &#39;trees__min_samples_split&#39;: 14, &#39;trees__min_samples_leaf&#39;: 14, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 2 with value: 0.7216169741757105.
[I 2024-03-04 21:55:13,046] Trial 3 finished with value: 0.7223793741621396 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 19, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7223793741621396.
[I 2024-03-04 21:55:14,081] Trial 4 finished with value: 0.7119539321150027 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 10, &#39;trees__min_samples_split&#39;: 10, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7223793741621396.
[I 2024-03-04 21:55:15,067] Trial 5 finished with value: 0.716153420183031 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 4, &#39;trees__min_samples_split&#39;: 4, &#39;trees__min_samples_leaf&#39;: 18, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7223793741621396.
[I 2024-03-04 21:55:16,073] Trial 6 finished with value: 0.6919622116248761 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 20, &#39;trees__min_samples_split&#39;: 23, &#39;trees__min_samples_leaf&#39;: 9, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7223793741621396.
[I 2024-03-04 21:55:17,025] Trial 7 finished with value: 0.7155241215630838 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 4, &#39;trees__min_samples_split&#39;: 20, &#39;trees__min_samples_leaf&#39;: 22, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7223793741621396.
[I 2024-03-04 21:55:18,018] Trial 8 finished with value: 0.722392451696645 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 6, &#39;trees__min_samples_leaf&#39;: 18, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 8 with value: 0.722392451696645.
[I 2024-03-04 21:55:18,983] Trial 9 finished with value: 0.6981398767233425 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 3, &#39;trees__min_samples_split&#39;: 6, &#39;trees__min_samples_leaf&#39;: 19, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 8 with value: 0.722392451696645.
[I 2024-03-04 21:55:19,996] Trial 10 finished with value: 0.7232421197266419 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 2, &#39;trees__min_samples_leaf&#39;: 23, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 10 with value: 0.7232421197266419.
[I 2024-03-04 21:55:20,998] Trial 11 finished with value: 0.7234772055870201 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 2, &#39;trees__min_samples_leaf&#39;: 25, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 11 with value: 0.7234772055870201.
[I 2024-03-04 21:55:21,890] Trial 12 finished with value: 0.7232421197266419 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 2, &#39;trees__min_samples_leaf&#39;: 24, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 11 with value: 0.7234772055870201.
[I 2024-03-04 21:55:22,952] Trial 13 finished with value: 0.7101104897348564 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: None, &#39;trees__min_samples_split&#39;: 2, &#39;trees__min_samples_leaf&#39;: 25, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 11 with value: 0.7234772055870201.
[I 2024-03-04 21:55:24,332] Trial 14 finished with value: 0.7108162654611996 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 10, &#39;trees__min_samples_leaf&#39;: 21, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 11 with value: 0.7234772055870201.
[I 2024-03-04 21:55:25,364] Trial 15 finished with value: 0.7108859672557909 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 15, &#39;trees__min_samples_split&#39;: 10, &#39;trees__min_samples_leaf&#39;: 25, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 11 with value: 0.7234772055870201.
[I 2024-03-04 21:55:26,679] Trial 16 finished with value: 0.7230738858622074 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 2, &#39;trees__min_samples_leaf&#39;: 16, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 11 with value: 0.7234772055870201.
[I 2024-03-04 21:55:27,709] Trial 17 finished with value: 0.7232056073854706 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 13, &#39;trees__min_samples_leaf&#39;: 22, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 11 with value: 0.7234772055870201.
[I 2024-03-04 21:55:29,046] Trial 18 finished with value: 0.7097965951146431 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: None, &#39;trees__min_samples_split&#39;: 25, &#39;trees__min_samples_leaf&#39;: 20, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 11 with value: 0.7234772055870201.
[I 2024-03-04 21:55:30,126] Trial 19 finished with value: 0.7101644387887017 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 20, &#39;trees__min_samples_split&#39;: 5, &#39;trees__min_samples_leaf&#39;: 23, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 11 with value: 0.7234772055870201.
[I 2024-03-04 21:55:31,513] Trial 20 finished with value: 0.706171859619324 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 8, &#39;trees__min_samples_leaf&#39;: 16, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 11 with value: 0.7234772055870201.
[I 2024-03-04 21:55:32,595] Trial 21 finished with value: 0.7234772055870201 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 2, &#39;trees__min_samples_leaf&#39;: 25, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 11 with value: 0.7234772055870201.
[I 2024-03-04 21:55:33,953] Trial 22 finished with value: 0.7234772055870201 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 4, &#39;trees__min_samples_leaf&#39;: 25, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 11 with value: 0.7234772055870201.
[I 2024-03-04 21:55:34,961] Trial 23 finished with value: 0.7234772055870201 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 4, &#39;trees__min_samples_leaf&#39;: 25, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 11 with value: 0.7234772055870201.
[I 2024-03-04 21:55:36,338] Trial 24 finished with value: 0.711745992557948 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 15, &#39;trees__min_samples_split&#39;: 4, &#39;trees__min_samples_leaf&#39;: 21, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 11 with value: 0.7234772055870201.
[I 2024-03-04 21:55:37,375] Trial 25 finished with value: 0.6981398767233425 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 3, &#39;trees__min_samples_split&#39;: 8, &#39;trees__min_samples_leaf&#39;: 23, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 11 with value: 0.7234772055870201.
[I 2024-03-04 21:55:38,713] Trial 26 finished with value: 0.7235075452417924 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 4, &#39;trees__min_samples_leaf&#39;: 10, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 26 with value: 0.7235075452417924.
[I 2024-03-04 21:55:39,768] Trial 27 finished with value: 0.7216573726059466 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 13, &#39;trees__min_samples_leaf&#39;: 8, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 26 with value: 0.7235075452417924.
[I 2024-03-04 21:55:41,055] Trial 28 finished with value: 0.7238078441822915 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 8, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:55:42,149] Trial 29 finished with value: 0.7116736155571162 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 10, &#39;trees__min_samples_split&#39;: 8, &#39;trees__min_samples_leaf&#39;: 10, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:55:43,435] Trial 30 finished with value: 0.7203736565874582 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 11, &#39;trees__min_samples_leaf&#39;: 3, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:55:44,315] Trial 31 finished with value: 0.7203531560596373 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 3, &#39;trees__min_samples_leaf&#39;: 7, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:55:45,295] Trial 32 finished with value: 0.7202227298993957 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 5, &#39;trees__min_samples_split&#39;: 6, &#39;trees__min_samples_leaf&#39;: 5, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:55:46,302] Trial 33 finished with value: 0.7238078441822915 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 5, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:55:47,667] Trial 34 finished with value: 0.7238078441822915 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 16, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:55:48,666] Trial 35 finished with value: 0.7214260827224234 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 5, &#39;trees__min_samples_split&#39;: 17, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:55:50,040] Trial 36 finished with value: 0.7110863542832283 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 10, &#39;trees__min_samples_split&#39;: 15, &#39;trees__min_samples_leaf&#39;: 14, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:55:51,012] Trial 37 finished with value: 0.7164945906428697 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 4, &#39;trees__min_samples_split&#39;: 16, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:55:52,000] Trial 38 finished with value: 0.7234224793153615 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 21, &#39;trees__min_samples_leaf&#39;: 9, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:55:53,313] Trial 39 finished with value: 0.6849675498046244 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 20, &#39;trees__min_samples_split&#39;: 18, &#39;trees__min_samples_leaf&#39;: 6, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:55:54,094] Trial 40 finished with value: 0.6981398767233425 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 3, &#39;trees__min_samples_split&#39;: 11, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:55:55,042] Trial 41 finished with value: 0.7228753123430005 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 7, &#39;trees__min_samples_leaf&#39;: 15, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:55:56,056] Trial 42 finished with value: 0.7238078441822915 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 5, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:55:57,053] Trial 43 finished with value: 0.7238078441822915 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 7, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:55:57,967] Trial 44 finished with value: 0.7238078441822915 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 9, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:55:59,022] Trial 45 finished with value: 0.6947342152750514 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: None, &#39;trees__min_samples_split&#39;: 7, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:00,048] Trial 46 finished with value: 0.7162431043602348 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 4, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 8, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:01,388] Trial 47 finished with value: 0.7017732010386964 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 6, &#39;trees__min_samples_leaf&#39;: 14, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:02,714] Trial 48 finished with value: 0.696297082024707 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 15, &#39;trees__min_samples_split&#39;: 15, &#39;trees__min_samples_leaf&#39;: 10, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:03,749] Trial 49 finished with value: 0.7223793741621396 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 9, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:05,030] Trial 50 finished with value: 0.7238078441822915 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 19, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:06,082] Trial 51 finished with value: 0.7238078441822915 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 9, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:07,422] Trial 52 finished with value: 0.722971901368311 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 5, &#39;trees__min_samples_leaf&#39;: 9, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:08,472] Trial 53 finished with value: 0.723145226572622 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 5, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:09,720] Trial 54 finished with value: 0.7216573726059466 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 9, &#39;trees__min_samples_leaf&#39;: 8, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:11,099] Trial 55 finished with value: 0.7035026176808582 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 20, &#39;trees__min_samples_split&#39;: 7, &#39;trees__min_samples_leaf&#39;: 15, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:12,152] Trial 56 finished with value: 0.7238078441822915 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 11, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:13,324] Trial 57 finished with value: 0.7210220308359028 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 5, &#39;trees__min_samples_split&#39;: 3, &#39;trees__min_samples_leaf&#39;: 10, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:14,223] Trial 58 finished with value: 0.723145226572622 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 6, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:15,061] Trial 59 finished with value: 0.6877541854249313 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: None, &#39;trees__min_samples_split&#39;: 8, &#39;trees__min_samples_leaf&#39;: 9, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:16,069] Trial 60 finished with value: 0.7176910329857774 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 10, &#39;trees__min_samples_split&#39;: 14, &#39;trees__min_samples_leaf&#39;: 17, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:17,436] Trial 61 finished with value: 0.7238078441822915 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 21, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:18,264] Trial 62 finished with value: 0.7223793741621396 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 19, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:19,233] Trial 63 finished with value: 0.7238078441822915 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 18, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:20,234] Trial 64 finished with value: 0.722971901368311 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 5, &#39;trees__min_samples_leaf&#39;: 9, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:21,216] Trial 65 finished with value: 0.6981398767233425 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 3, &#39;trees__min_samples_split&#39;: 22, &#39;trees__min_samples_leaf&#39;: 10, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:22,205] Trial 66 finished with value: 0.7223793741621396 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 3, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:23,439] Trial 67 finished with value: 0.6800278518819293 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 17, &#39;trees__min_samples_leaf&#39;: 7, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:24,329] Trial 68 finished with value: 0.7050260378104977 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 15, &#39;trees__min_samples_split&#39;: 20, &#39;trees__min_samples_leaf&#39;: 15, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:25,271] Trial 69 finished with value: 0.723145226572622 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 7, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:26,253] Trial 70 finished with value: 0.7164945906428697 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 4, &#39;trees__min_samples_split&#39;: 16, &#39;trees__min_samples_leaf&#39;: 14, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:27,254] Trial 71 finished with value: 0.7238078441822915 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 9, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:28,247] Trial 72 finished with value: 0.7238078441822915 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 10, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:29,265] Trial 73 finished with value: 0.7235075452417924 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 9, &#39;trees__min_samples_leaf&#39;: 10, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:30,269] Trial 74 finished with value: 0.723145226572622 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:31,389] Trial 75 finished with value: 0.7216573726059466 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 8, &#39;trees__min_samples_leaf&#39;: 8, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:32,358] Trial 76 finished with value: 0.722971901368311 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 6, &#39;trees__min_samples_leaf&#39;: 9, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:33,202] Trial 77 finished with value: 0.7238078441822915 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 10, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:34,433] Trial 78 finished with value: 0.6997729634127982 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 20, &#39;trees__min_samples_split&#39;: 5, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:35,547] Trial 79 finished with value: 0.7170748119003356 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 10, &#39;trees__min_samples_split&#39;: 24, &#39;trees__min_samples_leaf&#39;: 7, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:36,766] Trial 80 finished with value: 0.7235075452417924 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 7, &#39;trees__min_samples_leaf&#39;: 10, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:37,998] Trial 81 finished with value: 0.7238078441822915 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 8, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:39,343] Trial 82 finished with value: 0.7238078441822915 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 11, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:40,395] Trial 83 finished with value: 0.723145226572622 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:41,622] Trial 84 finished with value: 0.7235075452417924 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 10, &#39;trees__min_samples_leaf&#39;: 10, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:42,892] Trial 85 finished with value: 0.6981398767233425 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 3, &#39;trees__min_samples_split&#39;: 9, &#39;trees__min_samples_leaf&#39;: 14, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:44,225] Trial 86 finished with value: 0.6947342152750514 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: None, &#39;trees__min_samples_split&#39;: 11, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:45,231] Trial 87 finished with value: 0.7210220308359028 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 5, &#39;trees__min_samples_split&#39;: 6, &#39;trees__min_samples_leaf&#39;: 9, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:46,288] Trial 88 finished with value: 0.7223793741621396 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 13, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:47,662] Trial 89 finished with value: 0.6841642782988554 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 14, &#39;trees__min_samples_leaf&#39;: 8, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:48,536] Trial 90 finished with value: 0.6975602975153743 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 15, &#39;trees__min_samples_split&#39;: 7, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:49,571] Trial 91 finished with value: 0.7238078441822915 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 21, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:50,942] Trial 92 finished with value: 0.7237590315244274 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 23, &#39;trees__min_samples_leaf&#39;: 10, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:51,977] Trial 93 finished with value: 0.723145226572622 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 10, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:53,337] Trial 94 finished with value: 0.7238078441822915 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 18, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:54,548] Trial 95 finished with value: 0.7234224793153615 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 21, &#39;trees__min_samples_leaf&#39;: 9, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:55,633] Trial 96 finished with value: 0.7237590315244274 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 22, &#39;trees__min_samples_leaf&#39;: 10, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:56,797] Trial 97 finished with value: 0.7164945906428697 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 4, &#39;trees__min_samples_split&#39;: 19, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:58,212] Trial 98 finished with value: 0.7206900968776097 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 20, &#39;trees__min_samples_leaf&#39;: 2, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:59,006] Trial 99 finished with value: 0.723145226572622 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 4, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 28 with value: 0.7238078441822915.
[I 2024-03-04 21:56:59,024] A new study created in memory with name: no-name-9155b123-e45d-4786-90ea-534cd7c23525
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>random_over_sampler
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 21:57:00,799] Trial 0 finished with value: 0.7090635435486744 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 10, &#39;trees__min_samples_split&#39;: 17, &#39;trees__min_samples_leaf&#39;: 4, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 0 with value: 0.7090635435486744.
[I 2024-03-04 21:57:02,446] Trial 1 finished with value: 0.7172779248221314 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 5, &#39;trees__min_samples_split&#39;: 7, &#39;trees__min_samples_leaf&#39;: 6, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 1 with value: 0.7172779248221314.
[I 2024-03-04 21:57:04,060] Trial 2 finished with value: 0.7173346786185194 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 5, &#39;trees__min_samples_split&#39;: 14, &#39;trees__min_samples_leaf&#39;: 14, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 2 with value: 0.7173346786185194.
[I 2024-03-04 21:57:05,608] Trial 3 finished with value: 0.7248708472379289 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 19, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:07,548] Trial 4 finished with value: 0.7086505987134094 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 10, &#39;trees__min_samples_split&#39;: 10, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:09,124] Trial 5 finished with value: 0.7184385025053784 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 4, &#39;trees__min_samples_split&#39;: 4, &#39;trees__min_samples_leaf&#39;: 18, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:11,201] Trial 6 finished with value: 0.6423371246018251 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 20, &#39;trees__min_samples_split&#39;: 23, &#39;trees__min_samples_leaf&#39;: 9, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:12,735] Trial 7 finished with value: 0.7184385025053784 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 4, &#39;trees__min_samples_split&#39;: 20, &#39;trees__min_samples_leaf&#39;: 22, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:14,490] Trial 8 finished with value: 0.7245828486141227 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 6, &#39;trees__min_samples_leaf&#39;: 18, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:15,730] Trial 9 finished with value: 0.6983517868496593 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 3, &#39;trees__min_samples_split&#39;: 6, &#39;trees__min_samples_leaf&#39;: 19, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:17,533] Trial 10 finished with value: 0.724359285852624 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 25, &#39;trees__min_samples_leaf&#39;: 2, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:19,148] Trial 11 finished with value: 0.7247086565235912 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 15, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:20,774] Trial 12 finished with value: 0.7248708472379289 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 13, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:23,047] Trial 13 finished with value: 0.6323530240001932 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: None, &#39;trees__min_samples_split&#39;: 17, &#39;trees__min_samples_leaf&#39;: 10, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:25,228] Trial 14 finished with value: 0.6248431737781758 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 16, &#39;trees__min_samples_leaf&#39;: 8, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:27,187] Trial 15 finished with value: 0.6883779534079774 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 15, &#39;trees__min_samples_split&#39;: 19, &#39;trees__min_samples_leaf&#39;: 25, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:28,912] Trial 16 finished with value: 0.7248058932304126 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 11, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:30,304] Trial 17 finished with value: 0.7248142229778431 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 21, &#39;trees__min_samples_leaf&#39;: 17, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:32,377] Trial 18 finished with value: 0.6476539027001245 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: None, &#39;trees__min_samples_split&#39;: 14, &#39;trees__min_samples_leaf&#39;: 15, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:34,529] Trial 19 finished with value: 0.6719735042617782 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 20, &#39;trees__min_samples_split&#39;: 2, &#39;trees__min_samples_leaf&#39;: 21, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:36,661] Trial 20 finished with value: 0.6366739551675084 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 9, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:38,395] Trial 21 finished with value: 0.7248142229778431 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 21, &#39;trees__min_samples_leaf&#39;: 17, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:40,175] Trial 22 finished with value: 0.7246803443935482 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 22, &#39;trees__min_samples_leaf&#39;: 16, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:41,559] Trial 23 finished with value: 0.7247208273039817 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 25, &#39;trees__min_samples_leaf&#39;: 14, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:43,435] Trial 24 finished with value: 0.6845578377449005 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 15, &#39;trees__min_samples_split&#39;: 19, &#39;trees__min_samples_leaf&#39;: 20, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:44,942] Trial 25 finished with value: 0.6983517868496593 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 3, &#39;trees__min_samples_split&#39;: 16, &#39;trees__min_samples_leaf&#39;: 7, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:46,488] Trial 26 finished with value: 0.7248058932304126 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 18, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:48,222] Trial 27 finished with value: 0.7248708472379289 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 13, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:49,648] Trial 28 finished with value: 0.7248708472379289 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 13, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:51,332] Trial 29 finished with value: 0.7091609097917978 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 10, &#39;trees__min_samples_split&#39;: 15, &#39;trees__min_samples_leaf&#39;: 5, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:53,103] Trial 30 finished with value: 0.7247164681258131 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 9, &#39;trees__min_samples_leaf&#39;: 10, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:54,801] Trial 31 finished with value: 0.7248708472379289 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:56,207] Trial 32 finished with value: 0.7173346786185194 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 5, &#39;trees__min_samples_split&#39;: 13, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:57,912] Trial 33 finished with value: 0.7247208273039817 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 13, &#39;trees__min_samples_leaf&#39;: 14, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:57:59,278] Trial 34 finished with value: 0.7172779248221314 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 5, &#39;trees__min_samples_split&#39;: 15, &#39;trees__min_samples_leaf&#39;: 10, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:01,290] Trial 35 finished with value: 0.7102866196817098 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 10, &#39;trees__min_samples_split&#39;: 10, &#39;trees__min_samples_leaf&#39;: 15, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:02,841] Trial 36 finished with value: 0.7185845518700638 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 4, &#39;trees__min_samples_split&#39;: 8, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:05,155] Trial 37 finished with value: 0.6392220074472785 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 20, &#39;trees__min_samples_split&#39;: 11, &#39;trees__min_samples_leaf&#39;: 8, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:06,922] Trial 38 finished with value: 0.7247409392228962 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 14, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:08,372] Trial 39 finished with value: 0.6983517868496593 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 3, &#39;trees__min_samples_split&#39;: 17, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:10,019] Trial 40 finished with value: 0.7248142229778431 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 15, &#39;trees__min_samples_leaf&#39;: 17, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:11,742] Trial 41 finished with value: 0.7248708472379289 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:13,304] Trial 42 finished with value: 0.7247086565235912 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 15, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:15,120] Trial 43 finished with value: 0.7247208273039817 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 13, &#39;trees__min_samples_leaf&#39;: 14, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:17,250] Trial 44 finished with value: 0.6366863850205031 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: None, &#39;trees__min_samples_split&#39;: 10, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:19,411] Trial 45 finished with value: 0.6785262220111193 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 15, &#39;trees__min_samples_split&#39;: 16, &#39;trees__min_samples_leaf&#39;: 16, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:20,915] Trial 46 finished with value: 0.7184385025053784 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 4, &#39;trees__min_samples_split&#39;: 11, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:23,097] Trial 47 finished with value: 0.6292135652848657 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 7, &#39;trees__min_samples_leaf&#39;: 9, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:24,875] Trial 48 finished with value: 0.7245135354284377 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 9, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:26,671] Trial 49 finished with value: 0.7248058932304126 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 14, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:28,081] Trial 50 finished with value: 0.7246803443935482 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 18, &#39;trees__min_samples_leaf&#39;: 16, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:29,791] Trial 51 finished with value: 0.7248708472379289 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:31,560] Trial 52 finished with value: 0.7247409392228962 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 10, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:33,308] Trial 53 finished with value: 0.7247208273039817 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 11, &#39;trees__min_samples_leaf&#39;: 14, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:35,482] Trial 54 finished with value: 0.6625297953207027 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 20, &#39;trees__min_samples_split&#39;: 13, &#39;trees__min_samples_leaf&#39;: 18, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:37,095] Trial 55 finished with value: 0.7247086565235912 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 15, &#39;trees__min_samples_leaf&#39;: 15, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:39,129] Trial 56 finished with value: 0.6359934277560612 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: None, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:40,751] Trial 57 finished with value: 0.7172779248221314 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 5, &#39;trees__min_samples_split&#39;: 9, &#39;trees__min_samples_leaf&#39;: 10, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:42,629] Trial 58 finished with value: 0.7094541292917826 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 10, &#39;trees__min_samples_split&#39;: 23, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:44,108] Trial 59 finished with value: 0.7247409392228962 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 5, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:46,303] Trial 60 finished with value: 0.613247782377931 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 16, &#39;trees__min_samples_leaf&#39;: 3, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:47,925] Trial 61 finished with value: 0.7248708472379289 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:49,478] Trial 62 finished with value: 0.7247208273039817 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 14, &#39;trees__min_samples_leaf&#39;: 14, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:50,991] Trial 63 finished with value: 0.7241609714060214 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 25, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:52,510] Trial 64 finished with value: 0.6983517868496593 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 3, &#39;trees__min_samples_split&#39;: 13, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:53,906] Trial 65 finished with value: 0.7248708472379289 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 11, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:55,371] Trial 66 finished with value: 0.7246803443935482 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 8, &#39;trees__min_samples_leaf&#39;: 16, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:57,542] Trial 67 finished with value: 0.6803582257725833 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 15, &#39;trees__min_samples_split&#39;: 15, &#39;trees__min_samples_leaf&#39;: 14, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:58:59,138] Trial 68 finished with value: 0.7184385025053784 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 4, &#39;trees__min_samples_split&#39;: 14, &#39;trees__min_samples_leaf&#39;: 15, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:00,884] Trial 69 finished with value: 0.7247409392228962 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 11, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:02,320] Trial 70 finished with value: 0.7245135354284377 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 9, &#39;trees__min_samples_leaf&#39;: 9, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:03,840] Trial 71 finished with value: 0.7248708472379289 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:05,563] Trial 72 finished with value: 0.7248058932304126 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 13, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:07,205] Trial 73 finished with value: 0.7248708472379289 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:08,774] Trial 74 finished with value: 0.7247086565235912 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 10, &#39;trees__min_samples_leaf&#39;: 15, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:11,091] Trial 75 finished with value: 0.6450288101126446 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 20, &#39;trees__min_samples_split&#39;: 14, &#39;trees__min_samples_leaf&#39;: 10, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:12,850] Trial 76 finished with value: 0.7247208273039817 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 13, &#39;trees__min_samples_leaf&#39;: 14, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:14,680] Trial 77 finished with value: 0.7094541292917826 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 10, &#39;trees__min_samples_split&#39;: 17, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:16,117] Trial 78 finished with value: 0.7172779248221314 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 5, &#39;trees__min_samples_split&#39;: 10, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:18,007] Trial 79 finished with value: 0.7248142229778431 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 15, &#39;trees__min_samples_leaf&#39;: 17, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:19,421] Trial 80 finished with value: 0.6983517868496593 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 3, &#39;trees__min_samples_split&#39;: 20, &#39;trees__min_samples_leaf&#39;: 23, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:21,281] Trial 81 finished with value: 0.7248708472379289 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 11, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:23,121] Trial 82 finished with value: 0.7247208273039817 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 14, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:24,903] Trial 83 finished with value: 0.7248058932304126 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 11, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:26,775] Trial 84 finished with value: 0.7247086565235912 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 13, &#39;trees__min_samples_leaf&#39;: 15, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:28,272] Trial 85 finished with value: 0.7248708472379289 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 11, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:30,611] Trial 86 finished with value: 0.6366863850205031 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: None, &#39;trees__min_samples_split&#39;: 2, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:32,939] Trial 87 finished with value: 0.6439745704694492 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 30, &#39;trees__min_samples_split&#39;: 10, &#39;trees__min_samples_leaf&#39;: 14, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:35,020] Trial 88 finished with value: 0.6785262220111193 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 15, &#39;trees__min_samples_split&#39;: 8, &#39;trees__min_samples_leaf&#39;: 16, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:36,710] Trial 89 finished with value: 0.7247409392228962 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:38,520] Trial 90 finished with value: 0.7248058932304126 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 16, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:40,334] Trial 91 finished with value: 0.7248708472379289 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:42,174] Trial 92 finished with value: 0.7248708472379289 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 13, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:43,937] Trial 93 finished with value: 0.7247208273039817 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 14, &#39;trees__min_samples_leaf&#39;: 14, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:45,666] Trial 94 finished with value: 0.7248708472379289 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 13, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:47,209] Trial 95 finished with value: 0.7184385025053784 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 4, &#39;trees__min_samples_split&#39;: 11, &#39;trees__min_samples_leaf&#39;: 15, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:49,008] Trial 96 finished with value: 0.7248058932304126 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 14, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:50,737] Trial 97 finished with value: 0.7247409392228962 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 9, &#39;trees__min_samples_leaf&#39;: 11, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:52,608] Trial 98 finished with value: 0.7247208273039817 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 12, &#39;trees__min_samples_leaf&#39;: 14, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
[I 2024-03-04 21:59:54,182] Trial 99 finished with value: 0.7248058932304126 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;trees__max_depth&#39;: 7, &#39;trees__min_samples_split&#39;: 13, &#39;trees__min_samples_leaf&#39;: 12, &#39;trees__splitter&#39;: &#39;best&#39;, &#39;trees__criterion&#39;: &#39;entropy&#39;}. Best is trial 3 with value: 0.7248708472379289.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;knn&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_knn_refined</span>

<span class="k">for</span> <span class="n">sampler_name</span> <span class="ow">in</span> <span class="n">samplers_keys</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sampler_name</span><span class="p">)</span>
    
    <span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">imb_pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">],</span>  
                                <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                                <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                                <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                                <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                                <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                                <span class="n">n_trials</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                                <span class="n">random_state</span><span class="o">=</span><span class="mi">777</span><span class="p">)</span>

    <span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

    <span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
    <span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
    <span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 21:59:54,214] A new study created in memory with name: no-name-f0e451d4-abe6-426a-8e76-b5e4605aa03b
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>random_under_sampler
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 21:59:58,708] Trial 0 finished with value: 0.6625975146466697 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 3, &#39;knn__metric&#39;: &#39;cityblock&#39;}. Best is trial 0 with value: 0.6625975146466697.
[I 2024-03-04 22:00:09,916] Trial 1 finished with value: 0.7040577821118746 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 13, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 1 with value: 0.7040577821118746.
[I 2024-03-04 22:00:21,290] Trial 2 finished with value: 0.6894704062603115 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 5, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 1 with value: 0.7040577821118746.
[I 2024-03-04 22:00:49,298] Trial 3 finished with value: 0.6806832154586776 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 9, &#39;knn__metric&#39;: &#39;minkowski&#39;, &#39;knn__p&#39;: 3}. Best is trial 1 with value: 0.7040577821118746.
[I 2024-03-04 22:00:53,386] Trial 4 finished with value: 0.6950751210302727 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 9, &#39;knn__metric&#39;: &#39;minkowski&#39;, &#39;knn__p&#39;: 1}. Best is trial 1 with value: 0.7040577821118746.
[I 2024-03-04 22:00:57,736] Trial 5 finished with value: 0.6641144523291876 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 6, &#39;knn__metric&#39;: &#39;cityblock&#39;}. Best is trial 1 with value: 0.7040577821118746.
[I 2024-03-04 22:01:01,826] Trial 6 finished with value: 0.6879236583322559 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 10, &#39;knn__metric&#39;: &#39;cityblock&#39;}. Best is trial 1 with value: 0.7040577821118746.
[I 2024-03-04 22:01:06,025] Trial 7 finished with value: 0.644470243945693 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 4, &#39;knn__metric&#39;: &#39;cityblock&#39;}. Best is trial 1 with value: 0.7040577821118746.
[I 2024-03-04 22:01:10,144] Trial 8 finished with value: 0.6767425916104948 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 8, &#39;knn__metric&#39;: &#39;cityblock&#39;}. Best is trial 1 with value: 0.7040577821118746.
[I 2024-03-04 22:01:14,339] Trial 9 finished with value: 0.6945952340868539 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 12, &#39;knn__metric&#39;: &#39;cityblock&#39;}. Best is trial 1 with value: 0.7040577821118746.
[I 2024-03-04 22:01:14,344] A new study created in memory with name: no-name-c48fc7f2-43ff-438c-959c-35bd94a8ceeb
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>random_over_sampler
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 22:01:30,396] Trial 0 finished with value: 0.6105607545419764 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 3, &#39;knn__metric&#39;: &#39;cityblock&#39;}. Best is trial 0 with value: 0.6105607545419764.
[I 2024-03-04 22:02:35,764] Trial 1 finished with value: 0.6745123819921471 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 13, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 1 with value: 0.6745123819921471.
[I 2024-03-04 22:03:38,550] Trial 2 finished with value: 0.6504278156027535 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 5, &#39;knn__metric&#39;: &#39;cosine&#39;}. Best is trial 1 with value: 0.6745123819921471.
[I 2024-03-04 22:06:58,089] Trial 3 finished with value: 0.6465823615125704 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 9, &#39;knn__metric&#39;: &#39;minkowski&#39;, &#39;knn__p&#39;: 3}. Best is trial 1 with value: 0.6745123819921471.
[I 2024-03-04 22:07:14,250] Trial 4 finished with value: 0.6529766394683235 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 9, &#39;knn__metric&#39;: &#39;minkowski&#39;, &#39;knn__p&#39;: 1}. Best is trial 1 with value: 0.6745123819921471.
[I 2024-03-04 22:07:31,138] Trial 5 finished with value: 0.6276484964180734 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 6, &#39;knn__metric&#39;: &#39;cityblock&#39;}. Best is trial 1 with value: 0.6745123819921471.
[I 2024-03-04 22:07:48,336] Trial 6 finished with value: 0.648111430551387 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 10, &#39;knn__metric&#39;: &#39;cityblock&#39;}. Best is trial 1 with value: 0.6745123819921471.
[I 2024-03-04 22:08:05,494] Trial 7 finished with value: 0.6060974855066337 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 4, &#39;knn__metric&#39;: &#39;cityblock&#39;}. Best is trial 1 with value: 0.6745123819921471.
[I 2024-03-04 22:08:22,521] Trial 8 finished with value: 0.6423181785096301 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 8, &#39;knn__metric&#39;: &#39;cityblock&#39;}. Best is trial 1 with value: 0.6745123819921471.
[I 2024-03-04 22:08:39,662] Trial 9 finished with value: 0.654392116434167 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;knn__n_neighbors&#39;: 12, &#39;knn__metric&#39;: &#39;cityblock&#39;}. Best is trial 1 with value: 0.6745123819921471.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;extra_trees&#39;</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid_extra_trees_refined</span>

<span class="k">for</span> <span class="n">sampler_name</span> <span class="ow">in</span> <span class="n">samplers_keys</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sampler_name</span><span class="p">)</span>
    
    <span class="n">simple_eval</span> <span class="o">=</span> <span class="n">SimpleEvaluation</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">imb_pipelines</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">],</span>  
                                <span class="n">inner</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> 
                                <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                                <span class="n">search_method</span><span class="o">=</span><span class="s1">&#39;optuna&#39;</span><span class="p">,</span>
                                <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> 
                                <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;maximize&#39;</span><span class="p">,</span> 
                                <span class="n">n_trials</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> 
                                <span class="n">random_state</span><span class="o">=</span><span class="mi">777</span><span class="p">)</span>

    <span class="n">simple_eval</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>

    <span class="n">inner_score</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_score</span>
    <span class="n">best_params</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_best_params</span>
    <span class="n">inner_results</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">sampler_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">simple_eval</span><span class="o">.</span><span class="n">inner_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 22:11:53,131] A new study created in memory with name: no-name-938ab637-555a-46e0-b49d-c7b891553598
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>random_under_sampler
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 22:11:56,424] Trial 0 finished with value: 0.7029992621386949 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 150, &#39;extra_trees__max_depth&#39;: 3, &#39;extra_trees__min_samples_split&#39;: 20, &#39;extra_trees__min_samples_leaf&#39;: 13, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.6}. Best is trial 0 with value: 0.7029992621386949.
[I 2024-03-04 22:11:59,624] Trial 1 finished with value: 0.7164376622540743 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 4, &#39;extra_trees__min_samples_split&#39;: 2, &#39;extra_trees__min_samples_leaf&#39;: 11, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.6}. Best is trial 1 with value: 0.7164376622540743.
[I 2024-03-04 22:12:02,160] Trial 2 finished with value: 0.699962908687878 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 75, &#39;extra_trees__max_depth&#39;: 3, &#39;extra_trees__min_samples_split&#39;: 9, &#39;extra_trees__min_samples_leaf&#39;: 19, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 1 with value: 0.7164376622540743.
[I 2024-03-04 22:12:08,639] Trial 3 finished with value: 0.7260584473202104 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 150, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 7, &#39;extra_trees__min_samples_leaf&#39;: 17, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.6}. Best is trial 3 with value: 0.7260584473202104.
[I 2024-03-04 22:12:11,975] Trial 4 finished with value: 0.7173367455673411 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 4, &#39;extra_trees__min_samples_split&#39;: 14, &#39;extra_trees__min_samples_leaf&#39;: 10, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.6}. Best is trial 3 with value: 0.7260584473202104.
[I 2024-03-04 22:12:15,150] Trial 5 finished with value: 0.7192075933234152 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 75, &#39;extra_trees__max_depth&#39;: 5, &#39;extra_trees__min_samples_split&#39;: 4, &#39;extra_trees__min_samples_leaf&#39;: 18, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 3 with value: 0.7260584473202104.
[I 2024-03-04 22:12:20,174] Trial 6 finished with value: 0.728421521760488 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 17, &#39;extra_trees__min_samples_leaf&#39;: 7, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 6 with value: 0.728421521760488.
[I 2024-03-04 22:12:27,629] Trial 7 finished with value: 0.7292848699503963 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 5, &#39;extra_trees__min_samples_leaf&#39;: 16, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:12:34,336] Trial 8 finished with value: 0.7281450462355613 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 13, &#39;extra_trees__min_samples_leaf&#39;: 15, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:12:42,306] Trial 9 finished with value: 0.7262629344532119 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 10, &#39;extra_trees__min_samples_leaf&#39;: 15, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:12:48,515] Trial 10 finished with value: 0.7108791074637894 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 6, &#39;extra_trees__min_samples_leaf&#39;: 2, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:12:50,649] Trial 11 finished with value: 0.7281214818925933 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 30, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 18, &#39;extra_trees__min_samples_leaf&#39;: 6, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:13:10,956] Trial 12 finished with value: 0.7230069043299615 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 200, &#39;extra_trees__max_depth&#39;: 20, &#39;extra_trees__min_samples_split&#39;: 16, &#39;extra_trees__min_samples_leaf&#39;: 7, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.9}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:13:19,589] Trial 13 finished with value: 0.7271142413990317 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 12, &#39;extra_trees__min_samples_leaf&#39;: 8, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:13:24,471] Trial 14 finished with value: 0.7277437990915834 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 16, &#39;extra_trees__min_samples_leaf&#39;: 3, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:13:36,497] Trial 15 finished with value: 0.7239197128592444 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 7, &#39;extra_trees__min_samples_leaf&#39;: 5, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:13:44,901] Trial 16 finished with value: 0.7166053779733003 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 200, &#39;extra_trees__max_depth&#39;: 5, &#39;extra_trees__min_samples_split&#39;: 2, &#39;extra_trees__min_samples_leaf&#39;: 9, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:13:48,251] Trial 17 finished with value: 0.7243991210815466 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 30, &#39;extra_trees__max_depth&#39;: 20, &#39;extra_trees__min_samples_split&#39;: 20, &#39;extra_trees__min_samples_leaf&#39;: 13, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.9}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:13:52,526] Trial 18 finished with value: 0.727820146661677 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 15, &#39;extra_trees__min_samples_leaf&#39;: 20, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:13:57,492] Trial 19 finished with value: 0.7274875650218734 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 9, &#39;extra_trees__min_samples_leaf&#39;: 12, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:14:02,035] Trial 20 finished with value: 0.7268986197760573 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 5, &#39;extra_trees__min_samples_leaf&#39;: 16, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:14:09,068] Trial 21 finished with value: 0.7281450462355613 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 12, &#39;extra_trees__min_samples_leaf&#39;: 15, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:14:15,800] Trial 22 finished with value: 0.7282588129009415 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 13, &#39;extra_trees__min_samples_leaf&#39;: 14, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:14:22,642] Trial 23 finished with value: 0.7276820384355159 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 16, &#39;extra_trees__min_samples_leaf&#39;: 13, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:14:31,207] Trial 24 finished with value: 0.7263062221061878 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 18, &#39;extra_trees__min_samples_leaf&#39;: 4, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:14:41,088] Trial 25 finished with value: 0.7261524906755827 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 18, &#39;extra_trees__min_samples_leaf&#39;: 10, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.9}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:14:43,396] Trial 26 finished with value: 0.7137866342460958 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 4, &#39;extra_trees__min_samples_split&#39;: 11, &#39;extra_trees__min_samples_leaf&#39;: 17, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:14:46,911] Trial 27 finished with value: 0.6996939856925689 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 3, &#39;extra_trees__min_samples_split&#39;: 14, &#39;extra_trees__min_samples_leaf&#39;: 14, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:14:53,799] Trial 28 finished with value: 0.7275851903376012 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 75, &#39;extra_trees__max_depth&#39;: 20, &#39;extra_trees__min_samples_split&#39;: 8, &#39;extra_trees__min_samples_leaf&#39;: 8, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:14:57,880] Trial 29 finished with value: 0.7005823231247689 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 150, &#39;extra_trees__max_depth&#39;: 3, &#39;extra_trees__min_samples_split&#39;: 20, &#39;extra_trees__min_samples_leaf&#39;: 12, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:15:01,110] Trial 30 finished with value: 0.7247404210776875 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 30, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 4, &#39;extra_trees__min_samples_leaf&#39;: 17, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:15:07,816] Trial 31 finished with value: 0.7281450462355613 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 13, &#39;extra_trees__min_samples_leaf&#39;: 15, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:15:14,425] Trial 32 finished with value: 0.7282588129009415 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 13, &#39;extra_trees__min_samples_leaf&#39;: 14, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:15:21,262] Trial 33 finished with value: 0.7276820384355159 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 10, &#39;extra_trees__min_samples_leaf&#39;: 13, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:15:27,221] Trial 34 finished with value: 0.7207034334847195 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 200, &#39;extra_trees__max_depth&#39;: 5, &#39;extra_trees__min_samples_split&#39;: 17, &#39;extra_trees__min_samples_leaf&#39;: 11, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.6}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:15:34,502] Trial 35 finished with value: 0.7275772491990771 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 150, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 14, &#39;extra_trees__min_samples_leaf&#39;: 19, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:15:37,590] Trial 36 finished with value: 0.7165309283916338 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 4, &#39;extra_trees__min_samples_split&#39;: 11, &#39;extra_trees__min_samples_leaf&#39;: 16, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.6}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:15:44,117] Trial 37 finished with value: 0.7270086749447797 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 75, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 12, &#39;extra_trees__min_samples_leaf&#39;: 14, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 7 with value: 0.7292848699503963.
[I 2024-03-04 22:15:51,106] Trial 38 finished with value: 0.72975223692861 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 15, &#39;extra_trees__min_samples_leaf&#39;: 11, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 38 with value: 0.72975223692861.
[I 2024-03-04 22:15:54,122] Trial 39 finished with value: 0.7036298561216637 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 3, &#39;extra_trees__min_samples_split&#39;: 15, &#39;extra_trees__min_samples_leaf&#39;: 10, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.6}. Best is trial 38 with value: 0.72975223692861.
[I 2024-03-04 22:15:59,237] Trial 40 finished with value: 0.7289768889439775 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 19, &#39;extra_trees__min_samples_leaf&#39;: 11, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 38 with value: 0.72975223692861.
[I 2024-03-04 22:16:03,905] Trial 41 finished with value: 0.7274875650218734 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 19, &#39;extra_trees__min_samples_leaf&#39;: 12, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 38 with value: 0.72975223692861.
[I 2024-03-04 22:16:08,760] Trial 42 finished with value: 0.7282428010875913 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 17, &#39;extra_trees__min_samples_leaf&#39;: 9, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 38 with value: 0.72975223692861.
[I 2024-03-04 22:16:13,532] Trial 43 finished with value: 0.7289768889439775 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 19, &#39;extra_trees__min_samples_leaf&#39;: 11, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 38 with value: 0.72975223692861.
[I 2024-03-04 22:16:18,303] Trial 44 finished with value: 0.728368738533362 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 19, &#39;extra_trees__min_samples_leaf&#39;: 7, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 38 with value: 0.72975223692861.
[I 2024-03-04 22:16:25,005] Trial 45 finished with value: 0.7269681920343464 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 150, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 19, &#39;extra_trees__min_samples_leaf&#39;: 10, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 38 with value: 0.72975223692861.
[I 2024-03-04 22:16:30,074] Trial 46 finished with value: 0.7289768889439775 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 17, &#39;extra_trees__min_samples_leaf&#39;: 11, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 38 with value: 0.72975223692861.
[I 2024-03-04 22:16:34,773] Trial 47 finished with value: 0.7289768889439775 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 17, &#39;extra_trees__min_samples_leaf&#39;: 11, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 38 with value: 0.72975223692861.
[I 2024-03-04 22:16:38,801] Trial 48 finished with value: 0.7277842820020166 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 75, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 20, &#39;extra_trees__min_samples_leaf&#39;: 9, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 38 with value: 0.72975223692861.
[I 2024-03-04 22:16:40,940] Trial 49 finished with value: 0.7184943946037565 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 30, &#39;extra_trees__max_depth&#39;: 5, &#39;extra_trees__min_samples_split&#39;: 15, &#39;extra_trees__min_samples_leaf&#39;: 11, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.9}. Best is trial 38 with value: 0.72975223692861.
[I 2024-03-04 22:16:40,948] A new study created in memory with name: no-name-452f4df8-48d7-4b4c-9e24-87e0b2373a27
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>random_over_sampler
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2024-03-04 22:16:56,883] Trial 0 finished with value: 0.7054872377343259 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 150, &#39;extra_trees__max_depth&#39;: 3, &#39;extra_trees__min_samples_split&#39;: 20, &#39;extra_trees__min_samples_leaf&#39;: 13, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.6}. Best is trial 0 with value: 0.7054872377343259.
[I 2024-03-04 22:17:14,122] Trial 1 finished with value: 0.7183148967130467 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 4, &#39;extra_trees__min_samples_split&#39;: 2, &#39;extra_trees__min_samples_leaf&#39;: 11, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.6}. Best is trial 1 with value: 0.7183148967130467.
[I 2024-03-04 22:17:24,850] Trial 2 finished with value: 0.7059618080573428 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 75, &#39;extra_trees__max_depth&#39;: 3, &#39;extra_trees__min_samples_split&#39;: 9, &#39;extra_trees__min_samples_leaf&#39;: 19, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 1 with value: 0.7183148967130467.
[I 2024-03-04 22:18:03,206] Trial 3 finished with value: 0.7281798746048045 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 150, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 7, &#39;extra_trees__min_samples_leaf&#39;: 17, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.6}. Best is trial 3 with value: 0.7281798746048045.
[I 2024-03-04 22:18:17,005] Trial 4 finished with value: 0.7186066068335109 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 4, &#39;extra_trees__min_samples_split&#39;: 14, &#39;extra_trees__min_samples_leaf&#39;: 10, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.6}. Best is trial 3 with value: 0.7281798746048045.
[I 2024-03-04 22:18:34,053] Trial 5 finished with value: 0.7225296137445992 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 75, &#39;extra_trees__max_depth&#39;: 5, &#39;extra_trees__min_samples_split&#39;: 4, &#39;extra_trees__min_samples_leaf&#39;: 18, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 3 with value: 0.7281798746048045.
[I 2024-03-04 22:19:01,495] Trial 6 finished with value: 0.728464161731953 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 17, &#39;extra_trees__min_samples_leaf&#39;: 7, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 6 with value: 0.728464161731953.
[I 2024-03-04 22:19:58,870] Trial 7 finished with value: 0.7100802232962548 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 5, &#39;extra_trees__min_samples_leaf&#39;: 16, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 6 with value: 0.728464161731953.
[I 2024-03-04 22:20:41,339] Trial 8 finished with value: 0.7302107954382766 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 13, &#39;extra_trees__min_samples_leaf&#39;: 15, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 8 with value: 0.7302107954382766.
[I 2024-03-04 22:21:33,555] Trial 9 finished with value: 0.7262214546764532 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 10, &#39;extra_trees__min_samples_leaf&#39;: 15, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 8 with value: 0.7302107954382766.
[I 2024-03-04 22:21:57,645] Trial 10 finished with value: 0.725257890444774 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 13, &#39;extra_trees__min_samples_leaf&#39;: 2, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.9}. Best is trial 8 with value: 0.7302107954382766.
[I 2024-03-04 22:22:07,884] Trial 11 finished with value: 0.7269339662905091 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 30, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 18, &#39;extra_trees__min_samples_leaf&#39;: 6, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 8 with value: 0.7302107954382766.
[I 2024-03-04 22:24:19,825] Trial 12 finished with value: 0.6858025689684503 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 200, &#39;extra_trees__max_depth&#39;: 20, &#39;extra_trees__min_samples_split&#39;: 16, &#39;extra_trees__min_samples_leaf&#39;: 7, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 8 with value: 0.7302107954382766.
[I 2024-03-04 22:24:47,015] Trial 13 finished with value: 0.7285411569835577 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 13, &#39;extra_trees__min_samples_leaf&#39;: 8, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 8 with value: 0.7302107954382766.
[I 2024-03-04 22:25:33,535] Trial 14 finished with value: 0.7278285158331997 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 13, &#39;extra_trees__min_samples_leaf&#39;: 13, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.9}. Best is trial 8 with value: 0.7302107954382766.
[I 2024-03-04 22:27:11,842] Trial 15 finished with value: 0.6903452944451386 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 20, &#39;extra_trees__min_samples_split&#39;: 11, &#39;extra_trees__min_samples_leaf&#39;: 9, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 8 with value: 0.7302107954382766.
[I 2024-03-04 22:27:55,206] Trial 16 finished with value: 0.7236826445302418 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 200, &#39;extra_trees__max_depth&#39;: 5, &#39;extra_trees__min_samples_split&#39;: 14, &#39;extra_trees__min_samples_leaf&#39;: 3, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 8 with value: 0.7302107954382766.
[I 2024-03-04 22:28:13,780] Trial 17 finished with value: 0.7019226465073037 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 30, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 8, &#39;extra_trees__min_samples_leaf&#39;: 14, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 8 with value: 0.7302107954382766.
[I 2024-03-04 22:28:35,399] Trial 18 finished with value: 0.7293269467205475 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 12, &#39;extra_trees__min_samples_leaf&#39;: 20, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 8 with value: 0.7302107954382766.
[I 2024-03-04 22:28:57,091] Trial 19 finished with value: 0.7293269467205475 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 11, &#39;extra_trees__min_samples_leaf&#39;: 20, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 8 with value: 0.7302107954382766.
[I 2024-03-04 22:29:19,065] Trial 20 finished with value: 0.7293269467205475 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 16, &#39;extra_trees__min_samples_leaf&#39;: 20, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 8 with value: 0.7302107954382766.
[I 2024-03-04 22:29:40,812] Trial 21 finished with value: 0.7293269467205475 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 11, &#39;extra_trees__min_samples_leaf&#39;: 20, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 8 with value: 0.7302107954382766.
[I 2024-03-04 22:30:02,468] Trial 22 finished with value: 0.7303691451196546 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 11, &#39;extra_trees__min_samples_leaf&#39;: 18, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:30:25,671] Trial 23 finished with value: 0.7290133111729385 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 7, &#39;extra_trees__min_samples_leaf&#39;: 17, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:30:47,646] Trial 24 finished with value: 0.7290133111729385 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 15, &#39;extra_trees__min_samples_leaf&#39;: 17, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:31:09,489] Trial 25 finished with value: 0.7290619942945004 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 12, &#39;extra_trees__min_samples_leaf&#39;: 15, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:31:30,928] Trial 26 finished with value: 0.7303691451196546 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 9, &#39;extra_trees__min_samples_leaf&#39;: 18, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:32:25,652] Trial 27 finished with value: 0.7259776504597379 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 8, &#39;extra_trees__min_samples_leaf&#39;: 12, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.9}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:34:56,061] Trial 28 finished with value: 0.7082721724866653 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 200, &#39;extra_trees__max_depth&#39;: 30, &#39;extra_trees__min_samples_split&#39;: 9, &#39;extra_trees__min_samples_leaf&#39;: 18, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:35:15,272] Trial 29 finished with value: 0.7086388615980305 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 150, &#39;extra_trees__max_depth&#39;: 3, &#39;extra_trees__min_samples_split&#39;: 20, &#39;extra_trees__min_samples_leaf&#39;: 14, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:35:34,353] Trial 30 finished with value: 0.7093068183561546 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 30, &#39;extra_trees__max_depth&#39;: 20, &#39;extra_trees__min_samples_split&#39;: 5, &#39;extra_trees__min_samples_leaf&#39;: 16, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:35:55,585] Trial 31 finished with value: 0.7295250020945456 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 10, &#39;extra_trees__min_samples_leaf&#39;: 19, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:36:04,909] Trial 32 finished with value: 0.715946167731579 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 4, &#39;extra_trees__min_samples_split&#39;: 10, &#39;extra_trees__min_samples_leaf&#39;: 18, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:36:35,877] Trial 33 finished with value: 0.7289729577988074 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 75, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 9, &#39;extra_trees__min_samples_leaf&#39;: 19, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:36:41,691] Trial 34 finished with value: 0.707267365022779 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 3, &#39;extra_trees__min_samples_split&#39;: 7, &#39;extra_trees__min_samples_leaf&#39;: 16, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.6}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:37:42,367] Trial 35 finished with value: 0.7287374833295228 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 150, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 10, &#39;extra_trees__min_samples_leaf&#39;: 19, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:37:51,570] Trial 36 finished with value: 0.7213970384306725 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 50, &#39;extra_trees__max_depth&#39;: 5, &#39;extra_trees__min_samples_split&#39;: 12, &#39;extra_trees__min_samples_leaf&#39;: 18, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.6}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:38:05,085] Trial 37 finished with value: 0.715609615522513 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 75, &#39;extra_trees__max_depth&#39;: 4, &#39;extra_trees__min_samples_split&#39;: 2, &#39;extra_trees__min_samples_leaf&#39;: 12, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:38:54,245] Trial 38 finished with value: 0.7298981961810851 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 6, &#39;extra_trees__min_samples_leaf&#39;: 15, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:39:13,392] Trial 39 finished with value: 0.7051848605809791 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 3, &#39;extra_trees__min_samples_split&#39;: 4, &#39;extra_trees__min_samples_leaf&#39;: 14, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 1.0}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:39:51,690] Trial 40 finished with value: 0.7293004030426298 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 6, &#39;extra_trees__min_samples_leaf&#39;: 11, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.6}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:40:40,519] Trial 41 finished with value: 0.7295902151746665 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 9, &#39;extra_trees__min_samples_leaf&#39;: 17, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:41:30,043] Trial 42 finished with value: 0.7298981961810851 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 8, &#39;extra_trees__min_samples_leaf&#39;: 15, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:42:19,251] Trial 43 finished with value: 0.7298981961810851 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 6, &#39;extra_trees__min_samples_leaf&#39;: 15, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:43:14,150] Trial 44 finished with value: 0.7279625239537967 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 4, &#39;extra_trees__min_samples_leaf&#39;: 15, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.9}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:43:44,064] Trial 45 finished with value: 0.7278962351591667 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 7, &#39;extra_trees__min_samples_split&#39;: 8, &#39;extra_trees__min_samples_leaf&#39;: 13, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:44:05,522] Trial 46 finished with value: 0.7223401471906358 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 5, &#39;extra_trees__min_samples_split&#39;: 6, &#39;extra_trees__min_samples_leaf&#39;: 16, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:44:26,308] Trial 47 finished with value: 0.7159623090812314 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 120, &#39;extra_trees__max_depth&#39;: 4, &#39;extra_trees__min_samples_split&#39;: 14, &#39;extra_trees__min_samples_leaf&#39;: 13, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:45:20,737] Trial 48 finished with value: 0.7297640585241859 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 150, &#39;extra_trees__max_depth&#39;: 10, &#39;extra_trees__min_samples_split&#39;: 2, &#39;extra_trees__min_samples_leaf&#39;: 17, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.7}. Best is trial 22 with value: 0.7303691451196546.
[I 2024-03-04 22:46:25,019] Trial 49 finished with value: 0.6968851486764734 and parameters: {&#39;preprocessing__quant__scaler__apply&#39;: False, &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;, &#39;preprocessing__cat__imputer__apply&#39;: True, &#39;preprocessing__quant__imputer__apply&#39;: True, &#39;features_selector__apply&#39;: True, &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;, &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;, &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;, &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6, &#39;extra_trees__n_estimators&#39;: 100, &#39;extra_trees__max_depth&#39;: 20, &#39;extra_trees__min_samples_split&#39;: 7, &#39;extra_trees__min_samples_leaf&#39;: 10, &#39;extra_trees__criterion&#39;: &#39;gini&#39;, &#39;extra_trees__max_features&#39;: 0.8}. Best is trial 22 with value: 0.7303691451196546.
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Saving the results:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Saving the results as pickle files</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">r = 3</span>

<span class="sd">with open(f&#39;results/params_round_{r}&#39;, &#39;wb&#39;) as file:</span>
<span class="sd">    pickle.dump(best_params, file)</span>

<span class="sd">with open(f&#39;results/inner_scores_round_{r}&#39;, &#39;wb&#39;) as file:</span>
<span class="sd">    pickle.dump(inner_score, file)</span>

<span class="sd">with open(f&#39;results/results_round_{r}&#39;, &#39;wb&#39;) as file:</span>
<span class="sd">    pickle.dump(inner_results, file)</span>
<span class="sd">&#39;&#39;&#39;</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Opening the results</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r</span> <span class="o">=</span> <span class="mi">3</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;results/params_round_</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">best_params</span><span class="p">[</span><span class="n">r</span><span class="p">]</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;results/inner_scores_round_</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">inner_score</span><span class="p">[</span><span class="n">r</span><span class="p">]</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;results/results_round_</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">inner_results</span><span class="p">[</span><span class="n">r</span><span class="p">]</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id9">
<h5><strong>Selecting the best model</strong><a class="headerlink" href="#id9" title="Link to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inner_score_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">inner_score</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="n">model</span><span class="p">][</span><span class="n">sampler</span><span class="p">]</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">inner_score</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">for</span> <span class="n">sampler</span> <span class="ow">in</span> <span class="n">inner_score</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="n">model</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()])</span>
<span class="n">model_names</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">model</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="n">sampler</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">inner_score</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">for</span> <span class="n">sampler</span> <span class="ow">in</span> <span class="n">inner_score</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="n">model</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()])</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="n">model_names</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">inner_score_values</span><span class="p">)]</span>
<span class="n">score_best_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">inner_score_values</span><span class="p">)</span>

<span class="n">combined_models_score</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">model_names</span><span class="p">,</span> <span class="n">inner_score_values</span><span class="p">))</span>
<span class="n">sorted_combined_models_score</span><span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">combined_models_score</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Sort from greater to lower</span>
<span class="n">sorted_models</span><span class="p">,</span> <span class="n">sorted_scores</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">sorted_combined_models_score</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">sorted_models</span><span class="p">),</span> <span class="n">x</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">sorted_scores</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">best_model</span><span class="p">],</span> <span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">score_best_model</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Models&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Balanced Accuracy&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">inner_score_values</span><span class="p">),</span> <span class="mi">7</span><span class="p">),</span><span class="mi">3</span><span class="p">))</span> 
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>    
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Model Selection - 3-Fold CV&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/47541ad04ac5706b02237fe945825be15738dbcb279e4ad1cdeb15aeb7f1b949.png" src="_images/47541ad04ac5706b02237fe945825be15738dbcb279e4ad1cdeb15aeb7f1b949.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_model</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;XGB_random_over_sampler&#39;
</pre></div>
</div>
</div>
</div>
<p>As you can see the over/under sampling techniques have had a crucial impact in the results, increasing the balanced accuracy of all the models an average of 20% (approximately). Now we have a lot of models with a metric close to 0.75, that is the one obtained by logistic regression in the previous section (without using over/under sampling).</p>
<p>Another point to remark is that this sampling techniques seems not have any effect in those models that already worked well in the previous analysis (logistic regression and SVM), because them were already using their own over/under sampling effect assigning to each observation a the relative frequency of the class they belong to as weight.</p>
<p>Now the best model is XGBoost, but we said before, there are other models with a pretty similar performance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_model_name</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">best_sampler_name</span> <span class="o">=</span> <span class="s1">&#39;_&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">best_model</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_model_name</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;XGB&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_sampler_name</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;random_over_sampler&#39;
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="id10">
<h4><strong>Applying outer evaluation</strong><a class="headerlink" href="#id10" title="Link to this heading">#</a></h4>
<section id="id11">
<h5><strong>Estimation of future performance</strong><a class="headerlink" href="#id11" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Estimation performance of the best model (XGB)</strong></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">imb_pipelines</span><span class="p">[</span><span class="n">best_model_name</span><span class="p">][</span><span class="n">best_sampler_name</span><span class="p">]</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">best_params</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="n">best_model_name</span><span class="p">][</span><span class="n">best_sampler_name</span><span class="p">])</span>
<span class="n">imb_pipelines</span><span class="p">[</span><span class="n">best_model_name</span><span class="p">][</span><span class="n">best_sampler_name</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_test_hat</span> <span class="o">=</span> <span class="n">imb_pipelines</span><span class="p">[</span><span class="n">best_model_name</span><span class="p">][</span><span class="n">best_sampler_name</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">estimation_future_performance</span> <span class="o">=</span> <span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_test_hat</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="p">)</span>
<span class="n">estimation_future_performance</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7525094842538036
</pre></div>
</div>
</div>
</div>
<p>We can check the index of the selected features by this pipeline:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">selected_features</span> <span class="o">=</span> <span class="n">imb_pipelines</span><span class="p">[</span><span class="n">best_model_name</span><span class="p">][</span><span class="n">best_sampler_name</span><span class="p">]</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">features_selector_</span><span class="o">.</span><span class="n">get_support</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">selected_features</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
       17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35,
       36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55,
       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67], dtype=int64)
</pre></div>
</div>
</div>
</div>
</section>
<section id="id12">
<h5><strong>Confusion matrix</strong><a class="headerlink" href="#id12" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Confusion matrix of the best model</strong></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">Y_test_hat</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="s1">&#39;true&#39;</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">imb_pipelines</span><span class="p">[</span><span class="n">best_model_name</span><span class="p">][</span><span class="n">best_sampler_name</span><span class="p">]</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="c1"># normalize=&#39;true&#39; to normalize over the rows (true classes)</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">imb_pipelines</span><span class="p">[</span><span class="n">best_model_name</span><span class="p">][</span><span class="n">best_sampler_name</span><span class="p">]</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">,</span> <span class="n">values_format</span><span class="o">=</span><span class="s1">&#39;.3f&#39;</span><span class="p">,</span> <span class="n">text_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;fontsize&#39;</span><span class="p">:</span> <span class="mi">13</span><span class="p">})</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Confusion Matrix </span><span class="se">\n</span><span class="s1"> XGBoost&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">axs</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted class&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">axs</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;True class&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e1aac0f8fc722091a0fc3d6d4c48a42ea84fd5d4591f906433440685e2ff4111.png" src="_images/e1aac0f8fc722091a0fc3d6d4c48a42ea84fd5d4591f906433440685e2ff4111.png" />
</div>
</div>
</section>
</section>
</section>
<section id="trying-with-sequential-feature-selection-based-on-logistic-regression">
<h3><strong>Trying with Sequential Feature Selection based on Logistic Regression</strong><a class="headerlink" href="#trying-with-sequential-feature-selection-based-on-logistic-regression" title="Link to this heading">#</a></h3>
<p>In this section we are going to try several pipelines that incorporate sequential feature selection based on the logistic regression model (both backward and forward algorithms will be applied).</p>
<p>The idea is tu apply an strong feature selection method (a method that usually selects features) and see what happens with their inner performance, to compere them with the previous results.</p>
<p>The estimators for those pipelines will be the best one of each of the previous round, namely, Logistic Regression and XGBoost. This estimators (o better said, pipelines) will be trained with their corresponding best params (the ones obtained in the third round, the previous one).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">selected_features</span><span class="p">,</span> <span class="n">inner_score</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">best_params</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span> <span class="p">{},</span> <span class="p">{}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We are going to use over sampler as sampling method, in order not to make the process more complex</span>

<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;XGB&#39;</span><span class="p">,</span> <span class="s1">&#39;logistic_reg&#39;</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">feature_method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;forward&#39;</span><span class="p">,</span> <span class="s1">&#39;backward&#39;</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">feature_method</span><span class="p">)</span>

        <span class="n">params</span> <span class="o">=</span> <span class="n">best_params</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="n">model</span><span class="p">][</span><span class="s1">&#39;random_over_sampler&#39;</span><span class="p">]</span>
        <span class="n">params</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;preprocessing__cat__encoder__method&#39;</span><span class="p">:</span> <span class="s1">&#39;ordinal&#39;</span><span class="p">})</span>
        <span class="n">params</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;features_selector__method&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">feature_method</span><span class="si">}</span><span class="s1">_logistic_regression&#39;</span><span class="p">})</span>
        <span class="n">params</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;features_selector__cv&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">})</span>
        <span class="n">imb_pipelines</span><span class="p">[</span><span class="n">model</span><span class="p">][</span><span class="s1">&#39;random_over_sampler&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>

        <span class="c1"># Equivalent way to do this using the pipelines properties</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        imb_pipelines[model][&#39;random_over_sampler&#39;].set_params(**best_params[3][model][&#39;random_over_sampler&#39;])</span>
<span class="sd">        pipelines[&#39;logistic_reg&#39;].set_params(preprocessing__cat__encoder__method = &#39;ordinal&#39;)</span>
<span class="sd">        pipelines[&#39;logistic_reg&#39;].set_params(features_selector__method = f&#39;{feature_method}_logistic_regression&#39;)</span>
<span class="sd">        pipelines[&#39;logistic_reg&#39;].set_params(features_selector__cv = 3)</span>
<span class="sd">        &#39;&#39;&#39;</span>

        <span class="n">imb_pipelines</span><span class="p">[</span><span class="n">model</span><span class="p">][</span><span class="s1">&#39;random_over_sampler&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>
        <span class="n">selected_features_</span> <span class="o">=</span> <span class="n">imb_pipelines</span><span class="p">[</span><span class="n">model</span><span class="p">][</span><span class="s1">&#39;random_over_sampler&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">features_selector_</span><span class="o">.</span><span class="n">get_support</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">selected_features</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">feature_method</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">diabetes_df</span><span class="p">[</span><span class="n">predictors</span><span class="p">]</span><span class="o">.</span><span class="n">columns</span><span class="p">)[</span><span class="n">selected_features_</span><span class="p">]</span>

        <span class="n">inner_score</span><span class="p">[</span><span class="mi">4</span><span class="p">][</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">feature_method</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">,</span> 
                                                                        <span class="n">estimator</span><span class="o">=</span><span class="n">imb_pipelines</span><span class="p">[</span><span class="n">model</span><span class="p">][</span><span class="s1">&#39;random_over_sampler&#39;</span><span class="p">],</span> 
                                                                        <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">inner</span><span class="p">))</span>
        
        <span class="n">best_params</span><span class="p">[</span><span class="mi">4</span><span class="p">][</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">feature_method</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">params</span>
</pre></div>
</div>
</div>
</div>
<p>We save the obtained results to be used in below sections.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Saving the results as pickle files</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">r = 4</span>

<span class="sd">with open(f&#39;results/params_round_{r}&#39;, &#39;wb&#39;) as file:</span>
<span class="sd">    pickle.dump(best_params[r], file)</span>

<span class="sd">with open(f&#39;results/inner_scores_round_{r}&#39;, &#39;wb&#39;) as file:</span>
<span class="sd">    pickle.dump(inner_score[r], file)</span>

<span class="sd">with open(f&#39;results/selected_features&#39;, &#39;wb&#39;) as file:</span>
<span class="sd">    pickle.dump(selected_features, file)</span>
<span class="sd">&#39;&#39;&#39;</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="trying-with-stacking">
<h3><strong>Trying with Stacking</strong><a class="headerlink" href="#trying-with-stacking" title="Link to this heading">#</a></h3>
<p>With the same spirit as in the previous section we are going to try and extra alternative, one that uses stacking algorithm as estimator.</p>
<p>We are going to try with different base models (SVM-XGBoost and NN-XGBoost) but the same meta-model (logistic regression).</p>
<p>The params for the base models (better said, pipelines) will be set with the best params obtained in the third round, and for the meta-model (pipeline) the second round best params will be used.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inner_score</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">best_params</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span> <span class="p">{}</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Defining the base models</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">base_models_SVM_XGB</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;SVM&#39;</span><span class="p">,</span> <span class="n">imb_pipelines</span><span class="p">[</span><span class="s1">&#39;SVM&#39;</span><span class="p">][</span><span class="s1">&#39;random_over_sampler&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">best_params</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="s1">&#39;SVM&#39;</span><span class="p">][</span><span class="s1">&#39;random_over_sampler&#39;</span><span class="p">])),</span>
    <span class="p">(</span><span class="s1">&#39;XGB&#39;</span><span class="p">,</span>  <span class="n">imb_pipelines</span><span class="p">[</span><span class="s1">&#39;XGB&#39;</span><span class="p">][</span><span class="s1">&#39;random_over_sampler&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">best_params</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="s1">&#39;XGB&#39;</span><span class="p">][</span><span class="s1">&#39;random_over_sampler&#39;</span><span class="p">]))</span>
<span class="p">]</span>

<span class="n">base_models_NN_XGB</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;NN&#39;</span><span class="p">,</span> <span class="n">imb_pipelines</span><span class="p">[</span><span class="s1">&#39;NN&#39;</span><span class="p">][</span><span class="s1">&#39;random_under_sampler&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">best_params</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="s1">&#39;NN&#39;</span><span class="p">][</span><span class="s1">&#39;random_under_sampler&#39;</span><span class="p">])),</span>
    <span class="p">(</span><span class="s1">&#39;XGB&#39;</span><span class="p">,</span>  <span class="n">imb_pipelines</span><span class="p">[</span><span class="s1">&#39;XGB&#39;</span><span class="p">][</span><span class="s1">&#39;random_over_sampler&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">best_params</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="s1">&#39;XGB&#39;</span><span class="p">][</span><span class="s1">&#39;random_over_sampler&#39;</span><span class="p">]))</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Defining the stacking estimators</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logistic_params</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;__&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">value</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">best_params</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="s1">&#39;logistic_reg&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s1">&#39;logistic&#39;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stacking_SVM_XGB</span> <span class="o">=</span> <span class="n">StackingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="n">base_models_SVM_XGB</span><span class="p">,</span> 
                   <span class="n">final_estimator</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">()</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">logistic_params</span><span class="p">),</span> 
                   <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stacking_NN_XGB</span> <span class="o">=</span> <span class="n">StackingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="n">base_models_NN_XGB</span><span class="p">,</span> 
                   <span class="n">final_estimator</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">()</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">logistic_params</span><span class="p">),</span> 
                   <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Computing the inner score of each stacking estimator</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inner_score</span><span class="p">[</span><span class="mi">5</span><span class="p">][</span><span class="s1">&#39;stacking_SVM_XGB&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">,</span> 
                                                            <span class="n">estimator</span><span class="o">=</span><span class="n">stacking_SVM_XGB</span><span class="p">,</span> 
                                                            <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">inner</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `&#39;auto&#39;` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\svm\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inner_score</span><span class="p">[</span><span class="mi">5</span><span class="p">][</span><span class="s1">&#39;stacking_NN_XGB&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">,</span> 
                                                            <span class="n">estimator</span><span class="o">=</span><span class="n">stacking_NN_XGB</span><span class="p">,</span> 
                                                            <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">inner</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
c:\Users\fscielzo\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Saving the results</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inner_score</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;stacking_SVM_XGB&#39;: 0.720075514707991, &#39;stacking_NN_XGB&#39;: 0.7196399122838736}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Saving the results as pickle files</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">r = 5</span>

<span class="sd">with open(f&#39;results/params_round_{r}&#39;, &#39;wb&#39;) as file:</span>
<span class="sd">    pickle.dump(best_params[r], file)</span>

<span class="sd">with open(f&#39;results/inner_scores_round_{r}&#39;, &#39;wb&#39;) as file:</span>
<span class="sd">    pickle.dump(inner_score[r], file)</span>

<span class="sd">&#39;&#39;&#39;</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="selecting-the-best-overall-model">
<h3><strong>Selecting the best overall model</strong><a class="headerlink" href="#selecting-the-best-overall-model" title="Link to this heading">#</a></h3>
<p>In this section we are going to compare the inner performance of all the tried alternatives.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Opening the results</span>
<span class="n">best_params</span><span class="p">,</span> <span class="n">inner_score</span> <span class="o">=</span> <span class="p">{},{}</span>

<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">]:</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;results/params_round_</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">best_params</span><span class="p">[</span><span class="n">r</span><span class="p">]</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;results/inner_scores_round_</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">inner_score</span><span class="p">[</span><span class="n">r</span><span class="p">]</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Preprocessing the information to build a comparative plot</span>
<span class="n">inner_score_values</span><span class="p">,</span> <span class="n">best_params_values</span><span class="p">,</span> <span class="n">model_names</span> <span class="o">=</span> <span class="p">{},</span> <span class="p">{},</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">]:</span> 
    <span class="n">inner_score_values</span><span class="p">[</span><span class="n">r</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">inner_score</span><span class="p">[</span><span class="n">r</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    <span class="n">best_params_values</span><span class="p">[</span><span class="n">r</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">best_params</span><span class="p">[</span><span class="n">r</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    <span class="n">model_names</span><span class="p">[</span><span class="n">r</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">inner_score</span><span class="p">[</span><span class="n">r</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

<span class="n">inner_score_values</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">inner_score</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="n">model</span><span class="p">][</span><span class="n">sampler</span><span class="p">]</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">inner_score</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">for</span> <span class="n">sampler</span> <span class="ow">in</span> <span class="n">inner_score</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="n">model</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()]</span>
<span class="n">best_params_values</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">best_params</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="n">model</span><span class="p">][</span><span class="n">sampler</span><span class="p">]</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">best_params</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">for</span> <span class="n">sampler</span> <span class="ow">in</span> <span class="n">best_params</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="n">model</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()]</span>
<span class="n">model_names</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="n">sampler</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">inner_score</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">for</span> <span class="n">sampler</span> <span class="ow">in</span> <span class="n">inner_score</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="n">model</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()]</span>

<span class="n">inner_score_values_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inner_score_values</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">inner_score_values</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="n">inner_score_values</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">+</span> <span class="n">inner_score_values</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>
<span class="n">best_params_values_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">best_params_values</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">best_params_values</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="n">best_params_values</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
<span class="n">model_names_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">model_names</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">model_names</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="n">model_names</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">+</span>  <span class="n">model_names</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>

<span class="n">best_model_all</span> <span class="o">=</span> <span class="n">model_names_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">inner_score_values_</span><span class="p">)]</span>
<span class="n">score_best_model_all</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">inner_score_values_</span><span class="p">)</span>

<span class="n">combined_models_score</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">model_names_</span><span class="p">,</span> <span class="n">inner_score_values_</span><span class="p">))</span>
<span class="n">sorted_combined_models_score</span><span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">combined_models_score</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Sort from greater to lower</span>
<span class="n">sorted_models</span><span class="p">,</span> <span class="n">sorted_scores</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">sorted_combined_models_score</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">sorted_models</span><span class="p">),</span> <span class="n">x</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">sorted_scores</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">best_model_all</span><span class="p">],</span> <span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">score_best_model_all</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Models&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Balanced Accuracy&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">inner_score_values_</span><span class="p">),</span> <span class="mi">7</span><span class="p">),</span><span class="mi">3</span><span class="p">))</span> 
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>    
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Model Selection - 3-Fold CV&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/94d292bf01d61ae39d63ea6bacd3163e87654e77f616e596eeb4dd6a8ee26a9e.png" src="_images/94d292bf01d61ae39d63ea6bacd3163e87654e77f616e596eeb4dd6a8ee26a9e.png" />
</div>
</div>
<p>The top-performing model is the <code class="docutils literal notranslate"><span class="pre">'XGB_random_over_sampler'</span></code>, closely followed by the <code class="docutils literal notranslate"><span class="pre">'HGB_random_over_sampler'</span></code> model, with balanced accuracy scores of approximately <strong>0.748</strong>. These models utilize random oversampling to address class imbalance, which appears to have positively influenced their performance.</p>
<p>The plot also includes results for different feature selection methods <em>(forward and backward)</em> applied to XGBoost and Logistic Regression, as well as stacking models that combine SVM and XGBoost or NN and XGBoost. While these approaches show good performance, they do not outperform the leading models.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_model_all</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;XGB_random_over_sampler&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We can gather all the information in the same dictionaries</span>
<span class="n">inner_score_all</span><span class="p">,</span> <span class="n">best_params_all</span> <span class="o">=</span> <span class="p">{},</span> <span class="p">{}</span>
<span class="n">inner_score_all</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">model_names_</span><span class="p">,</span> <span class="n">inner_score_values_</span><span class="p">))</span>
<span class="n">best_params_all</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">model_names_</span><span class="p">,</span> <span class="n">best_params_values_</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>We can see the parameters of the best overall pipeline:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_params_all</span><span class="p">[</span><span class="n">best_model_all</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;preprocessing__quant__scaler__apply&#39;: False,
 &#39;preprocessing__cat__encoder__method&#39;: &#39;one-hot&#39;,
 &#39;preprocessing__cat__imputer__apply&#39;: True,
 &#39;preprocessing__quant__imputer__apply&#39;: True,
 &#39;features_selector__apply&#39;: True,
 &#39;features_selector__method&#39;: &#39;Fdr_f_class&#39;,
 &#39;preprocessing__quant__imputer__method&#39;: &#39;iterative_median&#39;,
 &#39;preprocessing__cat__imputer__method&#39;: &#39;simple_most_frequent&#39;,
 &#39;preprocessing__quant__imputer__n_nearest_features&#39;: 6,
 &#39;XGB__max_depth&#39;: 3,
 &#39;XGB__reg_lambda&#39;: 0.062444976756626386,
 &#39;XGB__n_estimators&#39;: 170,
 &#39;XGB__eta&#39;: 0.09498009017491364,
 &#39;XGB__alpha&#39;: 0.15054242055078138}
</pre></div>
</div>
</div>
</div>
<p>In conclusion, the first part of this analysis involved an extensive evaluation of various machine learning models and techniques to handle class imbalance, including hyperparameter optimization, feature selection methods, and ensemble strategies like stacking. The comparison of models based on 3-fold cross-validation resulted in the selection of the <code class="docutils literal notranslate"><span class="pre">'XGB_random_over_sampler'</span></code> model as the top performer in terms of balanced accuracy. This model, an XGBoost classifier with random oversampling, strikes the best balance between correctly predicting both classes.</p>
</section>
<section id="probabilistic-predictions">
<h3><strong>Probabilistic predictions</strong><a class="headerlink" href="#probabilistic-predictions" title="Link to this heading">#</a></h3>
<p>In this section we are going to make probabilistic predictions of the response using the logistic regression model. That is, we are going to estimate the risk of diabetes disease for the testing patients.</p>
<p>A table comparing the true classes vs predicted probabilities for each class vs predicted classes will be displayed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logistic_regression</span> <span class="o">=</span> <span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;logistic_reg&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">best_params_all</span><span class="p">[</span><span class="s1">&#39;logistic_reg&#39;</span><span class="p">])</span>
<span class="n">logistic_regression</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-11 {color: black;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-11" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,
                 ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                                  Pipeline(steps=[(&#x27;imputer&#x27;,
                                                                   imputer(method=&#x27;iterative_median&#x27;,
                                                                           n_nearest_features=6)),
                                                                  (&#x27;scaler&#x27;,
                                                                   scaler(apply=True))]),
                                                  [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;,
                                                   &#x27;PhysHlth&#x27;]),
                                                 (&#x27;cat&#x27;,
                                                  Pipeline(steps=[(&#x27;encoder&#x27;,
                                                                   encoder(method=&#x27;one-hot&#x27;)),
                                                                  (&#x27;imputer&#x27;,
                                                                   imputer(method=&#x27;simple_most_frequent&#x27;))]),
                                                  [&#x27;HighBP&#x27;, &#x27;High...
                                                   &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;,
                                                   &#x27;Veggies&#x27;,
                                                   &#x27;HvyAlcoholConsump&#x27;,
                                                   &#x27;AnyHealthcare&#x27;,
                                                   &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;,
                                                   &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;,
                                                   &#x27;Education&#x27;, &#x27;Income&#x27;])])),
                (&#x27;features_selector&#x27;,
                 features_selector(apply=True, cv=2, k=10, method=&#x27;Fdr_f_class&#x27;,
                                   n_jobs=-1)),
                (&#x27;logistic_reg&#x27;,
                 LogisticRegression(C=1.3338418110720158,
                                    class_weight=&#x27;balanced&#x27;, max_iter=250,
                                    random_state=123, solver=&#x27;saga&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-40" type="checkbox" ><label for="sk-estimator-id-40" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,
                 ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                                  Pipeline(steps=[(&#x27;imputer&#x27;,
                                                                   imputer(method=&#x27;iterative_median&#x27;,
                                                                           n_nearest_features=6)),
                                                                  (&#x27;scaler&#x27;,
                                                                   scaler(apply=True))]),
                                                  [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;,
                                                   &#x27;PhysHlth&#x27;]),
                                                 (&#x27;cat&#x27;,
                                                  Pipeline(steps=[(&#x27;encoder&#x27;,
                                                                   encoder(method=&#x27;one-hot&#x27;)),
                                                                  (&#x27;imputer&#x27;,
                                                                   imputer(method=&#x27;simple_most_frequent&#x27;))]),
                                                  [&#x27;HighBP&#x27;, &#x27;High...
                                                   &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;,
                                                   &#x27;Veggies&#x27;,
                                                   &#x27;HvyAlcoholConsump&#x27;,
                                                   &#x27;AnyHealthcare&#x27;,
                                                   &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;,
                                                   &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;,
                                                   &#x27;Education&#x27;, &#x27;Income&#x27;])])),
                (&#x27;features_selector&#x27;,
                 features_selector(apply=True, cv=2, k=10, method=&#x27;Fdr_f_class&#x27;,
                                   n_jobs=-1)),
                (&#x27;logistic_reg&#x27;,
                 LogisticRegression(C=1.3338418110720158,
                                    class_weight=&#x27;balanced&#x27;, max_iter=250,
                                    random_state=123, solver=&#x27;saga&#x27;))])</pre></div></div></div><div class="sk-serial"><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-41" type="checkbox" ><label for="sk-estimator-id-41" class="sk-toggleable__label sk-toggleable__label-arrow">preprocessing: ColumnTransformer</label><div class="sk-toggleable__content"><pre>ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                 Pipeline(steps=[(&#x27;imputer&#x27;,
                                                  imputer(method=&#x27;iterative_median&#x27;,
                                                          n_nearest_features=6)),
                                                 (&#x27;scaler&#x27;,
                                                  scaler(apply=True))]),
                                 [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;, &#x27;PhysHlth&#x27;]),
                                (&#x27;cat&#x27;,
                                 Pipeline(steps=[(&#x27;encoder&#x27;,
                                                  encoder(method=&#x27;one-hot&#x27;)),
                                                 (&#x27;imputer&#x27;,
                                                  imputer(method=&#x27;simple_most_frequent&#x27;))]),
                                 [&#x27;HighBP&#x27;, &#x27;HighChol&#x27;, &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;,
                                  &#x27;Stroke&#x27;, &#x27;HeartDiseaseorAttack&#x27;,
                                  &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;, &#x27;Veggies&#x27;,
                                  &#x27;HvyAlcoholConsump&#x27;, &#x27;AnyHealthcare&#x27;,
                                  &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;, &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;,
                                  &#x27;Age&#x27;, &#x27;Education&#x27;, &#x27;Income&#x27;])])</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-42" type="checkbox" ><label for="sk-estimator-id-42" class="sk-toggleable__label sk-toggleable__label-arrow">quant</label><div class="sk-toggleable__content"><pre>[&#x27;BMI&#x27;, &#x27;MentHlth&#x27;, &#x27;PhysHlth&#x27;]</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-43" type="checkbox" ><label for="sk-estimator-id-43" class="sk-toggleable__label sk-toggleable__label-arrow">imputer</label><div class="sk-toggleable__content"><pre>imputer(method=&#x27;iterative_median&#x27;, n_nearest_features=6)</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-44" type="checkbox" ><label for="sk-estimator-id-44" class="sk-toggleable__label sk-toggleable__label-arrow">scaler</label><div class="sk-toggleable__content"><pre>scaler(apply=True)</pre></div></div></div></div></div></div></div></div><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-45" type="checkbox" ><label for="sk-estimator-id-45" class="sk-toggleable__label sk-toggleable__label-arrow">cat</label><div class="sk-toggleable__content"><pre>[&#x27;HighBP&#x27;, &#x27;HighChol&#x27;, &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;, &#x27;Stroke&#x27;, &#x27;HeartDiseaseorAttack&#x27;, &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;, &#x27;Veggies&#x27;, &#x27;HvyAlcoholConsump&#x27;, &#x27;AnyHealthcare&#x27;, &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;, &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;, &#x27;Education&#x27;, &#x27;Income&#x27;]</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-46" type="checkbox" ><label for="sk-estimator-id-46" class="sk-toggleable__label sk-toggleable__label-arrow">encoder</label><div class="sk-toggleable__content"><pre>encoder(method=&#x27;one-hot&#x27;)</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-47" type="checkbox" ><label for="sk-estimator-id-47" class="sk-toggleable__label sk-toggleable__label-arrow">imputer</label><div class="sk-toggleable__content"><pre>imputer(method=&#x27;simple_most_frequent&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-48" type="checkbox" ><label for="sk-estimator-id-48" class="sk-toggleable__label sk-toggleable__label-arrow">features_selector</label><div class="sk-toggleable__content"><pre>features_selector(apply=True, cv=2, k=10, method=&#x27;Fdr_f_class&#x27;, n_jobs=-1)</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-49" type="checkbox" ><label for="sk-estimator-id-49" class="sk-toggleable__label sk-toggleable__label-arrow">LogisticRegression</label><div class="sk-toggleable__content"><pre>LogisticRegression(C=1.3338418110720158, class_weight=&#x27;balanced&#x27;, max_iter=250,
                   random_state=123, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y_test_hat</span> <span class="o">=</span> <span class="n">logistic_regression</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y_test_hat_prob</span> <span class="o">=</span> <span class="n">logistic_regression</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pl</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Y_test&#39;</span><span class="p">:</span> <span class="n">Y_test</span><span class="p">,</span> <span class="s1">&#39;predicted_prob_0&#39;</span><span class="p">:</span> <span class="n">Y_test_hat_prob</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
              <span class="s1">&#39;predicted_prob_1&#39;</span><span class="p">:</span> <span class="n">Y_test_hat_prob</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;Y_test_hat&#39;</span><span class="p">:</span> <span class="n">Y_test_hat</span><span class="p">})</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (15_855, 4)</small><table border="1" class="dataframe"><thead><tr><th>Y_test</th><th>predicted_prob_0</th><th>predicted_prob_1</th><th>Y_test_hat</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.0</td><td>0.951446</td><td>0.048554</td><td>0.0</td></tr><tr><td>0.0</td><td>0.84114</td><td>0.15886</td><td>0.0</td></tr><tr><td>0.0</td><td>0.606068</td><td>0.393932</td><td>0.0</td></tr><tr><td>0.0</td><td>0.711302</td><td>0.288698</td><td>0.0</td></tr><tr><td>1.0</td><td>0.347687</td><td>0.652313</td><td>1.0</td></tr><tr><td>0.0</td><td>0.567142</td><td>0.432858</td><td>0.0</td></tr><tr><td>0.0</td><td>0.449898</td><td>0.550102</td><td>1.0</td></tr><tr><td>0.0</td><td>0.877387</td><td>0.122613</td><td>0.0</td></tr><tr><td>0.0</td><td>0.60967</td><td>0.39033</td><td>0.0</td></tr><tr><td>0.0</td><td>0.697435</td><td>0.302565</td><td>0.0</td></tr><tr><td>0.0</td><td>0.921616</td><td>0.078384</td><td>0.0</td></tr><tr><td>1.0</td><td>0.204915</td><td>0.795085</td><td>1.0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0.0</td><td>0.140638</td><td>0.859362</td><td>1.0</td></tr><tr><td>0.0</td><td>0.539582</td><td>0.460418</td><td>0.0</td></tr><tr><td>0.0</td><td>0.577426</td><td>0.422574</td><td>0.0</td></tr><tr><td>0.0</td><td>0.897937</td><td>0.102063</td><td>0.0</td></tr><tr><td>0.0</td><td>0.79545</td><td>0.20455</td><td>0.0</td></tr><tr><td>0.0</td><td>0.895281</td><td>0.104719</td><td>0.0</td></tr><tr><td>0.0</td><td>0.98079</td><td>0.01921</td><td>0.0</td></tr><tr><td>0.0</td><td>0.923824</td><td>0.076176</td><td>0.0</td></tr><tr><td>0.0</td><td>0.627708</td><td>0.372292</td><td>0.0</td></tr><tr><td>0.0</td><td>0.857841</td><td>0.142159</td><td>0.0</td></tr><tr><td>0.0</td><td>0.874291</td><td>0.125709</td><td>0.0</td></tr><tr><td>1.0</td><td>0.831057</td><td>0.168943</td><td>0.0</td></tr></tbody></table></div></div></div>
</div>
</section>
<section id="saving-the-best-model">
<h3><strong>Saving the best model</strong><a class="headerlink" href="#saving-the-best-model" title="Link to this heading">#</a></h3>
<p>Once we have a best model we can save it as a pretrained model, that is, as a model that have been already trained and is ready to make predictions for new data.</p>
<p>Despite the best model was XGboost, we use here Logistic Regression because its performance is almost the same.</p>
<ul class="simple">
<li><p>Train the model with all the available data</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logistic_regression</span> <span class="o">=</span> <span class="n">pipelines</span><span class="p">[</span><span class="s1">&#39;logistic_reg&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">best_params_all</span><span class="p">[</span><span class="s1">&#39;logistic_reg&#39;</span><span class="p">])</span>
<span class="n">logistic_regression</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-12 {color: black;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-12" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,
                 ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                                  Pipeline(steps=[(&#x27;imputer&#x27;,
                                                                   imputer(method=&#x27;iterative_median&#x27;,
                                                                           n_nearest_features=6)),
                                                                  (&#x27;scaler&#x27;,
                                                                   scaler(apply=True))]),
                                                  [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;,
                                                   &#x27;PhysHlth&#x27;]),
                                                 (&#x27;cat&#x27;,
                                                  Pipeline(steps=[(&#x27;encoder&#x27;,
                                                                   encoder(method=&#x27;one-hot&#x27;)),
                                                                  (&#x27;imputer&#x27;,
                                                                   imputer(method=&#x27;simple_most_frequent&#x27;))]),
                                                  [&#x27;HighBP&#x27;, &#x27;High...
                                                   &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;,
                                                   &#x27;Veggies&#x27;,
                                                   &#x27;HvyAlcoholConsump&#x27;,
                                                   &#x27;AnyHealthcare&#x27;,
                                                   &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;,
                                                   &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;,
                                                   &#x27;Education&#x27;, &#x27;Income&#x27;])])),
                (&#x27;features_selector&#x27;,
                 features_selector(apply=True, cv=2, k=10, method=&#x27;Fdr_f_class&#x27;,
                                   n_jobs=-1)),
                (&#x27;logistic_reg&#x27;,
                 LogisticRegression(C=1.3338418110720158,
                                    class_weight=&#x27;balanced&#x27;, max_iter=250,
                                    random_state=123, solver=&#x27;saga&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-50" type="checkbox" ><label for="sk-estimator-id-50" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,
                 ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                                  Pipeline(steps=[(&#x27;imputer&#x27;,
                                                                   imputer(method=&#x27;iterative_median&#x27;,
                                                                           n_nearest_features=6)),
                                                                  (&#x27;scaler&#x27;,
                                                                   scaler(apply=True))]),
                                                  [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;,
                                                   &#x27;PhysHlth&#x27;]),
                                                 (&#x27;cat&#x27;,
                                                  Pipeline(steps=[(&#x27;encoder&#x27;,
                                                                   encoder(method=&#x27;one-hot&#x27;)),
                                                                  (&#x27;imputer&#x27;,
                                                                   imputer(method=&#x27;simple_most_frequent&#x27;))]),
                                                  [&#x27;HighBP&#x27;, &#x27;High...
                                                   &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;,
                                                   &#x27;Veggies&#x27;,
                                                   &#x27;HvyAlcoholConsump&#x27;,
                                                   &#x27;AnyHealthcare&#x27;,
                                                   &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;,
                                                   &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;,
                                                   &#x27;Education&#x27;, &#x27;Income&#x27;])])),
                (&#x27;features_selector&#x27;,
                 features_selector(apply=True, cv=2, k=10, method=&#x27;Fdr_f_class&#x27;,
                                   n_jobs=-1)),
                (&#x27;logistic_reg&#x27;,
                 LogisticRegression(C=1.3338418110720158,
                                    class_weight=&#x27;balanced&#x27;, max_iter=250,
                                    random_state=123, solver=&#x27;saga&#x27;))])</pre></div></div></div><div class="sk-serial"><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-51" type="checkbox" ><label for="sk-estimator-id-51" class="sk-toggleable__label sk-toggleable__label-arrow">preprocessing: ColumnTransformer</label><div class="sk-toggleable__content"><pre>ColumnTransformer(transformers=[(&#x27;quant&#x27;,
                                 Pipeline(steps=[(&#x27;imputer&#x27;,
                                                  imputer(method=&#x27;iterative_median&#x27;,
                                                          n_nearest_features=6)),
                                                 (&#x27;scaler&#x27;,
                                                  scaler(apply=True))]),
                                 [&#x27;BMI&#x27;, &#x27;MentHlth&#x27;, &#x27;PhysHlth&#x27;]),
                                (&#x27;cat&#x27;,
                                 Pipeline(steps=[(&#x27;encoder&#x27;,
                                                  encoder(method=&#x27;one-hot&#x27;)),
                                                 (&#x27;imputer&#x27;,
                                                  imputer(method=&#x27;simple_most_frequent&#x27;))]),
                                 [&#x27;HighBP&#x27;, &#x27;HighChol&#x27;, &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;,
                                  &#x27;Stroke&#x27;, &#x27;HeartDiseaseorAttack&#x27;,
                                  &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;, &#x27;Veggies&#x27;,
                                  &#x27;HvyAlcoholConsump&#x27;, &#x27;AnyHealthcare&#x27;,
                                  &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;, &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;,
                                  &#x27;Age&#x27;, &#x27;Education&#x27;, &#x27;Income&#x27;])])</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-52" type="checkbox" ><label for="sk-estimator-id-52" class="sk-toggleable__label sk-toggleable__label-arrow">quant</label><div class="sk-toggleable__content"><pre>[&#x27;BMI&#x27;, &#x27;MentHlth&#x27;, &#x27;PhysHlth&#x27;]</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-53" type="checkbox" ><label for="sk-estimator-id-53" class="sk-toggleable__label sk-toggleable__label-arrow">imputer</label><div class="sk-toggleable__content"><pre>imputer(method=&#x27;iterative_median&#x27;, n_nearest_features=6)</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-54" type="checkbox" ><label for="sk-estimator-id-54" class="sk-toggleable__label sk-toggleable__label-arrow">scaler</label><div class="sk-toggleable__content"><pre>scaler(apply=True)</pre></div></div></div></div></div></div></div></div><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-55" type="checkbox" ><label for="sk-estimator-id-55" class="sk-toggleable__label sk-toggleable__label-arrow">cat</label><div class="sk-toggleable__content"><pre>[&#x27;HighBP&#x27;, &#x27;HighChol&#x27;, &#x27;CholCheck&#x27;, &#x27;Smoker&#x27;, &#x27;Stroke&#x27;, &#x27;HeartDiseaseorAttack&#x27;, &#x27;PhysActivity&#x27;, &#x27;Fruits&#x27;, &#x27;Veggies&#x27;, &#x27;HvyAlcoholConsump&#x27;, &#x27;AnyHealthcare&#x27;, &#x27;NoDocbcCost&#x27;, &#x27;GenHlth&#x27;, &#x27;DiffWalk&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;, &#x27;Education&#x27;, &#x27;Income&#x27;]</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-56" type="checkbox" ><label for="sk-estimator-id-56" class="sk-toggleable__label sk-toggleable__label-arrow">encoder</label><div class="sk-toggleable__content"><pre>encoder(method=&#x27;one-hot&#x27;)</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-57" type="checkbox" ><label for="sk-estimator-id-57" class="sk-toggleable__label sk-toggleable__label-arrow">imputer</label><div class="sk-toggleable__content"><pre>imputer(method=&#x27;simple_most_frequent&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-58" type="checkbox" ><label for="sk-estimator-id-58" class="sk-toggleable__label sk-toggleable__label-arrow">features_selector</label><div class="sk-toggleable__content"><pre>features_selector(apply=True, cv=2, k=10, method=&#x27;Fdr_f_class&#x27;, n_jobs=-1)</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-59" type="checkbox" ><label for="sk-estimator-id-59" class="sk-toggleable__label sk-toggleable__label-arrow">LogisticRegression</label><div class="sk-toggleable__content"><pre>LogisticRegression(C=1.3338418110720158, class_weight=&#x27;balanced&#x27;, max_iter=250,
                   random_state=123, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div></div></div></div></div>
</div>
<ul class="simple">
<li><p>Save the model using <code class="docutils literal notranslate"><span class="pre">joblib</span></code> library</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">joblib</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">logistic_regression</span><span class="p">,</span> <span class="s2">&quot;results/final_model.joblib&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;results/final_model.joblib&#39;]
</pre></div>
</div>
</div>
</div>
</section>
<section id="predicting-new-data">
<h3><strong>Predicting new data</strong><a class="headerlink" href="#predicting-new-data" title="Link to this heading">#</a></h3>
<p>We can load the saved model as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">final_model</span> <span class="o">=</span> <span class="n">joblib</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;results/final_model.joblib&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Imagine that new data comes, but new data only for the predictors, since what we want to predict (the response) is unknown, otherwise it wouldn’t be necessary to predict it.</p>
<p>We read this new data and use our pretrained model for making predictions for the new data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_new</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;diabetes_new.csv&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y_new_hat</span> <span class="o">=</span> <span class="n">final_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_new</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y_new_hat</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1., 0., 0., ..., 0., 1., 0.])
</pre></div>
</div>
</div>
</div>
<p>We cannot compute any accuracy metric to see how well the model is predicting the new data since we have not new data for the response variable, what is common in a realistic scenario.</p>
</section>
</section>
<section id="interpretation-with-logistic-regression">
<h2><strong>Interpretation with Logistic Regression</strong><a class="headerlink" href="#interpretation-with-logistic-regression" title="Link to this heading">#</a></h2>
<p>In this section we are going to make inference/interpretations with the Logistic Regression model. We want to take advantage of the statistical capabilities of this model that allow us to analyze the relationship between the response and the prediction by mean of the (beta) coefficient of the model.</p>
<p>To carry out this analysis we are going to rely on the features selected by sequential feature selection based on Logistic Regression as well. This features are the most important ones from a predictive perspective according to the Logistic Regression model.</p>
<p>In this section <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> framework will be used since is more suitable for making statistical inference and interpretations, since the summary of the model that it provides is much more appropriate for this task.</p>
<p>In the below code we prepare the pipeline to be used.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;results/selected_features&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">selected_features</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We define the data to be used.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictors</span> <span class="o">=</span> <span class="n">selected_features</span><span class="p">[</span><span class="s1">&#39;logistic_reg_backward&#39;</span><span class="p">]</span>
<span class="c1"># we use str() because the elements of predictors array are numpy str which are not compatible with sklearn ColumnTransform</span>
<span class="n">quant_predictors</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">col</span><span class="p">)</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">predictors</span> <span class="k">if</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">quant_columns</span><span class="p">]</span>
<span class="n">cat_predictors</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">col</span><span class="p">)</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">predictors</span> <span class="k">if</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">cat_columns</span><span class="p">]</span>

<span class="n">Y</span> <span class="o">=</span> <span class="n">diabetes_df</span><span class="p">[</span><span class="n">response</span><span class="p">]</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">diabetes_df</span><span class="p">[</span><span class="n">predictors</span><span class="p">]</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span>
<span class="c1"># The Null values of the Polars columns that are define as Object type by Pandas are treated as None and not as NaN (that is what we want)</span>
<span class="c1"># The avoid this behavior the next step is necessary</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>

<span class="n">enc</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">()</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>We define the preprocessing pipeline to be used.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">quant_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;imputer&#39;</span><span class="p">,</span> <span class="n">imputer</span><span class="p">(</span><span class="n">apply</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;iterative_median&#39;</span><span class="p">))</span>
    <span class="p">])</span>

<span class="n">cat_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;encoder&#39;</span><span class="p">,</span> <span class="n">encoder</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;ordinal&#39;</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;imputer&#39;</span><span class="p">,</span> <span class="n">imputer</span><span class="p">(</span><span class="n">apply</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;simple_most_frequent&#39;</span><span class="p">))</span>
    <span class="p">])</span>

<span class="n">quant_cat_processing</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span><span class="n">transformers</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;quant&#39;</span><span class="p">,</span> <span class="n">quant_pipeline</span><span class="p">,</span> <span class="n">quant_predictors</span><span class="p">),</span>
                                                       <span class="p">(</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">cat_pipeline</span><span class="p">,</span> <span class="n">cat_predictors</span><span class="p">)])</span>

<span class="n">interpretation_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">&#39;preprocessing&#39;</span><span class="p">,</span> <span class="n">quant_cat_processing</span><span class="p">)])</span>
</pre></div>
</div>
</div>
</div>
<p>We apply the preprocessing pipeline to the predictors matrix, convert it into a <code class="docutils literal notranslate"><span class="pre">pandas</span></code> data-frame (sth needed by <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>), define the categorical columns as <code class="docutils literal notranslate"><span class="pre">category</span></code> type (to be read by <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> as categorical features), define the dummies associated to the categorical variables (one-hot encoding), and add the intercept.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">interpretation_pipeline</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">quant_predictors</span> <span class="o">+</span> <span class="n">cat_predictors</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">cat_predictors</span><span class="p">:</span>
    <span class="n">X</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;category&#39;</span><span class="p">)</span>
<span class="n">X_dummies</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cat_predictors</span><span class="p">],</span> <span class="n">drop_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">quant_predictors</span><span class="p">],</span> <span class="n">X_dummies</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We fit the model and print its summary:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">Logit</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.335306
         Iterations 8
                           Logit Regression Results                           
==============================================================================
Dep. Variable:                      y   No. Observations:               253680
Model:                          Logit   Df Residuals:                   253651
Method:                           MLE   Df Model:                           28
Date:                Mon, 11 Mar 2024   Pseudo R-squ.:                  0.1695
Time:                        09:13:55   Log-Likelihood:                -85060.
converged:                       True   LL-Null:                   -1.0242e+05
Covariance Type:            nonrobust   LLR p-value:                     0.000
===================================================================================
                      coef    std err          z      P&gt;|z|      [0.025      0.975]
-----------------------------------------------------------------------------------
const              -6.1269      0.124    -49.530      0.000      -6.369      -5.884
BMI                 0.0646      0.001     72.580      0.000       0.063       0.066
PhysHlth            0.0147      0.001     21.581      0.000       0.013       0.016
HighBP_1.0          1.0051      0.014     70.857      0.000       0.977       1.033
Smoker_1.0          0.0249      0.013      1.942      0.052      -0.000       0.050
Stroke_1.0          0.3655      0.024     15.054      0.000       0.318       0.413
Fruits_1.0         -0.0825      0.013     -6.382      0.000      -0.108      -0.057
NoDocbcCost_1.0     0.0546      0.022      2.471      0.013       0.011       0.098
DiffWalk_1.0        0.4112      0.016     25.383      0.000       0.379       0.443
Sex_1.0             0.3052      0.013     23.550      0.000       0.280       0.331
Age_1.0             0.1939      0.147      1.322      0.186      -0.094       0.481
Age_2.0             0.4919      0.132      3.716      0.000       0.232       0.751
Age_3.0             1.0061      0.125      8.033      0.000       0.761       1.252
Age_4.0             1.2999      0.122     10.618      0.000       1.060       1.540
Age_5.0             1.5454      0.121     12.810      0.000       1.309       1.782
Age_6.0             1.7777      0.119     14.885      0.000       1.544       2.012
Age_7.0             1.8897      0.119     15.878      0.000       1.656       2.123
Age_8.0             2.0679      0.118     17.452      0.000       1.836       2.300
Age_9.0             2.2852      0.119     19.253      0.000       2.053       2.518
Age_10.0            2.3411      0.119     19.667      0.000       2.108       2.574
Age_11.0            2.2735      0.120     18.996      0.000       2.039       2.508
Age_12.0            2.1345      0.120     17.816      0.000       1.900       2.369
Income_1.0         -0.0333      0.035     -0.955      0.339      -0.101       0.035
Income_2.0         -0.0975      0.033     -2.914      0.004      -0.163      -0.032
Income_3.0         -0.1644      0.033     -5.042      0.000      -0.228      -0.101
Income_4.0         -0.2830      0.032     -8.848      0.000      -0.346      -0.220
Income_5.0         -0.4167      0.031    -13.346      0.000      -0.478      -0.355
Income_6.0         -0.4891      0.031    -15.639      0.000      -0.550      -0.428
Income_7.0         -0.7418      0.030    -24.408      0.000      -0.801      -0.682
===================================================================================
</pre></div>
</div>
</div>
</div>
<p>The logistic regression model has been successfully fitted to the data with the following key results:</p>
<ol class="arabic simple">
<li><p>The model used <span class="math notranslate nohighlight">\(253,680\)</span> observations to estimate coefficients for <strong>28</strong> predictors after adding a constant term.</p></li>
<li><p>The Pseudo R-squared value is <span class="math notranslate nohighlight">\(0.1695\)</span>, indicating that around <strong>16.95%</strong> of the variance in the response variable is explained by the model.</p></li>
<li><p>The coefficients’ z-scores and their corresponding p-values suggest which variables are statistically significant predictors of the outcome. For instance:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">HighBP_1.0</span></code> has a positive coefficient of <span class="math notranslate nohighlight">\(1.0051\)</span>, with a p-value close to zero, indicating a strong and significant association with the outcome.</p></li>
<li><p>Age groups (<code class="docutils literal notranslate"><span class="pre">Age_1.0</span></code> through <code class="docutils literal notranslate"><span class="pre">Age_12.0</span></code>) show increasing coefficients, suggesting higher odds of diabetes as age increases, with most age groups significantly different from the baseline category.</p></li>
<li><p>Income levels (<code class="docutils literal notranslate"><span class="pre">Income_1.0</span></code> through <code class="docutils literal notranslate"><span class="pre">Income_7.0</span></code>) are inversely related to the outcome, where higher income levels are associated with lower odds of diabetes, with higher income categories showing significant negative coefficients.</p></li>
</ul>
</li>
<li><p>The Log-Likelihood of the model is <span class="math notranslate nohighlight">\(-85,060\)</span>, and the LL-Null is <span class="math notranslate nohighlight">\(-102,420\)</span>, which suggests that the model fits better than a null model with no predictors.</p></li>
<li><p>All variables have confidence intervals that do not cross zero, except for <code class="docutils literal notranslate"><span class="pre">Smoker_1.0</span></code>, <code class="docutils literal notranslate"><span class="pre">Age_1.0</span></code>, and <code class="docutils literal notranslate"><span class="pre">Income_1.0</span></code>, which indicates uncertainty about the direction of their associations.</p></li>
</ol>
<p>Overall, this model provides a statistically significant improvement over the null model, with several predictors showing strong associations with the probability of having diabetes. The results can inform healthcare strategies targeting the most influential factors associated with diabetes risk.</p>
<p>A more in-depth interpretation is provided in the next lines:</p>
<p>Now, we will be focus in the <code class="docutils literal notranslate"><span class="pre">coef</span></code> column of the summary, that contains the estimated values for the beta coefficients of the model. We will use them to make interpretations regarding how is the relationship between the responser and the given predictor.</p>
<p><strong>Interpreting <code class="docutils literal notranslate"><span class="pre">HighBP</span></code>: <span class="math notranslate nohighlight">\(\quad\widehat{\beta}_{HighBP} = 1 &gt; 0\)</span></strong></p>
<div class="math notranslate nohighlight">
\[\dfrac{OR(x_{i} | x_{ij} = 1)}{OR(x_{i} | x_{ij} = 0)} = \dfrac{e^{\widehat{\eta}_i |x_{ijr} = 1} }{e^{\widehat{\eta}_i |x_{ij}=0}} = e^{\widehat{\eta}_i |x_{ijr} = 1 - \widehat{\eta}_i |x_{ij}=0} = e^{\widehat{\beta}_{jr}}\]</div>
<ul class="simple">
<li><p>The possibility of having diabetes (<code class="docutils literal notranslate"><span class="pre">Diabetes=1</span></code>) with respect to not having (<code class="docutils literal notranslate"><span class="pre">Diabetes=0</span></code>) is 2.71 times <strong>greater</strong> when the patient has high blood pressure (<code class="docutils literal notranslate"><span class="pre">HighBP=1</span></code>) than when not (<code class="docutils literal notranslate"><span class="pre">HighBP=0</span></code>).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.718281828459045
</pre></div>
</div>
</div>
</div>
<p><strong>Interpreting <code class="docutils literal notranslate"><span class="pre">BMI</span></code>: <span class="math notranslate nohighlight">\(\quad\widehat{\beta}_{BMI} = 0.065 &gt; 0\)</span></strong></p>
<div class="math notranslate nohighlight">
\[\dfrac{OR(x_{i} | x_{ij}^\prime)}{OR(x_{i} | x_{ij})} =  \dfrac{e^{\widehat{\eta}_i |x'_{ij}} }{e^{\widehat{\eta}_i |x_{ij}}} = e^{\widehat{\eta}_i |x'_{ij} - \widehat{\eta}_i |x_{ij}} = e^{h\cdot \widehat{\beta}_j}\]</div>
<ul class="simple">
<li><p>The possibility of having diabetes (<code class="docutils literal notranslate"><span class="pre">Diabetes=1</span></code>) with respect to not having (<code class="docutils literal notranslate"><span class="pre">Diabetes=0</span></code>) is 1.07 times <strong>greater</strong> when the body max index (<code class="docutils literal notranslate"><span class="pre">BMI</span></code>) of the patient increases in one unit. If it increases in 5 units, the possibility is 1.4 times greater.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="mf">0.065</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0671590243841926
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="p">(</span><span class="mi">5</span><span class="o">*</span><span class="mf">0.065</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.3840306459807514
</pre></div>
</div>
</div>
</div>
<p><strong>Interpreting <code class="docutils literal notranslate"><span class="pre">Smoker</span></code>: <span class="math notranslate nohighlight">\(\quad\widehat{\beta}_{Smoker} = 0.025 &gt; 0\)</span></strong></p>
<ul class="simple">
<li><p>The possibility of having diabetes (<code class="docutils literal notranslate"><span class="pre">Diabetes=1</span></code>) with respect to not having (<code class="docutils literal notranslate"><span class="pre">Diabetes=0</span></code>) is 1.02 times <strong>greater</strong> if the patient is smoker (<code class="docutils literal notranslate"><span class="pre">Smoker=1</span></code>) than if not (<code class="docutils literal notranslate"><span class="pre">Smoker=0</span></code>).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="mf">0.025</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0253151205244289
</pre></div>
</div>
</div>
</div>
<p><strong>Interpreting <code class="docutils literal notranslate"><span class="pre">Stroke</span></code>: <span class="math notranslate nohighlight">\(\quad\widehat{\beta}_{Stroke} = 0.36 &gt; 0\)</span></strong></p>
<ul class="simple">
<li><p>The possibility of having diabetes (<code class="docutils literal notranslate"><span class="pre">Diabetes=1</span></code>) with respect to not having (<code class="docutils literal notranslate"><span class="pre">Diabetes=0</span></code>) is 1.43 times <strong>greater</strong> if the patient had a stroke (<code class="docutils literal notranslate"><span class="pre">Stroke=1</span></code>) than if not (<code class="docutils literal notranslate"><span class="pre">Stroke=0</span></code>).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="mf">0.36</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.4333294145603401
</pre></div>
</div>
</div>
</div>
<p><strong>Interpreting <code class="docutils literal notranslate"><span class="pre">Fruits</span></code>: <span class="math notranslate nohighlight">\(\quad\widehat{\beta}_{Fruits} = -0.083 &lt; 0\)</span></strong></p>
<ul class="simple">
<li><p>The possibility of having diabetes (<code class="docutils literal notranslate"><span class="pre">Diabetes=1</span></code>) with respect to not having (<code class="docutils literal notranslate"><span class="pre">Diabetes=0</span></code>) is 1.09 times <strong>lower</strong> if the patient eats fruits every day (<code class="docutils literal notranslate"><span class="pre">Fruits=1</span></code>) than if not (<code class="docutils literal notranslate"><span class="pre">Fruits=0</span></code>).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**-</span><span class="mf">0.083</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0865418085482381
</pre></div>
</div>
</div>
</div>
<p><strong>Interpreting <code class="docutils literal notranslate"><span class="pre">NoDocbcCost</span></code>: <span class="math notranslate nohighlight">\(\quad\widehat{\beta}_{NoDocbcCost} = 0.055 &gt; 0\)</span></strong></p>
<ul class="simple">
<li><p>The possibility of having diabetes (<code class="docutils literal notranslate"><span class="pre">Diabetes=1</span></code>) with respect to not having (<code class="docutils literal notranslate"><span class="pre">Diabetes=0</span></code>) is 1.06 times <strong>greater</strong> for patients who cannot afford medical visits sometimes  (<code class="docutils literal notranslate"><span class="pre">NoDocbcCost=1</span></code>) than for those that always can (<code class="docutils literal notranslate"><span class="pre">NoDocbcCost=0</span></code>).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="mf">0.055</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0565406146754943
</pre></div>
</div>
</div>
</div>
<p><strong>Interpreting <code class="docutils literal notranslate"><span class="pre">PhysHlth</span></code>: <span class="math notranslate nohighlight">\(\quad\widehat{\beta}_{PhysHlth} = 0.015 &gt; 0\)</span></strong></p>
<ul class="simple">
<li><p>The possibility of having diabetes (<code class="docutils literal notranslate"><span class="pre">Diabetes=1</span></code>) with respect to not having (<code class="docutils literal notranslate"><span class="pre">Diabetes=0</span></code>) is 1.02 times <strong>greater</strong> for patients who had one day of not good physical health in the las 30 days  (<code class="docutils literal notranslate"><span class="pre">PhysHlth=1</span></code>). For patients with 25 days of not good physical health (<code class="docutils literal notranslate"><span class="pre">PhysHlth=20</span></code>) the possibility is 1.45 times <strong>greater</strong>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="mf">0.015</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.015113064615719
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="p">(</span><span class="mi">25</span><span class="o">*</span><span class="mf">0.015</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.4549914146182013
</pre></div>
</div>
</div>
</div>
<p><strong>Interpreting <code class="docutils literal notranslate"><span class="pre">DiffWalk</span></code>: <span class="math notranslate nohighlight">\(\quad\widehat{\beta}_{DiffWalk} = 0.41 &gt; 0\)</span></strong></p>
<ul class="simple">
<li><p>The possibility of having diabetes (<code class="docutils literal notranslate"><span class="pre">Diabetes=1</span></code>) with respect to not having (<code class="docutils literal notranslate"><span class="pre">Diabetes=0</span></code>) is 1.5 times <strong>greater</strong> for patients with difficulties for walking  (<code class="docutils literal notranslate"><span class="pre">DiffWalk=1</span></code>) than for those without (<code class="docutils literal notranslate"><span class="pre">DiffWalk=0</span></code>).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="mf">0.41</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.5068177851128535
</pre></div>
</div>
</div>
</div>
<p><strong>Interpreting <code class="docutils literal notranslate"><span class="pre">Sex</span></code>: <span class="math notranslate nohighlight">\(\quad\widehat{\beta}_{Sex} = 0.31 &gt; 0\)</span></strong></p>
<ul class="simple">
<li><p>The possibility of having diabetes (<code class="docutils literal notranslate"><span class="pre">Diabetes=1</span></code>) with respect to not having (<code class="docutils literal notranslate"><span class="pre">Diabetes=0</span></code>) is 1.36 times <strong>greater</strong> for male patients  (<code class="docutils literal notranslate"><span class="pre">Sex=1</span></code>) than for female patients (<code class="docutils literal notranslate"><span class="pre">Sex=0</span></code>).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="mf">0.31</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.3634251141321778
</pre></div>
</div>
</div>
</div>
<p><strong>Interpreting <code class="docutils literal notranslate"><span class="pre">Age</span></code>:</strong></p>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}_{Age1} = 0.19 &gt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}_{Age2} = 0.49 &gt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}_{Age3} = 1 &gt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}_{Age4} = 1.3 &gt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}_{Age5} = 1.54 &gt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}_{Age6} = 1.77 &gt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}_{Age7} = 1.9 &gt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}_{Age8} = 2.06 &gt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}_{Age9} = 2.28 &gt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}_{Age10} = 2.34 &gt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}_{Age11} = 2.27 &gt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}_{Age12} = 2.13 &gt; 0\)</span></p>
<div class="math notranslate nohighlight">
\[\dfrac{OR(x_{i} | x_{ijr} = 1)}{OR(x_{i} | x_{ij} = 0)} = \dfrac{e^{\widehat{\eta}_i |x_{ijr} = 1} }{e^{\widehat{\eta}_i |x_{ij}=0}} = e^{\widehat{\eta}_i |x_{ijr} = 1 - \widehat{\eta}_i |x_{ij}=0} = e^{\widehat{\beta}_{jr}} \]</div>
<div class="math notranslate nohighlight">
\[\dfrac{OR(x_{i} | x_{ijr} = 1)}{OR(x_{i} | x_{ijc} = 1)} = \dfrac{OR(x_{i} | x_{ij} = r)}{OR(x_{i} | x_{ij} = c)} = \dfrac{e^{\widehat{\eta}_i |x_{ijr} = 1} }{e^{\widehat{\eta}_i |x_{ijc}=1}} = e^{\widehat{\eta}_i |x_{ijr} = 1 - \widehat{\eta}_i |x_{ijc}=1} = e^{\widehat{\beta}_{jr} - \widehat{\beta}_{jc}}\]</div>
<ul class="simple">
<li><p>The possibility of having diabetes (<code class="docutils literal notranslate"><span class="pre">Diabetes=1</span></code>) with respect to not having (<code class="docutils literal notranslate"><span class="pre">Diabetes=0</span></code>) is 5.87 times <strong>greater</strong> for individuals with an <code class="docutils literal notranslate"><span class="pre">Age</span></code> in [50, 54] than for those with an <code class="docutils literal notranslate"><span class="pre">Age</span></code> in [18, 24].</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="mf">1.77</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5.870853361382601
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The possibility of having diabetes (<code class="docutils literal notranslate"><span class="pre">Diabetes=1</span></code>) with respect to not having (<code class="docutils literal notranslate"><span class="pre">Diabetes=0</span></code>) is 10.4 times <strong>greater</strong> for individuals with an <code class="docutils literal notranslate"><span class="pre">Age</span></code> in [70, 74] than for those with an <code class="docutils literal notranslate"><span class="pre">Age</span></code> in [18, 24].</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="mf">2.34</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10.381236562731843
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The possibility of having diabetes (<code class="docutils literal notranslate"><span class="pre">Diabetes=1</span></code>) with respect to not having (<code class="docutils literal notranslate"><span class="pre">Diabetes=0</span></code>) is 1.77 times <strong>greater</strong> for individuals with an <code class="docutils literal notranslate"><span class="pre">Age</span></code> in [70, 74] than for those with an <code class="docutils literal notranslate"><span class="pre">Age</span></code> in [50, 54].</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="p">(</span><span class="mf">2.34</span> <span class="o">-</span> <span class="mf">1.77</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.7682670514337349
</pre></div>
</div>
</div>
</div>
<p><strong>Interpreting <code class="docutils literal notranslate"><span class="pre">Income</span></code>: <span class="math notranslate nohighlight">\(\quad\widehat{\beta}_{Income} = -0.11 &lt; 0\)</span></strong></p>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}_{Income1} = -0.033 &lt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}_{Income2} = -0.098 &lt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}_{Income3} = -0.16 &lt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}_{Income4} = -0.28 &lt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}_{Income5} = -0.41 &lt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}_{Income6} = -0.5 &lt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}_{Income7} = -0.74 &lt; 0\)</span></p>
<ul class="simple">
<li><p>The possibility of having diabetes (<code class="docutils literal notranslate"><span class="pre">Diabetes=1</span></code>) with respect to not having (<code class="docutils literal notranslate"><span class="pre">Diabetes=0</span></code>) is 1.32 times <strong>lower</strong> for individuals with an <code class="docutils literal notranslate"><span class="pre">Income</span></code> in [25k, 35k] than for those with an <code class="docutils literal notranslate"><span class="pre">Income</span></code> lower than 10k.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**-</span><span class="mf">0.28</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.3231298123374369
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The possibility of having diabetes (<code class="docutils literal notranslate"><span class="pre">Diabetes=1</span></code>) with respect to not having (<code class="docutils literal notranslate"><span class="pre">Diabetes=0</span></code>) is 2.1 times <strong>lower</strong> for individuals with an <code class="docutils literal notranslate"><span class="pre">Income</span></code> higher than 75k than for those with an <code class="docutils literal notranslate"><span class="pre">Income</span></code> lower than 10k.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**-</span><span class="mf">0.74</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.0959355144943643
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The possibility of having diabetes (<code class="docutils literal notranslate"><span class="pre">Diabetes=1</span></code>) with respect to not having (<code class="docutils literal notranslate"><span class="pre">Diabetes=0</span></code>) is 1.6  times <strong>lower</strong> for individuals with an <code class="docutils literal notranslate"><span class="pre">Income</span></code> higher than 75k than for those with an <code class="docutils literal notranslate"><span class="pre">Income</span></code> in [25k, 35k].</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mf">0.74</span> <span class="o">-</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.28</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.5840739849944816
</pre></div>
</div>
</div>
</div>
</section>
<section id="conclusions">
<h2><strong>Conclusions</strong><a class="headerlink" href="#conclusions" title="Link to this heading">#</a></h2>
<p>As conclusions of the prediction part, we have tested a large number of different models. The main problem we have encountered has been class imbalance. There are models that by adjusting one of their parameters are able to work correctly with this problem. The ones that are not able to do it, we have had to apply oversampling and undersampling to see how they work with balanced classes.</p>
<p>Applying the balancing techniques improves the results noticeably, increasing an average of 20% in general. Models that were able to work well with unbalanced classes did not experience any improvement.</p>
<p>Among all the possibilities, the one with the best results was XGBoost with balancing techniques, but logistic regression came very close. In the case of having to choose, this model would be more interesting as it allows not only to get very close to the result in terms of prediction, but also has that goodness in the part of interpretability as it appears in the last part of this work. It allows you to know the importance and degree to which each of the predictor variables affects the response variable.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Diabetes Analysis and Detection</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#objective-statement"><strong>Objective Statement</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#requirements"><strong>Requirements</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction"><strong>Introduction</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data"><strong>Data</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reading"><strong>Reading</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape"><strong>Shape</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-types"><strong>Data types</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unique-values"><strong>Unique values</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#missing-values"><strong>Missing values</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eda"><strong>EDA</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#descriptive-summary"><strong>Descriptive summary</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#outliers"><strong>Outliers</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#frequency-table"><strong>Frequency table</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization"><strong>Visualization</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#histogram-matrix"><strong>Histogram matrix</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#boxplot-matrix"><strong>Boxplot matrix</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ecdfplot-matrix"><strong>ECDFplot matrix</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#barplot-matrix"><strong>Barplot matrix</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#response-vs-predictors-analysis"><strong>Response vs Predictors Analysis</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#response-vs-quantitative-predictors"><strong>Response vs Quantitative Predictors</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-quant-cat-descriptive-summary"><strong>Cross quant-cat descriptive summary</strong></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#response-vs-categorical-predictors"><strong>Response vs Categorical Predictors</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-contingence-table"><strong>Conditional contingence table</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-of-conditional-contingence-table"><strong>Visualization of conditional contingence table</strong></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intro-to-machine-learning-with-sklearn"><strong>Intro to Machine Learning with <code class="docutils literal notranslate"><span class="pre">Sklearn</span></code></strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-response-and-predictors"><strong>Defining the response and predictors</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-outer-evaluation-train-test-split"><strong>Defining outer evaluation: train-test split</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-inner-evaluation-k-fold-cross-validation"><strong>Defining inner evaluation: K-Fold Cross Validation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-some-models"><strong>Defining some models</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applying-inner-evaluation"><strong>Applying inner evaluation</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#without-hpo"><strong>Without HPO</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#with-hpo"><strong>With HPO</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#grid-search"><strong>Grid Search</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#random-search"><strong>Random Search</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#optuna"><strong>Optuna</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#applying-hpo-to-all-the-models-together"><strong>Applying HPO to all the models together</strong></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#selecting-the-best-model"><strong>Selecting the best model</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applying-outer-evaluation"><strong>Applying outer evaluation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ml-with-pipelines"><strong>ML with Pipelines</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-machine-learning-workflow"><strong>Advanced Machine Learning Workflow</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"><strong>Defining the response and predictors</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2"><strong>Defining outer evaluation: train-test split</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3"><strong>Defining inner evaluation: K-Fold Cross Validation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-pipelines"><strong>Defining the Pipelines</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4"><strong>Applying inner evaluation</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#grids-for-hpo"><strong>Grids for HPO</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hyper-parameter-optimization-hpo"><strong>Hyper-parameter Optimization (HPO)</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5"><strong>Selecting the best model</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6"><strong>Applying outer evaluation</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-future-performance"><strong>Estimation of future performance</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix"><strong>Confusion matrix</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#improving-the-performance-using-imblearn"><strong>Improving the performance using <code class="docutils literal notranslate"><span class="pre">imblearn</span></code></strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pipelines"><strong>Pipelines</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7"><strong>Applying inner evaluation</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id8"><strong>Hyper-parameter optimization (HPO)</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id9"><strong>Selecting the best model</strong></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id10"><strong>Applying outer evaluation</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id11"><strong>Estimation of future performance</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id12"><strong>Confusion matrix</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trying-with-sequential-feature-selection-based-on-logistic-regression"><strong>Trying with Sequential Feature Selection based on Logistic Regression</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trying-with-stacking"><strong>Trying with Stacking</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#selecting-the-best-overall-model"><strong>Selecting the best overall model</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-predictions"><strong>Probabilistic predictions</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-the-best-model"><strong>Saving the best model</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-new-data"><strong>Predicting new data</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-with-logistic-regression"><strong>Interpretation with Logistic Regression</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusions"><strong>Conclusions</strong></a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Fabio Scielzo Ortiz
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>